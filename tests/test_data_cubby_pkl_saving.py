import plotbot as pb
import os
import shutil
import sys
import glob # Import glob for finding test files
import plotbot.data_tracker as dt
import plotbot.data_cubby as dc
import pytest # Add import at the top if not already present
from unittest.mock import patch, MagicMock # Add mock imports
import pandas as pd # Add pandas for timestamp
import numpy as np # Add numpy for array
import copy # Add copy for deepcopy mock
from plotbot.data_import import DataObject # Import DataObject for mocking
from datetime import datetime, timezone
import cdflib

# Define the MAIN storage directory - Tests will operate here
storage_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data_cubby')

def setup_function():
    """Ensures main storage dir exists and cleans ONLY specific test files/dirs within it."""
    print(f"\nDEBUG: Setup using MAIN storage directory: {storage_dir}")

    # 1. Ensure the main storage directory exists - NEVER DELETE IT
    try:
        os.makedirs(storage_dir, exist_ok=True)
        print(f"DEBUG: Ensured MAIN storage directory exists: {storage_dir}")
    except OSError as e:
        print(f"ERROR: Failed to create MAIN storage directory {storage_dir}: {e}")
        # If we can't create the main dir, something is wrong, but don't delete.
        
    # 2. Clean ONLY specific known test artifacts within the main directory
    #    - Index file
    #    - Specific PKL file pattern generated by these tests
    #    - The specific subdirectory containing those PKLs (safer if exclusive)
    
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    test_pkl_dir = os.path.join(storage_dir, 'fields', 'l2', 'mag_rtn_4_per_cycle') # Specific subdir for mag_rtn_4sa tests
    test_pkl_pattern = os.path.join(test_pkl_dir, 'psp_fld_l2_mag_RTN_4_Sa_per_Cyc_20240929_v*.pkl') # Use glob pattern
    general_pkl_path = os.path.join(storage_dir, 'general', 'test_obj.pkl') # From test_use_pkl_storage
    general_dir = os.path.join(storage_dir, 'general')

    # Remove specific index file
    try:
        if os.path.exists(index_path):
            os.remove(index_path)
            print(f"DEBUG: Removed test index file: {index_path}")
    except OSError as e:
        print(f"DEBUG: Error removing test index file {index_path}: {e}")

    # Remove specific PKL files matching the pattern
    try:
        for f in glob.glob(test_pkl_pattern):
            os.remove(f)
            print(f"DEBUG: Removed test PKL file: {f}")
        # Optionally remove the directory IF EMPTY and known to be test-specific
        # if os.path.exists(test_pkl_dir) and not os.listdir(test_pkl_dir):
        #     os.rmdir(test_pkl_dir)
        #     print(f"DEBUG: Removed empty test PKL directory: {test_pkl_dir}")
    except OSError as e:
        print(f"DEBUG: Error removing test PKL files/dir (mag_rtn_4sa) {test_pkl_pattern}: {e}")
        
    # Remove the specific file generated by test_use_pkl_storage_class_attribute
    try:
        if os.path.exists(general_pkl_path):
            os.remove(general_pkl_path)
            print(f"DEBUG: Removed test general PKL file: {general_pkl_path}")
        # Optionally remove the 'general' dir if empty and test-specific
        # if os.path.exists(general_dir) and not os.listdir(general_dir):
        #     os.rmdir(general_dir)
        #     print(f"DEBUG: Removed empty test general directory: {general_dir}")
    except OSError as e:
         print(f"DEBUG: Error removing test general PKL file/dir {general_dir}: {e}")

    # 3. Clean in-memory caches (DataCubby and global_tracker)
    print("DEBUG: Clearing in-memory data_cubby state...")
    dc.cubby.clear()
    # Keep registries, reset flag and directory pointer
    try:
        pb.data_cubby.use_pkl_storage = False
        print("DEBUG: Reset use_pkl_storage to False via setter.")
        # Reset storage directory to the main one (or None if preferred)
        pb.data_cubby.set_storage_directory(storage_dir) 
        print(f"DEBUG: Reset DataCubby storage directory to main: {storage_dir}")
    except Exception as e:
        print(f"DEBUG: Error during DataCubby memory reset: {e}")

    print("DEBUG: Clearing global_tracker state...")
    try:
        tracker = getattr(pb, 'global_tracker', None) or getattr(dt, 'global_tracker', None)
        if tracker:
            tracker.imported_ranges.clear()
            tracker.calculated_ranges.clear()
            print("DEBUG: Cleared global_tracker imported_ranges and calculated_ranges.")
        else:
            print("DEBUG: global_tracker not found.")
    except AttributeError:
        print("DEBUG: Error clearing global_tracker (AttributeError).")

def teardown_function():
    """Clean up SPECIFIC test files from MAIN directory, unless marker file exists."""
    marker_filename = ".persist_marker"
    # Marker is checked in the MAIN storage directory now
    marker_path = os.path.join(storage_dir, marker_filename) 

    if os.path.exists(marker_path):
        print(f"\nDEBUG: Persistence marker found in MAIN dir ({marker_path}). Skipping teardown cleanup.")
        try:
            os.remove(marker_path) # Remove the marker so cleanup runs next time
            print(f"DEBUG: Removed persistence marker from MAIN dir.")
        except OSError as e:
            print(f"DEBUG: Error removing persistence marker {marker_path}: {e}")
        return # Skip the cleanup

    # Proceed with cleanup of SPECIFIC test files if marker not found
    print(f"\nDEBUG: Cleaning up specific test files from MAIN directory: {storage_dir}")
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    test_pkl_dir = os.path.join(storage_dir, 'fields', 'l2', 'mag_rtn_4_per_cycle')
    test_pkl_pattern = os.path.join(test_pkl_dir, 'psp_fld_l2_mag_RTN_4_Sa_per_Cyc_20240929_v*.pkl')
    general_pkl_path = os.path.join(storage_dir, 'general', 'test_obj.pkl')
    general_dir = os.path.join(storage_dir, 'general')

    # Remove specific index file
    try:
        if os.path.exists(index_path):
            os.remove(index_path)
            print(f"DEBUG: Teardown removed test index file: {index_path}")
    except OSError as e:
        print(f"DEBUG: Teardown error removing test index file {index_path}: {e}")

    # Remove specific PKL files matching the pattern
    try:
        for f in glob.glob(test_pkl_pattern):
            os.remove(f)
            print(f"DEBUG: Teardown removed test PKL file: {f}")
        # Optionally remove the directory IF EMPTY and known to be test-specific
        # if os.path.exists(test_pkl_dir) and not os.listdir(test_pkl_dir):
        #     os.rmdir(test_pkl_dir)
        #     print(f"DEBUG: Teardown removed empty test PKL directory: {test_pkl_dir}")
    except OSError as e:
        print(f"DEBUG: Teardown error removing test PKL files/dir {test_pkl_pattern}: {e}")

    # Remove the specific file generated by test_use_pkl_storage_class_attribute
    try:
        if os.path.exists(general_pkl_path):
            os.remove(general_pkl_path)
            print(f"DEBUG: Teardown removed test general PKL file: {general_pkl_path}")
        # Optionally remove the 'general' dir if empty and test-specific
        # if os.path.exists(general_dir) and not os.listdir(general_dir):
        #     os.rmdir(general_dir)
        #     print(f"DEBUG: Teardown removed empty test general directory: {general_dir}")
    except OSError as e:
         print(f"DEBUG: Teardown error removing test general PKL file/dir {general_dir}: {e}")

def test_use_pkl_storage_class_attribute():
    """Test that use_pkl_storage works correctly as a class attribute."""
    # Ensure DataCubby is pointing to the main storage directory for this test
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Set storage directory for test to MAIN: {storage_dir}")

    # Get the initial value
    initial_value = pb.data_cubby._use_pkl_storage
    print(f"Class attribute _use_pkl_storage initial value: {initial_value}")
    
    # Set to the opposite value
    new_value = not initial_value
    print(f"Setting use_pkl_storage to: {new_value}")
    pb.data_cubby.use_pkl_storage = new_value
    
    # Check the class attribute directly after setting
    assert pb.data_cubby._use_pkl_storage == new_value, "Class attribute _use_pkl_storage not updated"
    print(f"Class attribute _use_pkl_storage after setting: {pb.data_cubby._use_pkl_storage}")
    
    # Create a simple test object
    test_obj = {'test': 'data'}
    
    # Call stash and verify it checks the class attribute properly
    print("Stashing test object...")
    # Stash with a unique identifier for this test
    pb.data_cubby.stash(test_obj, class_name="test_obj") 
    
    # Display expected directory
    expected_dir = storage_dir # Expecting the main dir
    print(f"Expected storage directory: {expected_dir}")
    
    # Check if directory exists (should exist if storage is enabled)
    dir_exists = os.path.exists(expected_dir)
    print(f"Storage directory exists: {dir_exists}")
    
    # Check for the specific test file created by stash (using default logic)
    # This assumes save_to_disk puts 'test_obj' in the 'general' subdir
    # MODIFIED: Check for the correct fallback filename pattern
    expected_file_path = os.path.join(storage_dir, 'general', 'test_obj_nodate_vXX.pkl')
    file_exists = os.path.exists(expected_file_path)
    print(f"Specific test PKL file exists ({expected_file_path}): {file_exists}")

    # Verify the directory and file state matches the setting
    if new_value:
        assert dir_exists, "Storage directory should exist when storage is enabled"
        assert file_exists, f"Specific test PKL file {expected_file_path} should exist when storage is enabled"
    else:
        # If storage is disabled, the directory might exist, but the specific file shouldn't
        assert not file_exists, f"Specific test PKL file {expected_file_path} should NOT exist when storage is disabled"

    # No cleanup needed here, teardown handles specific files
    print("Test completed successfully")

def test_pkl_saving_on_plotbot_call(capsys): # Add capsys fixture
    """Test if enabling pkl storage triggers saving during a plotbot call,
    AND that it loads from existing PKL after setup clears memory.
    """
    print("\n--- Starting test_pkl_saving_on_plotbot_call ---")
    
    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage (load_from_disk runs here)
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    print(f"DEBUG: After setting, pb.data_cubby._use_pkl_storage is: {pb.data_cubby._use_pkl_storage}")
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output

    # Define time range
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    variable = pb.mag_rtn_4sa.br
    identifier = 'mag_RTN_4sa' # Identifier for this data type
    print(f"DEBUG: Calling plotbot for trange: {trange}")

    # Clear captured output before the call
    _ = capsys.readouterr() 
    
    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        pytest.fail(f"plotbot call failed during test: {e}")

    print("DEBUG: plotbot call finished.")
    captured = capsys.readouterr() # Read captured stdout/stderr
    stdout = captured.out
    print("\n--- Captured STDOUT for plotbot call --- ")
    print(stdout)
    print("--- End Captured STDOUT --- ")

    # --- Verification 1: PKL file exists (as before) ---
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir}."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created."

    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    expected_pkl_pattern = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    print(f"DEBUG: Checking for specific PKL file pattern: {expected_pkl_pattern}")
    found_files = glob.glob(expected_pkl_pattern)
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_pattern} was not found."
    print(f"DEBUG: Found PKL file(s): {found_files}")

    # --- Verification 2: Check if download/update was SKIPPED --- 
    # This relies on a previous test (like persistence test) leaving the PKL file
    # If the PKL was loaded successfully by load_from_disk at the start of the test,
    # these messages should be absent from the captured stdout.
    print("DEBUG: Verifying download/import/update was potentially skipped (if PKL existed from previous run)...")
    download_msg_found = "Downloading missing files" in stdout
    import_msg_found = "CDF Data import complete" in stdout
    update_msg_found = f"Updating {identifier}" in stdout
    refresh_msg_found = "Import/Refresh required" in stdout
    
    if download_msg_found or import_msg_found or update_msg_found or refresh_msg_found:
        # This is not necessarily a FAILURE, it just means the PKL wasn't present 
        # beforehand (e.g., first run or persistence marker wasn't used).
        # We print a warning instead of asserting False.
        pb.print_manager.warning("Test ran, but download/update messages indicate PKL wasn't pre-loaded.")
        pb.print_manager.warning(f"  - Download msg found: {download_msg_found}")
        pb.print_manager.warning(f"  - Import msg found: {import_msg_found}")
        pb.print_manager.warning(f"  - Update msg found: {update_msg_found}")
        pb.print_manager.warning(f"  - Refresh msg found: {refresh_msg_found}")
    else:
        print("DEBUG: No download/import/update messages found. PKL likely loaded successfully.")

    print("--- Finished test_pkl_saving_on_plotbot_call ---")

def test_pkl_integrity_and_cache_load(capsys): # Add capsys fixture to capture output
    """Test PKL file integrity (size) and that data is loaded from cache."""
    print("\n--- Starting test_pkl_integrity_and_cache_load ---")

    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Ensure storage is enabled
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output for this test

    # Define time range and variable
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    variable = pb.mag_rtn_4sa.br
    variable_class_name = 'mag_rtn_4sa' # Manually specify class name for path finding
    variable_subclass_name = 'br' # Manually specify subclass name for path finding

    # --- First Call: Generate and Save Data ---
    print(f"DEBUG: Calling plotbot (1st time) for trange: {trange}")
    pb.plotbot(trange, variable, 1)
    print("DEBUG: plotbot call (1st time) finished.")

    # --- File Integrity Check ---
    # Construct the expected path based on the new save_to_disk logic
    identifier = "mag_rtn_4sa" # The identifier used
    date_str = "20240929" # From the trange used
    version_str = "vXX" # Placeholder
    expected_filename = f"psp_fld_l2_mag_RTN_4_Sa_per_Cyc_{date_str}_{version_str}.pkl"
    
    # Get the expected directory components from psp_data_types (consistent with save_to_disk)
    # Path needs to include 'psp_data' as per _get_storage_path_for_object logic
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle'] 
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    # Use glob to find the actual file regardless of version
    expected_pkl_pattern = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    print(f"DEBUG: Checking for PKL file pattern at correct location: {expected_pkl_pattern}")

    found_files = glob.glob(expected_pkl_pattern)
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_pattern} was not found at the derived path."
    # Use the found file path for size check
    actual_pkl_file = found_files[0] 
    print(f"DEBUG: Found actual PKL file: {actual_pkl_file}")

    # Check file size (e.g., > 100 bytes to ensure it's not empty)
    file_size = os.path.getsize(actual_pkl_file)
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 100, f"PKL file {actual_pkl_file} size ({file_size} bytes) is too small."

    # Simulate clearing memory before reloading from disk
    pb.data_cubby.cubby.clear() # Only clear the data cache
    # pb.data_cubby.class_registry.clear() # Do not clear registries
    # pb.data_cubby.subclass_registry.clear() # Do not clear registries

    print("DEBUG: Attempting to load from disk...")
    
    # Temporarily increase recursion depth for loading potentially complex pickles
    original_recursion_limit = sys.getrecursionlimit()
    print(f"DEBUG: Original recursion limit: {original_recursion_limit}")
    sys.setrecursionlimit(max(original_recursion_limit, 2000)) # Increase if needed, e.g., to 2000
    print(f"DEBUG: Temporarily set recursion limit to: {sys.getrecursionlimit()}")
    
    load_successful = False
    try:
        load_successful = pb.data_cubby.load_from_disk()
    except Exception as e:
        print(f"DEBUG: Exception during load_from_disk: {e}")
        # Ensure recursion limit is reset even if load fails
    finally:
        sys.setrecursionlimit(original_recursion_limit) # Reset recursion limit
        print(f"DEBUG: Reset recursion limit to: {sys.getrecursionlimit()}")
        
    assert load_successful, "Failed to load data back from disk after clearing memory (even with increased recursion limit)."
    
    # --- Second Call: Should use loaded data ---
    print(f"DEBUG: Calling plotbot (2nd time) after forced reload for same trange: {trange}")
    # Capture stdout/stderr for the second call
    _ = capsys.readouterr() 
    pb.plotbot(trange, variable, 1)
    print("DEBUG: plotbot call (2nd time) finished.")

    # --- Cache Load Verification ---
    captured = capsys.readouterr()
    stdout = captured.out
    print("\n--- Captured STDOUT for 2nd plotbot call (after forced reload) ---")
    print(stdout)
    print("--- End Captured STDOUT ---")

    # Check if the data acquisition was skipped (i.e., no import/update messages)
    # We should NOT see the "CDF Data import complete" or "Updating mag_RTN_4sa" messages
    import_msg = "CDF Data import complete"
    update_msg = "Updating mag_RTN_4sa"
    
    print(f"DEBUG: Checking if import message ('{import_msg}') is ABSENT")
    assert import_msg not in stdout, f"Data import seems to have run again on the second call when it should have been cached."
    
    print(f"DEBUG: Checking if update message ('{update_msg}') is ABSENT")
    assert update_msg not in stdout, f"Data update seems to have run again on the second call when it should have been cached."

    # # Previous check (now less relevant as we forced load_from_disk):
    # grab_success_msg = f"✅ Successfully retrieved {variable_class_name}" # Check grab success
    # print(f"DEBUG: Checking for cache hit message: '{grab_success_msg}'")
    # assert grab_success_msg in stdout, f"Cache load message not found in stdout for the second call. Should have retrieved '{variable_class_name}'."

    print("--- Finished test_pkl_integrity_and_cache_load ---")

def test_pkl_file_location_persistence():
    """Test saving a file and verify its location without automatic cleanup.
    
    This test intentionally leaves the created file and directory structure 
    in place for manual inspection. It does not use the standard setup/teardown.
    """
    print("\n--- Starting test_pkl_file_location_persistence --- ")
    # Use the MAIN storage directory defined at the top of the file
    storage_dir_persist = storage_dir 
    print(f"DEBUG: Target storage directory for persistence test: {storage_dir_persist}")

    # --- Manual Setup --- 
    # Ensure MAIN directory exists, clean ONLY specific test files from previous runs
    os.makedirs(storage_dir_persist, exist_ok=True)
    index_path = os.path.join(storage_dir_persist, 'data_cubby_index.json')
    test_pkl_dir = os.path.join(storage_dir_persist, 'fields', 'l2', 'mag_rtn_4_per_cycle')
    test_pkl_pattern = os.path.join(test_pkl_dir, 'psp_fld_l2_mag_RTN_4_Sa_per_Cyc_20240929_v*.pkl')
    try: 
        if os.path.exists(index_path): os.remove(index_path)
        for f in glob.glob(test_pkl_pattern): os.remove(f)
        print(f"DEBUG: Persistence setup cleaned specific test files from {storage_dir_persist}")
    except OSError as e:
        print(f"DEBUG: Error cleaning specific test files in persistence setup: {e}")

    # --- Configure DataCubby for this test instance ---
    # Explicitly set the storage directory for this test to the MAIN one
    pb.data_cubby.set_storage_directory(storage_dir_persist)
    # Corrected debug print to use the method/property that exists
    print(f"DEBUG: Explicitly set DataCubby storage dir via method. Current base dir: {pb.data_cubby.base_pkl_directory}")

    pb.data_cubby.use_pkl_storage = True
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output

    # Corrected this debug print - accesses base_pkl_directory which exists
    print(f"DEBUG: DataCubby configured: use_pkl_storage={pb.data_cubby.use_pkl_storage}, base_pkl_directory='{pb.data_cubby.base_pkl_directory}'")

    # --- Trigger Save Operation (e.g., via plotbot) ---
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    variable = pb.mag_rtn_4sa.br # Use a variable known to trigger saving
    print(f"DEBUG: Calling plotbot for trange: {trange}")
    
    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        assert False, f"plotbot call failed during test: {e}"

    print("DEBUG: plotbot call finished.")

    # --- Verification ---
    # 1. Check if storage directory was created
    print(f"DEBUG: Verifying existence of storage directory: {storage_dir_persist}")
    assert os.path.exists(storage_dir_persist), f"Storage directory was not created at {storage_dir_persist}."

    # 2. Check if the index file was created
    index_path = os.path.join(storage_dir_persist, 'data_cubby_index.json')
    print(f"DEBUG: Verifying existence of index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created."

    # 3. Check if the specific PKL file was created with the correct name
    # Construct the expected filename pattern based on save_to_disk logic
    identifier = "mag_rtn_4sa" # The identifier used in the plotbot call
    date_str = "20240929" # From the trange used
    # Use glob pattern to find the file regardless of version
    expected_filename_pattern = f"psp_fld_l2_mag_rtn_4_sa_per_cyc_{date_str}_v*.pkl"
    
    # For mag_rtn_4sa, path_components should include 'psp_data'
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir_persist, *expected_pkl_dir_components)
    expected_pkl_file_pattern = os.path.join(expected_pkl_dir, expected_filename_pattern) # MODIFIED: Use pattern variable
    
    print(f"DEBUG: Verifying existence of PKL file pattern: {expected_pkl_file_pattern}") # MODIFIED: Print pattern
    found_files = glob.glob(expected_pkl_file_pattern) # MODIFIED: Use glob with pattern
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_file_pattern} was not found." # MODIFIED: Assert based on found_files
    actual_pkl_file = found_files[0] # Use the first found file
    print(f"DEBUG: Found actual PKL file: {actual_pkl_file}") # Log the found file

    # 4. Optional: Check file size (basic integrity)
    file_size = os.path.getsize(actual_pkl_file) # MODIFIED: Use actual_pkl_file
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 100, f"PKL file {actual_pkl_file} size ({file_size} bytes) seems too small."

    print(f"--- Test Passed: PKL file created at {actual_pkl_file}. Leaving artifacts for inspection. ---")
    
    # --- Create marker file IN MAIN DIR to prevent teardown for this specific test --- 
    marker_filename = ".persist_marker"
    marker_path = os.path.join(storage_dir_persist, marker_filename) # Path is now in MAIN dir
    try:
        # Ensure the directory exists before trying to create the file inside it
        os.makedirs(storage_dir_persist, exist_ok=True)
        with open(marker_path, 'w') as f:
            f.write('Persist this run\n')
        print(f"DEBUG: Created persistence marker file: {marker_path}")
    except OSError as e:
        print(f"DEBUG: Failed to create persistence marker file {marker_path}: {e}")
        # If marker fails, teardown might run, warn the user.
        print_manager.warning(f"Could not create marker file; teardown might remove {storage_dir_persist}")

    print("--- Finished test_pkl_file_location_persistence ---")

def test_load_from_disk_after_stash():
    # ... existing code ...
    pass # Placeholder if the function is empty for now

def test_pkl_saving_full_day():
    """Test if enabling pkl storage triggers saving correctly for a full day's range."""
    print("\n--- Starting test_pkl_saving_full_day ---")
    
    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output

    # Define FULL DAY time range
    trange = ['2024-09-29/00:00:00.000', '2024-09-29/23:59:59.999']
    variable = pb.mag_rtn_4sa.br # Variable to plot
    print(f"DEBUG: Calling plotbot for FULL DAY trange: {trange}")

    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        assert False, f"plotbot call failed during test: {e}"

    print("DEBUG: plotbot call finished for full day.")

    # --- Verification ---
    # Assert that the storage directory and index file were created
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir} after plotbot call."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created after plotbot call."

    # Check for the specific pkl file for mag_rtn_4sa in the correct subfolder
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    # Use glob to check for the versioned file (lowercase name)
    expected_pkl_pattern = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    print(f"DEBUG: Checking for specific PKL file pattern: {expected_pkl_pattern}")
    found_files = glob.glob(expected_pkl_pattern)
    
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_pattern} was not found."
    print(f"DEBUG: Found PKL file(s): {found_files}")
    
    # Check size of the found file
    actual_pkl_file = found_files[0]
    file_size = os.path.getsize(actual_pkl_file)
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 100, f"PKL file {actual_pkl_file} size ({file_size} bytes) seems too small."

    print("--- Finished test_pkl_saving_full_day ---")

def test_pkl_saving_two_days():
    """Test if PKL storage saves separate files correctly for a two-day range."""
    print("\n--- Starting test_pkl_saving_two_days ---")
    
    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True

    # Define TWO DAY time range
    trange = ['2024-09-28/00:00:00.000', '2024-09-29/23:59:59.999']
    variable = pb.mag_rtn_4sa.br
    print(f"DEBUG: Calling plotbot for TWO DAY trange: {trange}")

    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        # If plotbot fails here, check data availability for 2024-09-28
        assert False, f"plotbot call failed during test: {e}" 

    print("DEBUG: plotbot call finished for two days.")

    # --- Verification ---
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir}."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created."

    # Check for the two specific pkl files
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    
    # Define patterns for both days
    pattern_day1 = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240928_v*.pkl')
    pattern_day2 = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    
    print(f"DEBUG: Checking for Day 1 PKL file pattern: {pattern_day1}")
    found_day1 = glob.glob(pattern_day1)
    assert len(found_day1) > 0, f"Expected PKL file pattern for Day 1 ({pattern_day1}) was not found."
    print(f"DEBUG: Found Day 1 PKL file(s): {found_day1}")
    
    print(f"DEBUG: Checking for Day 2 PKL file pattern: {pattern_day2}")
    found_day2 = glob.glob(pattern_day2)
    assert len(found_day2) > 0, f"Expected PKL file pattern for Day 2 ({pattern_day2}) was not found."
    print(f"DEBUG: Found Day 2 PKL file(s): {found_day2}")

    # Check size of both found files
    actual_pkl_day1 = found_day1[0]
    size_day1 = os.path.getsize(actual_pkl_day1)
    print(f"DEBUG: Day 1 PKL file size: {size_day1} bytes")
    assert size_day1 > 100, f"Day 1 PKL file {actual_pkl_day1} size ({size_day1} bytes) seems too small."

    actual_pkl_day2 = found_day2[0]
    size_day2 = os.path.getsize(actual_pkl_day2)
    print(f"DEBUG: Day 2 PKL file size: {size_day2} bytes")
    assert size_day2 > 100, f"Day 2 PKL file {actual_pkl_day2} size ({size_day2} bytes) seems too small."

    print("--- Finished test_pkl_saving_two_days ---")

# Patch the function where it's looked up by the code under test (get_data)
@patch('plotbot.get_data.import_data_function')
def test_pkl_saving_mag_rtn_hourly(mock_import_data_function):
    """Test if PKL storage saves correctly for mag_rtn (6-hourly files),
    using a MOCKED data import to avoid actual download/load.
    """
    print("\n--- Starting test_pkl_saving_mag_rtn_hourly (MOCKED IMPORT) ---")

    # --- Mock Configuration --- 
    # Define the data for the mock object first
    mock_source_filenames = ['psp_data/fields/l2/mag_rtn/2024/psp_fld_l2_mag_RTN_2024092912_v02.cdf']
    # Provide times as TT2000 int64, which is what cdflib usually works with internally
    # Use cdflib to convert a known datetime to TT2000 for the mock
    mock_dt = datetime(2024, 9, 29, 12, 30, 0, tzinfo=timezone.utc)
    mock_times_tt2000 = np.array([cdflib.cdfepoch.compute_tt2000([mock_dt.year, mock_dt.month, mock_dt.day, mock_dt.hour, mock_dt.minute, mock_dt.second, int(mock_dt.microsecond/1000)])], dtype=np.int64)
    # mock_times = np.array([pd.Timestamp('2024-09-29 12:30:00').to_datetime64()], dtype='datetime64[ns]') # Old way
    mock_data_dict = {'psp_fld_l2_mag_RTN': np.array([[1.0, 2.0, 3.0]], dtype=np.float32)}

    # Create a mock DataObject, passing required args to constructor
    mock_data = DataObject(
        times=mock_times_tt2000, # Use TT2000 times
        data=mock_data_dict,
        source_filenames=mock_source_filenames
    )

    # Configure the mock function to return this pre-built object
    mock_import_data_function.return_value = mock_data
    print(f"DEBUG: Configured mock_import_data_function to return DataObject with source_filenames: {mock_data.source_filenames}")
    # --- End Mock Configuration ---

    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True

    # Define 1-hour time range (still needed for plotbot call signature)
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/13:10:00.000']
    variable = pb.mag_rtn.br # Use mag_rtn data type
    print(f"DEBUG: Calling plotbot for mag_rtn 1-hour trange: {trange} (Data import is mocked)")

    try:
        # This call will now use the mocked import_data_function
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        import traceback
        print(traceback.format_exc())
        assert False, f"plotbot call failed during test: {e}"

    print("DEBUG: plotbot call finished for mag_rtn 1-hour.")

    # --- Verification (Focus on save_to_disk outcome) ---
    # Check if mock_import_data_function was called (confirming the mock worked)
    mock_import_data_function.assert_called_once()
    print("DEBUG: Asserted that mock_import_data_function was called.")

    # Check if the index file was created (Primary failure point previously)
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir}."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    # THIS IS THE KEY ASSERTION NOW - Does the index get created?
    assert os.path.exists(index_path), "data_cubby_index.json was not created (even with mocked import). Check save_to_disk regex/logic."

    # Check if the specific PKL file was created (Secondary check)
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    expected_filename_base = 'psp_fld_l2_mag_RTN_2024092912'
    expected_pkl_pattern = os.path.join(expected_pkl_dir, f'{expected_filename_base}_v*.pkl')

    print(f"DEBUG: Checking for mag_rtn PKL file pattern: {expected_pkl_pattern}")
    found_files = glob.glob(expected_pkl_pattern)

    assert len(found_files) > 0, f"Expected PKL file pattern for mag_rtn ({expected_pkl_pattern}) was not found (even with mocked import)."
    print(f"DEBUG: Found mag_rtn PKL file(s): {found_files}")

    # Optional: Check size (basic integrity check)
    actual_pkl_file = found_files[0]
    file_size = os.path.getsize(actual_pkl_file)
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 10, f"PKL file {actual_pkl_file} size ({file_size} bytes) seems too small (might be empty due to error)."

    print("--- Finished test_pkl_saving_mag_rtn_hourly (MOCKED IMPORT) ---")

@patch('plotbot.data_cubby.pickle.dump') # Mock the actual file writing
@patch('plotbot.data_cubby.os.makedirs') # Mock directory creation
# Revert to patching copy.deepcopy directly from the standard library
@patch('copy.deepcopy') 
@patch('plotbot.data_cubby.np.any') # Mock the mask check
@patch('plotbot.data_cubby.pd.to_datetime') # Mock datetime conversion inside the loop
def test_save_to_disk_hourly_regex_match(mock_pd_to_datetime, mock_np_any, mock_deepcopy, mock_os_makedirs, mock_pickle_dump):
    """Unit test: Verify save_to_disk regex logic correctly matches a 6-hourly filename."""
    print("\n--- Starting test_save_to_disk_hourly_regex_match ---")
    
    # --- Setup ---
    # Ensure PKL storage is enabled for this test
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage for test"
    
    # Configure mocks
    mock_np_any.return_value = True # Simulate data exists for the day
    # Configure the standard library deepcopy mock
    mock_deepcopy.side_effect = lambda x: x # Make deepcopy return the object passed
    # Mock pd.to_datetime to return a valid TZ-aware timestamp when called for mask range
    mock_pd_to_datetime.return_value = pd.Timestamp('2024-09-29', tz='UTC')
    
    # Create a mock object with the specific 6-hourly filename
    mock_obj = MagicMock()
    mock_obj.source_filenames = ['psp_fld_l2_mag_RTN_2024092912_v02.cdf']
    # Provide a minimal datetime_array needed for filtering logic
    # Needs to be timezone-aware (UTC) for comparison
    # Explicitly create a non-empty numpy array
    dt_series = pd.to_datetime(['2024-09-29 12:30:00+00:00']).tz_convert('UTC')
    mock_obj.datetime_array = np.array(dt_series)
    # Add assertions to verify the mock attribute type and length in the test setup
    assert isinstance(mock_obj.datetime_array, np.ndarray), "Mock datetime_array is not a numpy array"
    print(f"DEBUG: Mock object setup with source_filenames: {mock_obj.source_filenames}")

    # --- Action ---
    print("DEBUG: Calling save_to_disk with mock object...")
    try:
        pb.data_cubby.save_to_disk(identifier_to_save='mock_hourly_test', obj_to_save=mock_obj)
    except Exception as e:
        # If any unexpected error occurs during the save call, fail the test
        print(f"ERROR: save_to_disk raised an unexpected exception: {e}")
        import traceback
        print(traceback.format_exc())
        pytest.fail(f"save_to_disk failed unexpectedly: {e}")
    print("DEBUG: save_to_disk call completed.")

    # --- Verification ---
    # Check if the mocked copy.deepcopy was called
    try:
        mock_deepcopy.assert_called_once()
        print("SUCCESS: Mocked copy.deepcopy was called.")
    except AssertionError:
        print("FAILURE: Mocked copy.deepcopy was NOT called.")
        pytest.fail("copy.deepcopy was not called, check logic before or within the subset creation.")

    # The core assertion: Was pickle.dump called?
    # This implies the regex matched AND the subset creation logic ran successfully.
    try:
        mock_pickle_dump.assert_called_once()
        print("SUCCESS: pickle.dump was called, indicating regex match succeeded and subset processing passed.")
    except AssertionError:
        print("FAILURE: pickle.dump was NOT called. Regex match likely failed OR subset processing failed.")
        pytest.fail("pickle.dump was not called. Check regex match or subset processing logic.")
        
    # Optional: Verify directory creation was attempted
    try:
        mock_os_makedirs.assert_called()
        print("DEBUG: os.makedirs was called.")
    except AssertionError:
        print("WARNING: os.makedirs was not called.") # Might be okay if dir existed

# =============================================================================
# Test Loading Specific Known Files
# =============================================================================

def test_load_specific_known_pkl():
    """Test loading a specific PKL file known to exist and be valid."""
    print("\n--- Starting test_load_specific_known_pkl ---")
    # Import necessary class locally to avoid top-level circular imports if run standalone
    from plotbot.data_classes.psp_mag_classes import mag_rtn_4sa_class
    import pickle
    import os
    import numpy as np

    # Define the path relative to the project root
    project_root = os.path.dirname(os.path.dirname(__file__)) # Gets Plotbot directory
    pkl_path_relative = os.path.join(
        'data_cubby', 'psp_data', 'fields', 'l2',
        'mag_rtn_4_per_cycle', 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240330_v02.pkl'
    )
    pkl_path_absolute = os.path.join(project_root, pkl_path_relative)

    print(f"DEBUG: Attempting to load PKL file: {pkl_path_absolute}")

    # 1. Check if file exists
    assert os.path.exists(pkl_path_absolute), f"PKL file does not exist at expected path: {pkl_path_absolute}"
    print(f"DEBUG: File exists at {pkl_path_absolute}")

    # 2. Try loading the file
    loaded_obj = None
    try:
        with open(pkl_path_absolute, "rb") as f:
            loaded_obj = pickle.load(f)
        print("DEBUG: Successfully loaded object using pickle.load()")
    except pickle.UnpicklingError as e:
        assert False, f"Failed to unpickle file {pkl_path_absolute}. It might be corrupted. Error: {e}"
    except Exception as e:
        assert False, f"An unexpected error occurred during loading {pkl_path_absolute}: {e}"

    # 3. Check if loading returned an object
    assert loaded_obj is not None, "Pickle loading did not return an object."

    # 4. Check the type of the loaded object (Adjust class name if needed)
    expected_class = mag_rtn_4sa_class
    assert isinstance(loaded_obj, expected_class), f"Loaded object is not of expected type {expected_class.__name__}. Got {type(loaded_obj).__name__}."
    print(f"DEBUG: Loaded object is of expected type: {type(loaded_obj).__name__}")

    # 5. Check for essential attributes like datetime_array
    assert hasattr(loaded_obj, 'datetime_array'), "Loaded object is missing 'datetime_array' attribute."
    assert loaded_obj.datetime_array is not None, "'datetime_array' attribute is None."
    assert isinstance(loaded_obj.datetime_array, np.ndarray), "'datetime_array' is not a NumPy array."
    assert len(loaded_obj.datetime_array) > 0, "'datetime_array' is empty."
    print(f"DEBUG: datetime_array found and is a non-empty NumPy array. Length: {len(loaded_obj.datetime_array)}")
    print(f"DEBUG: Time range (first/last): {loaded_obj.datetime_array[0]} / {loaded_obj.datetime_array[-1]}")

    # 6. Optional: Check source_filenames if expected
    if hasattr(loaded_obj, 'source_filenames'):
        print(f"DEBUG: Source filenames found: {loaded_obj.source_filenames}")
        # Add assertion if you know the expected value, e.g.:
        # assert loaded_obj.source_filenames == ['expected/path/to/source.cdf']
    else:
        print("DEBUG: Loaded object does not have a 'source_filenames' attribute.")

    print("--- Test finished: test_load_specific_known_pkl ---")

def test_load_specific_mag_rtn_hourly_pkl():
    """Test loading the specific PKL file expected from the hourly mag_RTN test.

    NOTE: This test is expected to FAIL initially if the corresponding saving
    test (`test_pkl_saving_mag_rtn_hourly`) is failing, as the file won't exist.
    Its failure confirms the issue is in the saving/regex step.
    """
    print("\n--- Starting test_load_specific_mag_rtn_hourly_pkl ---")
    # Import necessary class locally
    from plotbot.data_classes.psp_mag_classes import mag_rtn_class
    import pickle
    import os
    import numpy as np

    # Define the path relative to the project root
    project_root = os.path.dirname(os.path.dirname(__file__)) # Gets Plotbot directory
    pkl_path_relative = os.path.join(
        'data_cubby', 'psp_data', 'fields', 'l2',
        'mag_rtn', 'psp_fld_l2_mag_RTN_2024092912_v02.pkl' # Target hourly file
    )
    pkl_path_absolute = os.path.join(project_root, pkl_path_relative)

    print(f"DEBUG: Attempting to load PKL file: {pkl_path_absolute}")

    # 1. Check if file exists (THIS IS THE LIKELY FAILURE POINT)
    assert os.path.exists(pkl_path_absolute), f"PKL file does not exist at expected path: {pkl_path_absolute}. This is expected if the corresponding save test is failing."
    print(f"DEBUG: File exists at {pkl_path_absolute}")

    # 2. Try loading the file (only runs if file exists)
    loaded_obj = None
    try:
        with open(pkl_path_absolute, "rb") as f:
            loaded_obj = pickle.load(f)
        print("DEBUG: Successfully loaded object using pickle.load()")
    except pickle.UnpicklingError as e:
        assert False, f"Failed to unpickle file {pkl_path_absolute}. It might be corrupted. Error: {e}"
    except Exception as e:
        assert False, f"An unexpected error occurred during loading {pkl_path_absolute}: {e}"

    # 3. Check if loading returned an object
    assert loaded_obj is not None, "Pickle loading did not return an object."

    # 4. Check the type of the loaded object
    expected_class = mag_rtn_class
    assert isinstance(loaded_obj, expected_class), f"Loaded object is not of expected type {expected_class.__name__}. Got {type(loaded_obj).__name__}."
    print(f"DEBUG: Loaded object is of expected type: {type(loaded_obj).__name__}")

    # 5. Check for essential attributes like datetime_array
    assert hasattr(loaded_obj, 'datetime_array'), "Loaded object is missing 'datetime_array' attribute."
    assert loaded_obj.datetime_array is not None, "'datetime_array' attribute is None."
    assert isinstance(loaded_obj.datetime_array, np.ndarray), "'datetime_array' is not a NumPy array."
    assert len(loaded_obj.datetime_array) > 0, "'datetime_array' is empty."
    print(f"DEBUG: datetime_array found and is a non-empty NumPy array. Length: {len(loaded_obj.datetime_array)}")
    print(f"DEBUG: Time range (first/last): {loaded_obj.datetime_array[0]} / {loaded_obj.datetime_array[-1]}")

    # 6. Optional: Check source_filenames if expected
    if hasattr(loaded_obj, 'source_filenames'):
        print(f"DEBUG: Source filenames found: {loaded_obj.source_filenames}")
    else:
        print("DEBUG: Loaded object does not have a 'source_filenames' attribute.")

    print("--- Test finished: test_load_specific_mag_rtn_hourly_pkl ---")

# =============================================================================
# Run Tests (if executed directly)
# =============================================================================

# Simple manual test - execute when file is run directly
if __name__ == "__main__":
    print("Running data_cubby_pkl_saving tests directly...")
    initial_value = pb.data_cubby._use_pkl_storage
    print(f"Initial pickle storage setting: {initial_value}")
    
    # Set the variable 
    pb.data_cubby.use_pkl_storage = True
    print(f"After setting, pickle storage setting: {pb.data_cubby.use_pkl_storage}")
    
    # Try a simple plotbot call
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    print(f"Running plotbot call with trange: {trange}")
    pb.plotbot(trange, pb.mag_rtn_4sa.br, 1)
    
    # Check setting after the call
    print(f"Final pickle storage setting: {pb.data_cubby.use_pkl_storage}")
