import plotbot as pb
import os
import shutil
import sys
import glob # Import glob for finding test files
import plotbot.data_tracker as dt
import plotbot.data_cubby as dc
import pytest # Add import at the top if not already present

# Define the MAIN storage directory - Tests will operate here
storage_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data_cubby')

def setup_function():
    """Ensures main storage dir exists and cleans ONLY specific test files/dirs within it."""
    print(f"\nDEBUG: Setup using MAIN storage directory: {storage_dir}")

    # 1. Ensure the main storage directory exists - NEVER DELETE IT
    try:
        os.makedirs(storage_dir, exist_ok=True)
        print(f"DEBUG: Ensured MAIN storage directory exists: {storage_dir}")
    except OSError as e:
        print(f"ERROR: Failed to create MAIN storage directory {storage_dir}: {e}")
        # If we can't create the main dir, something is wrong, but don't delete.
        
    # 2. Clean ONLY specific known test artifacts within the main directory
    #    - Index file
    #    - Specific PKL file pattern generated by these tests
    #    - The specific subdirectory containing those PKLs (safer if exclusive)
    
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    test_pkl_dir = os.path.join(storage_dir, 'fields', 'l2', 'mag_rtn_4_per_cycle') # Specific subdir for mag_rtn_4sa tests
    test_pkl_pattern = os.path.join(test_pkl_dir, 'psp_fld_l2_mag_RTN_4_Sa_per_Cyc_20240929_v*.pkl') # Use glob pattern
    general_pkl_path = os.path.join(storage_dir, 'general', 'test_obj.pkl') # From test_use_pkl_storage
    general_dir = os.path.join(storage_dir, 'general')

    # Remove specific index file
    try:
        if os.path.exists(index_path):
            os.remove(index_path)
            print(f"DEBUG: Removed test index file: {index_path}")
    except OSError as e:
        print(f"DEBUG: Error removing test index file {index_path}: {e}")

    # Remove specific PKL files matching the pattern
    try:
        for f in glob.glob(test_pkl_pattern):
            os.remove(f)
            print(f"DEBUG: Removed test PKL file: {f}")
        # Optionally remove the directory IF EMPTY and known to be test-specific
        # if os.path.exists(test_pkl_dir) and not os.listdir(test_pkl_dir):
        #     os.rmdir(test_pkl_dir)
        #     print(f"DEBUG: Removed empty test PKL directory: {test_pkl_dir}")
    except OSError as e:
        print(f"DEBUG: Error removing test PKL files/dir {test_pkl_pattern}: {e}")
        
    # Remove the specific file generated by test_use_pkl_storage_class_attribute
    try:
        if os.path.exists(general_pkl_path):
            os.remove(general_pkl_path)
            print(f"DEBUG: Removed test general PKL file: {general_pkl_path}")
        # Optionally remove the 'general' dir if empty and test-specific
        # if os.path.exists(general_dir) and not os.listdir(general_dir):
        #     os.rmdir(general_dir)
        #     print(f"DEBUG: Removed empty test general directory: {general_dir}")
    except OSError as e:
         print(f"DEBUG: Error removing test general PKL file/dir {general_dir}: {e}")

    # 3. Clean in-memory caches (DataCubby and global_tracker)
    print("DEBUG: Clearing in-memory data_cubby state...")
    dc.cubby.clear()
    # Keep registries, reset flag and directory pointer
    try:
        pb.data_cubby.use_pkl_storage = False
        print("DEBUG: Reset use_pkl_storage to False via setter.")
        # Reset storage directory to the main one (or None if preferred)
        pb.data_cubby.set_storage_directory(storage_dir) 
        print(f"DEBUG: Reset DataCubby storage directory to main: {storage_dir}")
    except Exception as e:
        print(f"DEBUG: Error during DataCubby memory reset: {e}")

    print("DEBUG: Clearing global_tracker state...")
    try:
        tracker = getattr(pb, 'global_tracker', None) or getattr(dt, 'global_tracker', None)
        if tracker:
            tracker.imported_ranges.clear()
            tracker.calculated_ranges.clear()
            print("DEBUG: Cleared global_tracker imported_ranges and calculated_ranges.")
        else:
            print("DEBUG: global_tracker not found.")
    except AttributeError:
        print("DEBUG: Error clearing global_tracker (AttributeError).")

def teardown_function():
    """Clean up SPECIFIC test files from MAIN directory, unless marker file exists."""
    marker_filename = ".persist_marker"
    # Marker is checked in the MAIN storage directory now
    marker_path = os.path.join(storage_dir, marker_filename) 

    if os.path.exists(marker_path):
        print(f"\nDEBUG: Persistence marker found in MAIN dir ({marker_path}). Skipping teardown cleanup.")
        try:
            os.remove(marker_path) # Remove the marker so cleanup runs next time
            print(f"DEBUG: Removed persistence marker from MAIN dir.")
        except OSError as e:
            print(f"DEBUG: Error removing persistence marker {marker_path}: {e}")
        return # Skip the cleanup

    # Proceed with cleanup of SPECIFIC test files if marker not found
    print(f"\nDEBUG: Cleaning up specific test files from MAIN directory: {storage_dir}")
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    test_pkl_dir = os.path.join(storage_dir, 'fields', 'l2', 'mag_rtn_4_per_cycle')
    test_pkl_pattern = os.path.join(test_pkl_dir, 'psp_fld_l2_mag_RTN_4_Sa_per_Cyc_20240929_v*.pkl')
    general_pkl_path = os.path.join(storage_dir, 'general', 'test_obj.pkl')
    general_dir = os.path.join(storage_dir, 'general')

    # Remove specific index file
    try:
        if os.path.exists(index_path):
            os.remove(index_path)
            print(f"DEBUG: Teardown removed test index file: {index_path}")
    except OSError as e:
        print(f"DEBUG: Teardown error removing test index file {index_path}: {e}")

    # Remove specific PKL files matching the pattern
    try:
        for f in glob.glob(test_pkl_pattern):
            os.remove(f)
            print(f"DEBUG: Teardown removed test PKL file: {f}")
        # Optionally remove the directory IF EMPTY and known to be test-specific
        # if os.path.exists(test_pkl_dir) and not os.listdir(test_pkl_dir):
        #     os.rmdir(test_pkl_dir)
        #     print(f"DEBUG: Teardown removed empty test PKL directory: {test_pkl_dir}")
    except OSError as e:
        print(f"DEBUG: Teardown error removing test PKL files/dir {test_pkl_pattern}: {e}")

    # Remove the specific file generated by test_use_pkl_storage_class_attribute
    try:
        if os.path.exists(general_pkl_path):
            os.remove(general_pkl_path)
            print(f"DEBUG: Teardown removed test general PKL file: {general_pkl_path}")
        # Optionally remove the 'general' dir if empty and test-specific
        # if os.path.exists(general_dir) and not os.listdir(general_dir):
        #     os.rmdir(general_dir)
        #     print(f"DEBUG: Teardown removed empty test general directory: {general_dir}")
    except OSError as e:
         print(f"DEBUG: Teardown error removing test general PKL file/dir {general_dir}: {e}")

def test_use_pkl_storage_class_attribute():
    """Test that use_pkl_storage works correctly as a class attribute."""
    # Ensure DataCubby is pointing to the main storage directory for this test
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Set storage directory for test to MAIN: {storage_dir}")

    # Get the initial value
    initial_value = pb.data_cubby._use_pkl_storage
    print(f"Class attribute _use_pkl_storage initial value: {initial_value}")
    
    # Set to the opposite value
    new_value = not initial_value
    print(f"Setting use_pkl_storage to: {new_value}")
    pb.data_cubby.use_pkl_storage = new_value
    
    # Check the class attribute directly after setting
    assert pb.data_cubby._use_pkl_storage == new_value, "Class attribute _use_pkl_storage not updated"
    print(f"Class attribute _use_pkl_storage after setting: {pb.data_cubby._use_pkl_storage}")
    
    # Create a simple test object
    test_obj = {'test': 'data'}
    
    # Call stash and verify it checks the class attribute properly
    print("Stashing test object...")
    # Stash with a unique identifier for this test
    pb.data_cubby.stash(test_obj, class_name="test_obj") 
    
    # Display expected directory
    expected_dir = storage_dir # Expecting the main dir
    print(f"Expected storage directory: {expected_dir}")
    
    # Check if directory exists (should exist if storage is enabled)
    dir_exists = os.path.exists(expected_dir)
    print(f"Storage directory exists: {dir_exists}")
    
    # Check for the specific test file created by stash (using default logic)
    # This assumes save_to_disk puts 'test_obj' in the 'general' subdir
    # MODIFIED: Check for the correct fallback filename pattern
    expected_file_path = os.path.join(storage_dir, 'general', 'test_obj_nodate_vXX.pkl')
    file_exists = os.path.exists(expected_file_path)
    print(f"Specific test PKL file exists ({expected_file_path}): {file_exists}")

    # Verify the directory and file state matches the setting
    if new_value:
        assert dir_exists, "Storage directory should exist when storage is enabled"
        assert file_exists, f"Specific test PKL file {expected_file_path} should exist when storage is enabled"
    else:
        # If storage is disabled, the directory might exist, but the specific file shouldn't
        assert not file_exists, f"Specific test PKL file {expected_file_path} should NOT exist when storage is disabled"

    # No cleanup needed here, teardown handles specific files
    print("Test completed successfully")

def test_pkl_saving_on_plotbot_call(capsys): # Add capsys fixture
    """Test if enabling pkl storage triggers saving during a plotbot call,
    AND that it loads from existing PKL after setup clears memory.
    """
    print("\n--- Starting test_pkl_saving_on_plotbot_call ---")
    
    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage (load_from_disk runs here)
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    print(f"DEBUG: After setting, pb.data_cubby._use_pkl_storage is: {pb.data_cubby._use_pkl_storage}")
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output

    # Define time range
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    variable = pb.mag_rtn_4sa.br
    identifier = 'mag_RTN_4sa' # Identifier for this data type
    print(f"DEBUG: Calling plotbot for trange: {trange}")

    # Clear captured output before the call
    _ = capsys.readouterr() 
    
    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        pytest.fail(f"plotbot call failed during test: {e}")

    print("DEBUG: plotbot call finished.")
    captured = capsys.readouterr() # Read captured stdout/stderr
    stdout = captured.out
    print("\n--- Captured STDOUT for plotbot call --- ")
    print(stdout)
    print("--- End Captured STDOUT --- ")

    # --- Verification 1: PKL file exists (as before) ---
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir}."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created."

    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    expected_pkl_pattern = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    print(f"DEBUG: Checking for specific PKL file pattern: {expected_pkl_pattern}")
    found_files = glob.glob(expected_pkl_pattern)
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_pattern} was not found."
    print(f"DEBUG: Found PKL file(s): {found_files}")

    # --- Verification 2: Check if download/update was SKIPPED --- 
    # This relies on a previous test (like persistence test) leaving the PKL file
    # If the PKL was loaded successfully by load_from_disk at the start of the test,
    # these messages should be absent from the captured stdout.
    print("DEBUG: Verifying download/import/update was potentially skipped (if PKL existed from previous run)...")
    download_msg_found = "Downloading missing files" in stdout
    import_msg_found = "CDF Data import complete" in stdout
    update_msg_found = f"Updating {identifier}" in stdout
    refresh_msg_found = "Import/Refresh required" in stdout
    
    if download_msg_found or import_msg_found or update_msg_found or refresh_msg_found:
        # This is not necessarily a FAILURE, it just means the PKL wasn't present 
        # beforehand (e.g., first run or persistence marker wasn't used).
        # We print a warning instead of asserting False.
        pb.print_manager.warning("Test ran, but download/update messages indicate PKL wasn't pre-loaded.")
        pb.print_manager.warning(f"  - Download msg found: {download_msg_found}")
        pb.print_manager.warning(f"  - Import msg found: {import_msg_found}")
        pb.print_manager.warning(f"  - Update msg found: {update_msg_found}")
        pb.print_manager.warning(f"  - Refresh msg found: {refresh_msg_found}")
    else:
        print("DEBUG: No download/import/update messages found. PKL likely loaded successfully.")

    print("--- Finished test_pkl_saving_on_plotbot_call ---")

def test_pkl_integrity_and_cache_load(capsys): # Add capsys fixture to capture output
    """Test PKL file integrity (size) and that data is loaded from cache."""
    print("\n--- Starting test_pkl_integrity_and_cache_load ---")

    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Ensure storage is enabled
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output for this test

    # Define time range and variable
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    variable = pb.mag_rtn_4sa.br
    variable_class_name = 'mag_rtn_4sa' # Manually specify class name for path finding
    variable_subclass_name = 'br' # Manually specify subclass name for path finding

    # --- First Call: Generate and Save Data ---
    print(f"DEBUG: Calling plotbot (1st time) for trange: {trange}")
    pb.plotbot(trange, variable, 1)
    print("DEBUG: plotbot call (1st time) finished.")

    # --- File Integrity Check ---
    # Construct the expected path based on the new save_to_disk logic
    identifier = "mag_rtn_4sa" # The identifier used
    date_str = "20240929" # From the trange used
    version_str = "vXX" # Placeholder
    expected_filename = f"psp_fld_l2_mag_RTN_4_Sa_per_Cyc_{date_str}_{version_str}.pkl"
    
    # Get the expected directory components from psp_data_types (consistent with save_to_disk)
    # Path needs to include 'psp_data' as per _get_storage_path_for_object logic
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle'] 
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    # Use glob to find the actual file regardless of version
    expected_pkl_pattern = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    print(f"DEBUG: Checking for PKL file pattern at correct location: {expected_pkl_pattern}")

    found_files = glob.glob(expected_pkl_pattern)
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_pattern} was not found at the derived path."
    # Use the found file path for size check
    actual_pkl_file = found_files[0] 
    print(f"DEBUG: Found actual PKL file: {actual_pkl_file}")

    # Check file size (e.g., > 100 bytes to ensure it's not empty)
    file_size = os.path.getsize(actual_pkl_file)
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 100, f"PKL file {actual_pkl_file} size ({file_size} bytes) is too small."

    # Simulate clearing memory before reloading from disk
    pb.data_cubby.cubby.clear() # Only clear the data cache
    # pb.data_cubby.class_registry.clear() # Do not clear registries
    # pb.data_cubby.subclass_registry.clear() # Do not clear registries

    print("DEBUG: Attempting to load from disk...")
    
    # Temporarily increase recursion depth for loading potentially complex pickles
    original_recursion_limit = sys.getrecursionlimit()
    print(f"DEBUG: Original recursion limit: {original_recursion_limit}")
    sys.setrecursionlimit(max(original_recursion_limit, 2000)) # Increase if needed, e.g., to 2000
    print(f"DEBUG: Temporarily set recursion limit to: {sys.getrecursionlimit()}")
    
    load_successful = False
    try:
        load_successful = pb.data_cubby.load_from_disk()
    except Exception as e:
        print(f"DEBUG: Exception during load_from_disk: {e}")
        # Ensure recursion limit is reset even if load fails
    finally:
        sys.setrecursionlimit(original_recursion_limit) # Reset recursion limit
        print(f"DEBUG: Reset recursion limit to: {sys.getrecursionlimit()}")
        
    assert load_successful, "Failed to load data back from disk after clearing memory (even with increased recursion limit)."
    
    # --- Second Call: Should use loaded data ---
    print(f"DEBUG: Calling plotbot (2nd time) after forced reload for same trange: {trange}")
    # Capture stdout/stderr for the second call
    _ = capsys.readouterr() 
    pb.plotbot(trange, variable, 1)
    print("DEBUG: plotbot call (2nd time) finished.")

    # --- Cache Load Verification ---
    captured = capsys.readouterr()
    stdout = captured.out
    print("\n--- Captured STDOUT for 2nd plotbot call (after forced reload) ---")
    print(stdout)
    print("--- End Captured STDOUT ---")

    # Check if the data acquisition was skipped (i.e., no import/update messages)
    # We should NOT see the "CDF Data import complete" or "Updating mag_RTN_4sa" messages
    import_msg = "CDF Data import complete"
    update_msg = "Updating mag_RTN_4sa"
    
    print(f"DEBUG: Checking if import message ('{import_msg}') is ABSENT")
    assert import_msg not in stdout, f"Data import seems to have run again on the second call when it should have been cached."
    
    print(f"DEBUG: Checking if update message ('{update_msg}') is ABSENT")
    assert update_msg not in stdout, f"Data update seems to have run again on the second call when it should have been cached."

    # # Previous check (now less relevant as we forced load_from_disk):
    # grab_success_msg = f"✅ Successfully retrieved {variable_class_name}" # Check grab success
    # print(f"DEBUG: Checking for cache hit message: '{grab_success_msg}'")
    # assert grab_success_msg in stdout, f"Cache load message not found in stdout for the second call. Should have retrieved '{variable_class_name}'."

    print("--- Finished test_pkl_integrity_and_cache_load ---")

def test_pkl_file_location_persistence():
    """Test saving a file and verify its location without automatic cleanup.
    
    This test intentionally leaves the created file and directory structure 
    in place for manual inspection. It does not use the standard setup/teardown.
    """
    print("\n--- Starting test_pkl_file_location_persistence --- ")
    # Use the MAIN storage directory defined at the top of the file
    storage_dir_persist = storage_dir 
    print(f"DEBUG: Target storage directory for persistence test: {storage_dir_persist}")

    # --- Manual Setup --- 
    # Ensure MAIN directory exists, clean ONLY specific test files from previous runs
    os.makedirs(storage_dir_persist, exist_ok=True)
    index_path = os.path.join(storage_dir_persist, 'data_cubby_index.json')
    test_pkl_dir = os.path.join(storage_dir_persist, 'fields', 'l2', 'mag_rtn_4_per_cycle')
    test_pkl_pattern = os.path.join(test_pkl_dir, 'psp_fld_l2_mag_RTN_4_Sa_per_Cyc_20240929_v*.pkl')
    try: 
        if os.path.exists(index_path): os.remove(index_path)
        for f in glob.glob(test_pkl_pattern): os.remove(f)
        print(f"DEBUG: Persistence setup cleaned specific test files from {storage_dir_persist}")
    except OSError as e:
        print(f"DEBUG: Error cleaning specific test files in persistence setup: {e}")

    # --- Configure DataCubby for this test instance ---
    # Explicitly set the storage directory for this test to the MAIN one
    pb.data_cubby.set_storage_directory(storage_dir_persist)
    # Corrected debug print to use the method/property that exists
    print(f"DEBUG: Explicitly set DataCubby storage dir via method. Current base dir: {pb.data_cubby.base_pkl_directory}")

    pb.data_cubby.use_pkl_storage = True
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output

    # Corrected this debug print - accesses base_pkl_directory which exists
    print(f"DEBUG: DataCubby configured: use_pkl_storage={pb.data_cubby.use_pkl_storage}, base_pkl_directory='{pb.data_cubby.base_pkl_directory}'")

    # --- Trigger Save Operation (e.g., via plotbot) ---
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    variable = pb.mag_rtn_4sa.br # Use a variable known to trigger saving
    print(f"DEBUG: Calling plotbot for trange: {trange}")
    
    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        assert False, f"plotbot call failed during test: {e}"

    print("DEBUG: plotbot call finished.")

    # --- Verification ---
    # 1. Check if storage directory was created
    print(f"DEBUG: Verifying existence of storage directory: {storage_dir_persist}")
    assert os.path.exists(storage_dir_persist), f"Storage directory was not created at {storage_dir_persist}."

    # 2. Check if the index file was created
    index_path = os.path.join(storage_dir_persist, 'data_cubby_index.json')
    print(f"DEBUG: Verifying existence of index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created."

    # 3. Check if the specific PKL file was created with the correct name
    # Construct the expected filename pattern based on save_to_disk logic
    identifier = "mag_rtn_4sa" # The identifier used in the plotbot call
    date_str = "20240929" # From the trange used
    # Use glob pattern to find the file regardless of version
    expected_filename_pattern = f"psp_fld_l2_mag_rtn_4_sa_per_cyc_{date_str}_v*.pkl"
    
    # For mag_rtn_4sa, path_components should include 'psp_data'
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir_persist, *expected_pkl_dir_components)
    expected_pkl_file_pattern = os.path.join(expected_pkl_dir, expected_filename_pattern) # MODIFIED: Use pattern variable
    
    print(f"DEBUG: Verifying existence of PKL file pattern: {expected_pkl_file_pattern}") # MODIFIED: Print pattern
    found_files = glob.glob(expected_pkl_file_pattern) # MODIFIED: Use glob with pattern
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_file_pattern} was not found." # MODIFIED: Assert based on found_files
    actual_pkl_file = found_files[0] # Use the first found file
    print(f"DEBUG: Found actual PKL file: {actual_pkl_file}") # Log the found file

    # 4. Optional: Check file size (basic integrity)
    file_size = os.path.getsize(actual_pkl_file) # MODIFIED: Use actual_pkl_file
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 100, f"PKL file {actual_pkl_file} size ({file_size} bytes) seems too small."

    print(f"--- Test Passed: PKL file created at {actual_pkl_file}. Leaving artifacts for inspection. ---")
    
    # --- Create marker file IN MAIN DIR to prevent teardown for this specific test --- 
    marker_filename = ".persist_marker"
    marker_path = os.path.join(storage_dir_persist, marker_filename) # Path is now in MAIN dir
    try:
        # Ensure the directory exists before trying to create the file inside it
        os.makedirs(storage_dir_persist, exist_ok=True)
        with open(marker_path, 'w') as f:
            f.write('Persist this run\n')
        print(f"DEBUG: Created persistence marker file: {marker_path}")
    except OSError as e:
        print(f"DEBUG: Failed to create persistence marker file {marker_path}: {e}")
        # If marker fails, teardown might run, warn the user.
        print_manager.warning(f"Could not create marker file; teardown might remove {storage_dir_persist}")

    print("--- Finished test_pkl_file_location_persistence ---")

def test_load_from_disk_after_stash():
    # ... existing code ...
    pass # Placeholder if the function is empty for now

def test_pkl_saving_full_day():
    """Test if enabling pkl storage triggers saving correctly for a full day's range."""
    print("\n--- Starting test_pkl_saving_full_day ---")
    
    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True # Enable debug output

    # Define FULL DAY time range
    trange = ['2024-09-29/00:00:00.000', '2024-09-29/23:59:59.999']
    variable = pb.mag_rtn_4sa.br # Variable to plot
    print(f"DEBUG: Calling plotbot for FULL DAY trange: {trange}")

    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        assert False, f"plotbot call failed during test: {e}"

    print("DEBUG: plotbot call finished for full day.")

    # --- Verification ---
    # Assert that the storage directory and index file were created
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir} after plotbot call."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created after plotbot call."

    # Check for the specific pkl file for mag_rtn_4sa in the correct subfolder
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    # Use glob to check for the versioned file (lowercase name)
    expected_pkl_pattern = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    print(f"DEBUG: Checking for specific PKL file pattern: {expected_pkl_pattern}")
    found_files = glob.glob(expected_pkl_pattern)
    
    assert len(found_files) > 0, f"Expected PKL file pattern {expected_pkl_pattern} was not found."
    print(f"DEBUG: Found PKL file(s): {found_files}")
    
    # Check size of the found file
    actual_pkl_file = found_files[0]
    file_size = os.path.getsize(actual_pkl_file)
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 100, f"PKL file {actual_pkl_file} size ({file_size} bytes) seems too small."

    print("--- Finished test_pkl_saving_full_day ---")

def test_pkl_saving_two_days():
    """Test if PKL storage saves separate files correctly for a two-day range."""
    print("\n--- Starting test_pkl_saving_two_days ---")
    
    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True

    # Define TWO DAY time range
    trange = ['2024-09-28/00:00:00.000', '2024-09-29/23:59:59.999']
    variable = pb.mag_rtn_4sa.br
    print(f"DEBUG: Calling plotbot for TWO DAY trange: {trange}")

    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        # If plotbot fails here, check data availability for 2024-09-28
        assert False, f"plotbot call failed during test: {e}" 

    print("DEBUG: plotbot call finished for two days.")

    # --- Verification ---
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir}."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created."

    # Check for the two specific pkl files
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn_4_per_cycle']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)
    
    # Define patterns for both days
    pattern_day1 = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240928_v*.pkl')
    pattern_day2 = os.path.join(expected_pkl_dir, 'psp_fld_l2_mag_rtn_4_sa_per_cyc_20240929_v*.pkl')
    
    print(f"DEBUG: Checking for Day 1 PKL file pattern: {pattern_day1}")
    found_day1 = glob.glob(pattern_day1)
    assert len(found_day1) > 0, f"Expected PKL file pattern for Day 1 ({pattern_day1}) was not found."
    print(f"DEBUG: Found Day 1 PKL file(s): {found_day1}")
    
    print(f"DEBUG: Checking for Day 2 PKL file pattern: {pattern_day2}")
    found_day2 = glob.glob(pattern_day2)
    assert len(found_day2) > 0, f"Expected PKL file pattern for Day 2 ({pattern_day2}) was not found."
    print(f"DEBUG: Found Day 2 PKL file(s): {found_day2}")

    # Check size of both found files
    actual_pkl_day1 = found_day1[0]
    size_day1 = os.path.getsize(actual_pkl_day1)
    print(f"DEBUG: Day 1 PKL file size: {size_day1} bytes")
    assert size_day1 > 100, f"Day 1 PKL file {actual_pkl_day1} size ({size_day1} bytes) seems too small."

    actual_pkl_day2 = found_day2[0]
    size_day2 = os.path.getsize(actual_pkl_day2)
    print(f"DEBUG: Day 2 PKL file size: {size_day2} bytes")
    assert size_day2 > 100, f"Day 2 PKL file {actual_pkl_day2} size ({size_day2} bytes) seems too small."

    print("--- Finished test_pkl_saving_two_days ---")

def test_pkl_saving_mag_rtn_hourly():
    """Test if PKL storage saves correctly for mag_rtn (6-hourly files)."""
    print("\n--- Starting test_pkl_saving_mag_rtn_hourly ---")

    # Ensure DataCubby uses the main storage directory
    pb.data_cubby.set_storage_directory(storage_dir)
    print(f"DEBUG: Ensured storage directory is set to MAIN: {storage_dir}")

    # Enable PKL storage
    print("DEBUG: Setting use_pkl_storage = True")
    pb.data_cubby.use_pkl_storage = True
    assert pb.data_cubby.use_pkl_storage is True, "Failed to enable PKL storage"

    # Enable status prints
    pb.print_manager.show_status = True
    pb.print_manager.show_debug = True

    # Define 1-hour time range
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/13:10:00.000']
    variable = pb.mag_rtn.br # Use mag_rtn data type
    print(f"DEBUG: Calling plotbot for mag_rtn 1-hour trange: {trange}")

    try:
        pb.plotbot(trange, variable, 1)
    except Exception as e:
        print(f"DEBUG: plotbot call failed with error: {e}")
        assert False, f"plotbot call failed during test: {e}"

    print("DEBUG: plotbot call finished for mag_rtn 1-hour.")

    # --- Verification ---
    assert os.path.exists(storage_dir), f"Storage directory was not created at {storage_dir}."
    index_path = os.path.join(storage_dir, 'data_cubby_index.json')
    print(f"DEBUG: Checking for index file: {index_path}")
    assert os.path.exists(index_path), "data_cubby_index.json was not created."

    # Check for the specific pkl file for mag_rtn
    # Path derived from psp_data_types.py for 'mag_RTN'
    expected_pkl_dir_components = ['psp_data', 'fields', 'l2', 'mag_rtn']
    expected_pkl_dir = os.path.join(storage_dir, *expected_pkl_dir_components)

    # Expected filename pattern based on the 12:00 6-hour file
    expected_filename_base = 'psp_fld_l2_mag_RTN_2024092912' # date + hour block - Use Uppercase RTN
    # Construct glob pattern using the case derived name (consistent with save_to_disk for mag_RTN)
    # MODIFIED: Use Uppercase RTN in the pattern
    expected_pkl_pattern = os.path.join(expected_pkl_dir, f'{expected_filename_base}_v*.pkl')

    print(f"DEBUG: Checking for mag_rtn PKL file pattern: {expected_pkl_pattern}")
    found_files = glob.glob(expected_pkl_pattern)

    assert len(found_files) > 0, f"Expected PKL file pattern for mag_rtn ({expected_pkl_pattern}) was not found."
    print(f"DEBUG: Found mag_rtn PKL file(s): {found_files}")

    # Check size of the found file
    actual_pkl_file = found_files[0]
    file_size = os.path.getsize(actual_pkl_file)
    print(f"DEBUG: PKL file size: {file_size} bytes")
    assert file_size > 100, f"PKL file {actual_pkl_file} size ({file_size} bytes) seems too small."

    print("--- Finished test_pkl_saving_mag_rtn_hourly ---")

# Simple manual test - execute when file is run directly
if __name__ == "__main__":
    print("Running data_cubby_pkl_saving tests directly...")
    initial_value = pb.data_cubby._use_pkl_storage
    print(f"Initial pickle storage setting: {initial_value}")
    
    # Set the variable 
    pb.data_cubby.use_pkl_storage = True
    print(f"After setting, pickle storage setting: {pb.data_cubby.use_pkl_storage}")
    
    # Try a simple plotbot call
    trange = ['2024-09-29/12:10:00.000', '2024-09-29/12:11:00.000']
    print(f"Running plotbot call with trange: {trange}")
    pb.plotbot(trange, pb.mag_rtn_4sa.br, 1)
    
    # Check setting after the call
    print(f"Final pickle storage setting: {pb.data_cubby.use_pkl_storage}")
