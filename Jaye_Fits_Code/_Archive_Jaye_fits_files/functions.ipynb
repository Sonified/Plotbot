{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc2ef2-2230-4c75-835e-22b593fea72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['mathtext.fontset'] = 'stix' #resolves a warning for symbol interpretation\n",
    "\n",
    "#-----Plotpbot Options-----\\\n",
    "\n",
    "plotbot_custom_options = {}\n",
    "\n",
    "class PlotbotManager:\n",
    "    def __init__(self):\n",
    "        self.custom_options = {}\n",
    "\n",
    "    def set_option(self, variable, option, value):\n",
    "        if variable not in self.custom_options:\n",
    "            self.custom_options[variable] = {}\n",
    "        self.custom_options[variable][option] = value\n",
    "\n",
    "plotbot_manager = PlotbotManager()\n",
    "\n",
    "def ploptions(variable, option, value):\n",
    "    plotbot_manager.set_option(variable, option, value)\n",
    "\n",
    "#-----Other Functions-----\\\n",
    "\n",
    "def time_clip(tarray, start, stop): #Currently called within Plotbot\n",
    "    #input time array in date_time format\n",
    "    #input start and stop times in '%Y-%m-%d/%H:%M:%S.%f format\n",
    "\n",
    "    # Ensure datetime_spe is a numpy array\n",
    "    tarray_np = np.array(tarray)\n",
    "    start_dt = parse(start).replace(tzinfo=timezone.utc)\n",
    "    stop_dt = parse(stop).replace(tzinfo=timezone.utc)\n",
    "\n",
    "    return np.where((tarray_np >= start_dt) & (tarray_np <= stop_dt))[0] #outputs the subset of indexes for which that is true\n",
    "\n",
    "def apply_time_range(trange, datetime_array, *data_arrays): #Applies the time range to the data\n",
    "    datetime_array = np.array(datetime_array)\n",
    "    time_indices = time_clip(datetime_array, trange[0], trange[1])\n",
    "    datetime_clipped = datetime_array[time_indices]\n",
    "    data_arrays_clipped = [np.array(data_array)[time_indices] for data_array in data_arrays]\n",
    "    return [datetime_clipped] + data_arrays_clipped\n",
    "\n",
    "def parse_axis_spec(spec): #Parses the axis specification for the plotbot function\n",
    "    if isinstance(spec, int):\n",
    "        return spec, False\n",
    "    elif isinstance(spec, str) and spec.endswith('r'):\n",
    "        return int(spec[:-1]), True\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid axis specification: {spec}\")\n",
    "    \n",
    "def resample(data, times, new_times): #Currently unused\n",
    "    ###interpolate data to times from data2\n",
    "    interpol_f = interpolate.interp1d(times, data,fill_value=\"extrapolate\")\n",
    "    new_data1 = interpol_f(new_times)    \n",
    "    return new_times, new_data1\n",
    "\n",
    "\n",
    "#FITS Code Functions\n",
    "#Functions:\n",
    "#define functions\n",
    "def resample(data, times, new_times):\n",
    "    ###interpolate data to times from data2\n",
    "    interpol_f = interpolate.interp1d(times, data,fill_value=\"extrapolate\")\n",
    "    new_data1 = interpol_f(new_times)    \n",
    "    return new_times, new_data1\n",
    "\n",
    "def convert_time(start, stop):\n",
    "    #input start and stop times as 'yyyy-mm-dd/hh:mm:ss'\n",
    "    #return dataframe at those indices\n",
    "    tindex_sf00 = np.where((df_sf00['datetime'] > start) & (df_sf00['datetime'] < stop))\n",
    "    return df_sf00.iloc[tindex_sf00]\n",
    "\n",
    "# def time_clip(tarray,start,stop): #NOTE this original funciton was simpler, could cause issues?\n",
    "#     #input time array in unix time format\n",
    "#     #input start and stop times as 'yyyy-mm-dd/hh:mm:ss'\n",
    "#     #return indices\n",
    "#     ind = np.logical_and(tarray>= time_double(start),tarray <= time_double(stop))\n",
    "#     return ind\n",
    "\n",
    "def downsample_time_based(x_time, x_values, target_times):\n",
    "    \"\"\"Interpolate x_values to match target_times.\"\"\"\n",
    "    # Convert datetime arrays to numeric timestamps\n",
    "    x_time_numeric = np.array([dt.timestamp() for dt in x_time])\n",
    "    target_times_numeric = np.array([dt.timestamp() for dt in target_times])\n",
    "\n",
    "    f = interpolate.interp1d(\n",
    "        x_time_numeric, x_values,\n",
    "        kind='linear',\n",
    "        bounds_error=False,\n",
    "        fill_value='extrapolate'\n",
    "    )\n",
    "    new_values = f(target_times_numeric)\n",
    "    return new_values\n",
    "\n",
    "\n",
    "\n",
    "# Function to downsample x_time and x_values to match target_times within a tolerance\n",
    "\n",
    "# Function to downsample x_time and x_values to match target_times within a tolerance\n",
    "def downsample_to_match(x_time, x_values, target_times):\n",
    "    # Convert datetime arrays to numeric timestamps\n",
    "    x_time_numeric = np.array([dt.timestamp() for dt in x_time])\n",
    "    target_times_numeric = np.array([dt.timestamp() for dt in target_times])\n",
    "    tolerance = target_times_numeric[1] - target_times_numeric[0]\n",
    "\n",
    "    # Sort x_time and keep track of the original indices\n",
    "    sorted_indices = np.argsort(x_time_numeric)\n",
    "    x_time_sorted = x_time_numeric[sorted_indices]\n",
    "    x_values_sorted = x_values[sorted_indices]\n",
    "    \n",
    "    #downsampled_x_time = []\n",
    "    downsampled_x_values = []\n",
    "\n",
    "    # Function to find the closest match in x_time for a single element in target_times\n",
    "    def find_closest_within_tolerance(x):\n",
    "        # Binary search to find the closest index\n",
    "        pos = np.searchsorted(x_time_sorted, x)\n",
    "        closest_idx = None\n",
    "        min_diff = float('inf')\n",
    "        \n",
    "        # Check the position and its neighbors\n",
    "        if pos > 0 and abs(x_time_sorted[pos - 1] - x) < min_diff:\n",
    "            min_diff = abs(x_time_sorted[pos - 1] - x)\n",
    "            closest_idx = pos - 1\n",
    "        if pos < len(x_time_sorted) and abs(x_time_sorted[pos] - x) < min_diff:\n",
    "            min_diff = abs(x_time_sorted[pos] - x)\n",
    "            closest_idx = pos\n",
    "        \n",
    "        # Return the closest index if within tolerance\n",
    "        return closest_idx if closest_idx is not None and min_diff < tolerance else None\n",
    "\n",
    "    # Iterate through target_times and find closest matches in x_time\n",
    "    for x in target_times_numeric:\n",
    "        match_idx = find_closest_within_tolerance(x)\n",
    "        if match_idx is not None:\n",
    "            #downsampled_x_time.append(x_time_sorted[match_idx])\n",
    "            downsampled_x_values.append(x_values_sorted[match_idx])\n",
    "        else:\n",
    "            # Fill with NaN for no match\n",
    "            #downsampled_x_time.append(np.nan)        \n",
    "            downsampled_x_values.append(np.nan)        \n",
    "\n",
    "    #return np.array(downsampled_x_time), np.array(downsampled_x_values)\n",
    "    return np.array(downsampled_x_values)\n",
    "\n",
    "    \n",
    "# Perform the downsampling\n",
    "#A tolerance of 1 finds timestamp to nearest second, which works when comparing between sf00, sf0a, spe, waves. \n",
    "#may need to adjust to smaller tolerance if comparing electric field and mag for example, since comparison would rely on sub-seconds\n",
    "#tolerance = 1e0 \n",
    "#x_time = datetime_spi\n",
    "#x_values = vmag\n",
    "#df_sf00['datetime'] = pd.to_datetime(df_sf00['time'], unit='s', utc=True)\n",
    "# Convert 'datetime' column to NumPy array of datetime.datetime objects\n",
    "#target_times = df_sf00['datetime'].dt.to_pydatetime()\n",
    "#downsampled_x_time, downsampled_x_values = downsample_to_match(x_time, x_values, target_times, tolerance=tolerance)\n",
    "\n",
    "# Output results\n",
    "#print(f\"Downsampled x_time to match x_time: {downsampled_x_time}\")\n",
    "#print(f\"Downsampled x_values associated with x_time: {downsampled_x_values}\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to downsample x_time and x_values to match target_times \n",
    "def downsample_to_min_ind(x_time, x_values, target_times):\n",
    "    # Convert datetime arrays to numeric timestamps\n",
    "    x_time_numeric = np.array([dt.timestamp() for dt in x_time])\n",
    "    target_times_numeric = np.array([dt.timestamp() for dt in target_times])    \n",
    "    x_values_ds = []\n",
    "    for target_times_idx in range(len(target_times_numeric)): \n",
    "        i = np.argmin(np.abs(x_time_numeric - target_times_numeric[target_times_idx]))\n",
    "        x_values_ds.append(x_values[i])\n",
    "    return np.asarray(x_values_ds)\n",
    "\n",
    "\n",
    "def upsample_to_match(x_times, x_values, target_times):\n",
    "    x_times_numeric = np.array([dt.timestamp() for dt in x_times])   \n",
    "    target_times_numeric = np.array([dt.timestamp() for dt in target_times]) \n",
    "    all_values = np.zeros_like(target_times_numeric) + np.nanmin(x_values[np.nonzero(x_values)])*5e-1\n",
    "    #all_values = np.zeros_like(target_times_numeric) + np.nanmin(x_values[np.nonzero(x_values)])*np.nan\n",
    "\n",
    "    for x_idx, x_time in enumerate(x_times_numeric):\n",
    "        target_idx = np.argmin(np.abs(target_times_numeric - x_time))\n",
    "        all_values[target_idx] = x_values[x_idx]\n",
    "    return np.asarray(all_values)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def upsample_to_match2(data, x_times, target_times, fill_strategy='nan', labels=None, min_value=None):\n",
    "    \"\"\"\n",
    "    Upsample sparse data to a regular time grid.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame, dict of arrays, or array (1D or 2D)\n",
    "    - x_times: list/array of datetime objects (same length as data rows)\n",
    "    - target_times: list/array of datetime objects (desired time grid)\n",
    "    - fill_strategy: 'nan' (default) or 'minscale'\n",
    "    - labels: optional list of column names if data is a plain array\n",
    "    - min_value: optional fill value for 'minscale' strategy\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame indexed by target_times\n",
    "    \"\"\"\n",
    "    x_times_numeric = np.array([dt.timestamp() for dt in x_times])\n",
    "    target_times_numeric = np.array([dt.timestamp() for dt in target_times])\n",
    "    upsampled_data = {}\n",
    "\n",
    "    # Detect input format\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data_items = data.items()\n",
    "    elif isinstance(data, dict):\n",
    "        data_items = data.items()\n",
    "    elif isinstance(data, (list, np.ndarray)):\n",
    "        data = np.asarray(data)\n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(-1, 1)\n",
    "        if labels is None:\n",
    "            labels = [f\"var_{i}\" for i in range(data.shape[1])]\n",
    "        data_items = zip(labels, data.T)\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported data type for 'data'.\")\n",
    "\n",
    "    for label, x_values in data_items:\n",
    "        x_values = np.asarray(x_values)\n",
    "\n",
    "        # Support multi-dimensional variables\n",
    "        if x_values.ndim == 1:\n",
    "            x_values = x_values[:, np.newaxis]\n",
    "\n",
    "        for i in range(x_values.shape[1]):\n",
    "            col_label = f\"{label}_{i}\" if x_values.shape[1] > 1 else label\n",
    "            column_data = x_values[:, i]\n",
    "\n",
    "            # If it's datetime-like, just use target_times directly\n",
    "            is_datetime = (\n",
    "                np.issubdtype(column_data.dtype, np.datetime64)\n",
    "                or isinstance(column_data[0], (pd.Timestamp, np.datetime64))\n",
    "            )\n",
    "            if is_datetime:\n",
    "                upsampled_data[col_label] = target_times\n",
    "                continue\n",
    "\n",
    "            # Choose fill value\n",
    "            if fill_strategy == 'nan':\n",
    "                fill_value = np.nan\n",
    "            elif fill_strategy == 'minscale':\n",
    "                if min_value is not None:\n",
    "                    fill_value = min_value\n",
    "                else:\n",
    "                    min_nonzero = np.nanmin(column_data[np.nonzero(column_data)])\n",
    "                    fill_value = min_nonzero * 0.5\n",
    "            else:\n",
    "                raise ValueError(\"fill_strategy must be 'nan' or 'minscale'\")\n",
    "\n",
    "            # Fill array and insert values at correct indices\n",
    "            filled_array = np.full(target_times_numeric.shape, fill_value, dtype=np.float64)\n",
    "\n",
    "            for x_idx, x_time in enumerate(x_times_numeric):\n",
    "                target_idx = np.argmin(np.abs(target_times_numeric - x_time))\n",
    "                filled_array[target_idx] = column_data[x_idx]\n",
    "\n",
    "            upsampled_data[col_label] = filled_array\n",
    "\n",
    "    return pd.DataFrame(upsampled_data, index=target_times)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "#example use\n",
    "#upsampled_df = upsample_data(\n",
    "    #data=df_sf01,\n",
    "    #x_times=datetime_array_sf01,\n",
    "    #target_times=datetime_spi_sf0a,\n",
    "    #fill_strategy='minscale',\n",
    "    #min_value=1e-3  # optional, can be left out\n",
    "#)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_nans_phi_fov(var, thresh = 163.125):\n",
    "    #input variable, \n",
    "    #output variable with insufficent fov phi angle coverage indices filled as nans\n",
    "    \n",
    "    #Define insufficent phi angle coverage threshold\n",
    "    #phi_thresh = spi_nrg_vals_phi[0,1] #163.125 degrees, \n",
    "    #note this is very conservative, one could also go up to 165 or even 168 (with care)\n",
    "\n",
    "    #Find the boolean mask where centroids_spi_phi is greater than or equal to phi_thresh\n",
    "    #Note, could instead use the maximum phi flux value, rather than weighted mean. Unsure which is better.\n",
    "    mask = centroids_spi_phi >= thresh\n",
    "\n",
    "    # Replace the corresponding indices in variable with NaNs, so they will not plot\n",
    "    var = np.array(var) #convert to numpy array\n",
    "    var[mask] = np.nan\n",
    "    return var\n",
    "\n",
    "\n",
    "def apply_nans_phi_fov_pb(var, thresh = 163.125):\n",
    "    #input plotbot variable, \n",
    "    #output plotbot variable named plotbotvariable with insufficent fov phi angle coverage indices filled as nans\n",
    "    \n",
    "    #Define insufficent phi angle coverage threshold\n",
    "    #phi_thresh = spi_nrg_vals_phi[0,1] #163.125 degrees, \n",
    "    #note this is very conservative, one could also go up to 165 or even 168 (with care)\n",
    "\n",
    "    #Find the boolean mask where centroids_spi_phi is greater than or equal to phi_thresh\n",
    "    #Note, could instead use the maximum phi flux value, rather than weighted mean. Unsure which is better.\n",
    "    mask = centroids_spi_phi >= thresh\n",
    "\n",
    "    # Replace the corresponding indices in variable with NaNs, so they will not plot\n",
    "    var = np.array(var) #convert to numpy array\n",
    "    var[mask] = np.nan\n",
    "    return var\n",
    "\n",
    "#define function to compute correlation coefficient between 2 variables \n",
    "#uses masked_invalid to avoid nans\n",
    "def corr_nan(A,B):\n",
    "    a=ma.masked_invalid(A)\n",
    "    b=ma.masked_invalid(B)\n",
    "\n",
    "    msk = (~a.mask & ~b.mask)\n",
    "    corr_matrix = ma.corrcoef(a[msk],b[msk])\n",
    "    corr_matrix.data\n",
    "    return corr_matrix.data[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4e08d-5814-4303-b256-1abc79b64879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fov_filter(\n",
    "    keys,\n",
    "    centroid_proton=None,\n",
    "    threshold_proton=None,\n",
    "    centroid_alpha=None,\n",
    "    threshold_alpha=None,\n",
    "    suffix=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Flexible filtering by centroid threshold(s). Supports proton-only, alpha-only, or both.\n",
    "    Returns list of new variable keys added to zipped_data.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if isinstance(keys, str):\n",
    "        keys = [keys]\n",
    "\n",
    "    new_keys = []\n",
    "\n",
    "    for key in keys:\n",
    "        if key not in zipped_data:\n",
    "            print(f\"⚠️ '{key}' not in zipped_data, skipping.\")\n",
    "            continue\n",
    "\n",
    "        entry = zipped_data[key]\n",
    "        plot_type = entry[0]\n",
    "        data_arrays = [np.asarray(arr) for arr in entry[1]]\n",
    "        var_names = entry[2]\n",
    "        time_array = np.asarray(entry[3])\n",
    "        ylabels = entry[4]\n",
    "        legends = entry[5]\n",
    "        colors = entry[6]\n",
    "\n",
    "        # Generate mask based on whichever centroids are provided\n",
    "        mask = np.full_like(time_array, True, dtype=bool)\n",
    "        key_suffix_parts = []\n",
    "        legend_suffix_parts = []\n",
    "\n",
    "        if centroid_proton is not None and threshold_proton is not None:\n",
    "            if len(centroid_proton) != len(time_array):\n",
    "                print(f\"⚠️ Proton centroid length mismatch for '{key}', skipping.\")\n",
    "                continue\n",
    "            mask &= centroid_proton <= threshold_proton\n",
    "            key_suffix_parts.append(f\"p_le{threshold_proton}\")\n",
    "            legend_suffix_parts.append(f\"p ≤ {threshold_proton}°\")\n",
    "\n",
    "        if centroid_alpha is not None and threshold_alpha is not None:\n",
    "            if len(centroid_alpha) != len(time_array):\n",
    "                print(f\"⚠️ Alpha centroid length mismatch for '{key}', skipping.\")\n",
    "                continue\n",
    "            mask &= centroid_alpha <= threshold_alpha\n",
    "            key_suffix_parts.append(f\"a_le{threshold_alpha}\")\n",
    "            legend_suffix_parts.append(f\"α ≤ {threshold_alpha}°\")\n",
    "\n",
    "        if np.all(mask == True):\n",
    "            print(f\"⚠️ No filtering applied to '{key}' (mask is all True).\")\n",
    "        elif not np.any(mask):\n",
    "            print(f\"⚠️ No data left after filtering '{key}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        filtered_data = [arr[mask] for arr in data_arrays]\n",
    "        filtered_time = time_array[mask]\n",
    "\n",
    "        # Build key and updated legends\n",
    "        key_suffix = \"_\" + \"_\".join(key_suffix_parts) if suffix and key_suffix_parts else \"\"\n",
    "        new_key = f\"{key}{key_suffix}\"\n",
    "        legends = [f\"{l} ({', '.join(legend_suffix_parts)})\" for l in legends]\n",
    "\n",
    "        # Rebuild entry based on plot type\n",
    "        if plot_type == \"time_series\":\n",
    "            zipped_data[new_key] = (\n",
    "                plot_type,\n",
    "                filtered_data,\n",
    "                var_names,\n",
    "                filtered_time,\n",
    "                ylabels,\n",
    "                legends,\n",
    "                colors,\n",
    "                entry[7], entry[8], entry[9], entry[10],\n",
    "                entry[11], entry[12], entry[13], entry[14]\n",
    "            )\n",
    "        elif plot_type == \"scatter\":\n",
    "            zipped_data[new_key] = (\n",
    "                plot_type,\n",
    "                filtered_data,\n",
    "                var_names,\n",
    "                filtered_time,\n",
    "                ylabels,\n",
    "                legends,\n",
    "                colors,\n",
    "                entry[7], entry[8], entry[9],\n",
    "                entry[10], entry[11]\n",
    "            )\n",
    "        else:\n",
    "            print(f\"⚠️ Unsupported plot type '{plot_type}' for '{key}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"✅ Filtered '{key}' ➜ '{new_key}' with legend updated.\")\n",
    "        new_keys.append(new_key)\n",
    "\n",
    "    return new_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef31cc-375d-414e-9f57-58eea045d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "filter_metadata = {}\n",
    "_filter_counter = [1]  # Mutable counter to generate unique suffixes\n",
    "_filter_lookup = {}  # Maps parameter hash to filter index\n",
    "\n",
    "\n",
    "def filter_variable(\n",
    "    zipped_data,\n",
    "    keys,\n",
    "    centroid_proton=None,\n",
    "    threshold_proton=None,\n",
    "    centroid_alpha=None,\n",
    "    threshold_alpha=None,\n",
    "    uncertainty_thresholds_proton=None,\n",
    "    uncertainty_thresholds_alpha=None,\n",
    "    additional_thresholds=None,  # e.g., {\"np2\": (\">=\", \"2*'dens_spi'\")}\n",
    "    mode=\"proton\",\n",
    "    store_metadata=True\n",
    "):\n",
    "    if isinstance(keys, str):\n",
    "        keys = [keys]\n",
    "\n",
    "    new_keys = []\n",
    "\n",
    "    # --- Generate a hash of the filtering criteria ---\n",
    "    def sort_nested(d):\n",
    "        if not d:\n",
    "            return None\n",
    "        return {k: d[k] if not isinstance(d[k], dict) else sort_nested(d[k]) for k in sorted(d)}\n",
    "\n",
    "    sorted_uncertainty_proton = sort_nested(uncertainty_thresholds_proton)\n",
    "    sorted_uncertainty_alpha = sort_nested(uncertainty_thresholds_alpha)\n",
    "    sorted_additional = sort_nested(additional_thresholds)\n",
    "\n",
    "    filter_params = {\n",
    "        'centroid_proton': float(threshold_proton) if threshold_proton is not None else None,\n",
    "        'centroid_alpha': float(threshold_alpha) if threshold_alpha is not None else None,\n",
    "        'uncertainty_thresholds_proton': sorted_uncertainty_proton,\n",
    "        'uncertainty_thresholds_alpha': sorted_uncertainty_alpha,\n",
    "        'additional_thresholds': sorted_additional,\n",
    "        'mode': mode\n",
    "    }\n",
    "    filter_str = json.dumps(filter_params, sort_keys=True)\n",
    "    filter_hash = hashlib.md5(filter_str.encode()).hexdigest()\n",
    "\n",
    "    # --- Reuse or assign a filter index ---\n",
    "    if filter_hash in _filter_lookup:\n",
    "        filter_index = _filter_lookup[filter_hash]\n",
    "    else:\n",
    "        filter_index = _filter_counter[0]\n",
    "        _filter_counter[0] += 1\n",
    "        _filter_lookup[filter_hash] = filter_index\n",
    "\n",
    "    for key in keys:\n",
    "        if key not in zipped_data:\n",
    "            print(f\"⚠️ '{key}' not in zipped_data, skipping.\")\n",
    "            continue\n",
    "\n",
    "        entry = zipped_data[key]\n",
    "        plot_type = entry[0]\n",
    "        data_arrays = [np.asarray(arr) for arr in entry[1]]\n",
    "        var_names = entry[2]\n",
    "        time_array = np.asarray(entry[3])\n",
    "        ylabels = entry[4]\n",
    "        legends = entry[5]\n",
    "        colors = entry[6]\n",
    "\n",
    "        mask = np.full_like(time_array, True, dtype=bool)\n",
    "        metadata = {\n",
    "            'centroid_proton': None,\n",
    "            'centroid_alpha': None,\n",
    "            'uncertainty_thresholds_proton': None,\n",
    "            'uncertainty_thresholds_alpha': None,\n",
    "            'additional_thresholds': None,\n",
    "            'mode': mode,\n",
    "        }\n",
    "\n",
    "        if mode in ('proton', 'both') and centroid_proton is not None and threshold_proton is not None:\n",
    "            if len(centroid_proton) != len(time_array):\n",
    "                print(f\"⚠️ Proton centroid length mismatch for '{key}', skipping proton centroid filter.\")\n",
    "            else:\n",
    "                mask &= centroid_proton <= threshold_proton\n",
    "                metadata['centroid_proton'] = threshold_proton\n",
    "\n",
    "        if mode in ('alpha', 'both') and centroid_alpha is not None and threshold_alpha is not None:\n",
    "            if len(centroid_alpha) != len(time_array):\n",
    "                print(f\"⚠️ Alpha centroid length mismatch for '{key}', skipping alpha centroid filter.\")\n",
    "            else:\n",
    "                mask &= centroid_alpha <= threshold_alpha\n",
    "                metadata['centroid_alpha'] = threshold_alpha\n",
    "\n",
    "        def apply_uncertainty_filter(thresholds_dict, particle_type):\n",
    "            umask = np.full_like(time_array, True, dtype=bool)\n",
    "            for param, thresh in thresholds_dict.items():\n",
    "                val_key = param\n",
    "                err_key = f\"{param}_delta\"\n",
    "                if val_key not in zipped_data or err_key not in zipped_data:\n",
    "                    print(f\"⚠️ Missing value or uncertainty for '{param}', skipping.\")\n",
    "                    continue\n",
    "                val_arr = np.asarray(zipped_data[val_key][1][0])\n",
    "                err_arr = np.asarray(zipped_data[err_key][1][0])\n",
    "\n",
    "                if len(val_arr) != len(time_array) or len(err_arr) != len(time_array):\n",
    "                    print(f\"⚠️ Length mismatch for '{param}', skipping.\")\n",
    "                    continue\n",
    "\n",
    "                rel_unc = np.abs(err_arr / val_arr)\n",
    "                umask &= rel_unc <= thresh\n",
    "            return umask\n",
    "\n",
    "        if mode in ('proton', 'both') and uncertainty_thresholds_proton:\n",
    "            pmask = apply_uncertainty_filter(uncertainty_thresholds_proton, 'proton')\n",
    "            mask &= pmask\n",
    "            metadata['uncertainty_thresholds_proton'] = uncertainty_thresholds_proton\n",
    "\n",
    "        if mode in ('alpha', 'both') and uncertainty_thresholds_alpha:\n",
    "            amask = apply_uncertainty_filter(uncertainty_thresholds_alpha, 'alpha')\n",
    "            mask &= amask\n",
    "            metadata['uncertainty_thresholds_alpha'] = uncertainty_thresholds_alpha\n",
    "\n",
    "        # --- Additional threshold filtering ---\n",
    "        if metadata['additional_thresholds'] is None:\n",
    "            metadata['additional_thresholds'] = {}\n",
    "        if additional_thresholds:\n",
    "            logic_mask = np.full_like(time_array, True, dtype=bool)\n",
    "            for param, (op, compare_expr) in additional_thresholds.items():\n",
    "                if param not in zipped_data:\n",
    "                    print(f\"⚠️ '{param}' not found for additional thresholding, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    eval_context = {\"np\": np, \"zipped_data\": zipped_data}\n",
    "                    expr = compare_expr\n",
    "                    import re\n",
    "                    matches = re.findall(r\"'([a-zA-Z0-9_]+)'\", expr)\n",
    "                    def var_replacer(match):\n",
    "                        varname = match.group(1)\n",
    "                        if varname in zipped_data:\n",
    "                            eval_context[varname] = np.asarray(zipped_data[varname][1][0])\n",
    "                            return varname\n",
    "                        return match.group(0)\n",
    "                    expr = re.sub(r\"'([a-zA-Z0-9_]+)'\", var_replacer, expr)\n",
    "                    result = eval(expr, eval_context)\n",
    "                    compare_arr = np.asarray(result)\n",
    "                    if compare_arr.ndim == 0:\n",
    "                        compare_arr = np.full_like(time_array, compare_arr.item(), dtype=float)\n",
    "                    param_arr = np.asarray(zipped_data[param][1][0])\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error evaluating threshold expression '{compare_expr}' for '{param}': {e}\")\n",
    "                    continue\n",
    "\n",
    "                if len(compare_arr) != len(time_array) or len(param_arr) != len(time_array):\n",
    "                    print(f\"⚠️ Length mismatch for additional threshold '{param}', skipping.\")\n",
    "                    continue\n",
    "\n",
    "                current_mask = np.full_like(time_array, True, dtype=bool)\n",
    "                if op == '>=':\n",
    "                    current_mask = param_arr >= compare_arr\n",
    "                elif op == '<=':\n",
    "                    current_mask = param_arr <= compare_arr\n",
    "                elif op == '>':\n",
    "                    current_mask = param_arr > compare_arr\n",
    "                elif op == '<':\n",
    "                    current_mask = param_arr < compare_arr\n",
    "                elif op == '==':\n",
    "                    current_mask = param_arr == compare_arr\n",
    "                elif op == '!=': \n",
    "                    current_mask = param_arr != compare_arr\n",
    "                else:\n",
    "                    print(f\"⚠️ Unsupported operator '{op}' for '{param}', skipping.\")\n",
    "                    continue\n",
    "\n",
    "                logic_mask &= current_mask\n",
    "\n",
    "                if metadata['additional_thresholds'] is None:\n",
    "                    metadata['additional_thresholds'] = {}\n",
    "                metadata['additional_thresholds'][param] = (op, compare_expr)\n",
    "\n",
    "            mask &= logic_mask\n",
    "\n",
    "        if not np.any(mask):\n",
    "            print(f\"⚠️ All data filtered out for '{key}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        filtered_data = [arr[mask] for arr in data_arrays]\n",
    "        filtered_time = time_array[mask]\n",
    "\n",
    "        new_key = f\"{key}_filt{filter_index}\"\n",
    "        updated_legends = [f\"{l} (filt{filter_index})\" for l in legends]\n",
    "\n",
    "        if plot_type == \"time_series\":\n",
    "            zipped_data[new_key] = (\n",
    "                plot_type, filtered_data, var_names, filtered_time, ylabels,\n",
    "                updated_legends, colors,\n",
    "                entry[7], entry[8], entry[9], entry[10],\n",
    "                entry[11], entry[12], entry[13], entry[14]\n",
    "            )\n",
    "        elif plot_type == \"scatter\":\n",
    "            zipped_data[new_key] = (\n",
    "                plot_type, filtered_data, var_names, filtered_time, ylabels,\n",
    "                updated_legends, colors,\n",
    "                entry[7], entry[8], entry[9],\n",
    "                entry[10], entry[11]\n",
    "            )\n",
    "        else:\n",
    "            print(f\"⚠️ Unsupported plot type '{plot_type}' for '{key}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        if store_metadata and f\"filt{filter_index}\" not in filter_metadata:\n",
    "            filter_metadata[f\"filt{filter_index}\"] = metadata\n",
    "\n",
    "        print(f\"✅ Filtered '{key}' ➜ '{new_key}'\")\n",
    "        new_keys.append(new_key)\n",
    "\n",
    "    return new_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f4273-3985-4db9-a0e6-818065a7f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the cell that currently holds your plotbot function with this code:\n",
    "\n",
    "def plot_efficient_spectrogram(ax, time_data, energy_data, intensity_data, fig, \n",
    "                             cmap=None, norm=None):\n",
    "    \"\"\"\n",
    "    Plot a spectrogram efficiently by matching data resolution to display resolution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axis to plot on\n",
    "    time_data : array-like\n",
    "        Time values (x-axis)\n",
    "    energy_data : array-like\n",
    "        Energy values (y-axis)\n",
    "    intensity_data : array-like\n",
    "        2D intensity values (color)\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The figure object (needed for resolution calculation)\n",
    "    cmap : str or matplotlib.colors.Colormap, optional\n",
    "        The colormap to use\n",
    "    norm : matplotlib.colors.Normalize, optional\n",
    "        The normalization to use\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    im : matplotlib.collections.QuadMesh\n",
    "        The plotted pcolormesh object\n",
    "    \"\"\"\n",
    "    # Calculate appropriate downsample factor based on display width\n",
    "    fig_width_px = fig.get_size_inches()[0] * fig.dpi\n",
    "    downsample = max(1, len(time_data) // int(fig_width_px))\n",
    "    \n",
    "    # Downsample the data\n",
    "    time_downsampled = time_data[::downsample]\n",
    "    energy_downsampled = energy_data[::downsample]\n",
    "    intensity_downsampled = intensity_data[::downsample]\n",
    "    \n",
    "    # Create the efficient pcolormesh\n",
    "    im = ax.pcolormesh(time_downsampled, \n",
    "                      energy_downsampled, \n",
    "                      intensity_downsampled,\n",
    "                      cmap=cmap,\n",
    "                      norm=norm,\n",
    "                      shading='nearest',\n",
    "                      rasterized=True,\n",
    "                      snap=True,\n",
    "                      antialiased=False)\n",
    "    \n",
    "    return im\n",
    "\n",
    "\n",
    "def plotbot(trange, *args):\n",
    "    \"\"\"\n",
    "    Run \"help(plotbot)\" for more information!\n",
    "    \n",
    "    Plots multiple variables on different axes within a single figure, performing time clipping internally.\n",
    "    Supports plotting on right axis when 'r' is appended to the axis number.\n",
    "    Creates a single, combined legend for each subplot, including all variables.\n",
    "    Positions the legend outside the plot, moving it further right for subplots with a right axis.\n",
    "    Plot numbering starts at 1.\n",
    "    \"\"\"\n",
    "    # Group variables by axis\n",
    "    axis_groups = defaultdict(list)\n",
    "    for i in range(0, len(args), 2):\n",
    "        variable, axis_spec = args[i], args[i+1]\n",
    "        axis_num, is_right = parse_axis_spec(axis_spec)\n",
    "        axis_groups[(axis_num, is_right)].append(variable)\n",
    "\n",
    "    # Determine the number of subplots needed\n",
    "    num_subplots = max(axis_num for axis_num, _ in axis_groups.keys())\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, axs = plt.subplots(num_subplots, 1, sharex=True, figsize=(12, 2*num_subplots))\n",
    "    if num_subplots == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Adjust the subplot parameters\n",
    "    plt.subplots_adjust(right=0.75)  # Adjust this value to make room for legends\n",
    "\n",
    "    # Plot each group of variables\n",
    "    for axis_index in range(1, num_subplots + 1):\n",
    "        ax = axs[axis_index - 1]\n",
    "        ax_right = None\n",
    "        legend_handles = []\n",
    "        legend_labels = []\n",
    "        has_right_axis = False\n",
    "\n",
    "        for (axis_num, is_right), variable_list in axis_groups.items():\n",
    "            if axis_num != axis_index:\n",
    "                continue\n",
    "\n",
    "            if is_right and ax_right is None:\n",
    "                ax_right = ax.twinx()\n",
    "                has_right_axis = True\n",
    "\n",
    "            for variable in variable_list:\n",
    "                data = zipped_data.get(variable)\n",
    "                if data is None:\n",
    "                    print(f\"No data found for variable: {variable}\")\n",
    "                    continue\n",
    "\n",
    "                plot_type = data[0]\n",
    "\n",
    "                if plot_type == 'time_series':\n",
    "                    # Unpack data\n",
    "                    (plot_type, variables_data, var_names, datetime_array,\n",
    "                     y_axis_labels, var_legend_labels, colors_list, y_axis_scaling,\n",
    "                     y_axis_limits, line_widths, line_styles, additional_data,\n",
    "                     colorbar_map, colorbar_scale, colorbar_limits) = data\n",
    "\n",
    "                    # Apply custom options if they exist\n",
    "                    if variable in plotbot_manager.custom_options:\n",
    "                        custom_opts = plotbot_manager.custom_options[variable]\n",
    "                        plot_type = custom_opts.get('plot_type', plot_type)\n",
    "                        variables_data = custom_opts.get('variables_data', variables_data)\n",
    "                        var_names = custom_opts.get('var_names', var_names)\n",
    "                        datetime_array = custom_opts.get('datetime_array', datetime_array)\n",
    "                        y_axis_labels = custom_opts.get('y_label', y_axis_labels)\n",
    "                        var_legend_labels = custom_opts.get('legend_label', var_legend_labels)\n",
    "                        y_axis_limits = custom_opts.get('y_lim', y_axis_limits)\n",
    "                        line_widths = custom_opts.get('line_width', line_widths)\n",
    "                        line_styles = custom_opts.get('line_style', line_styles)\n",
    "                        additional_data = custom_opts.get('additional_data', additional_data)\n",
    "                        colorbar_map = custom_opts.get('colorbar_map', colorbar_map)\n",
    "                        colorbar_scale = custom_opts.get('colorbar_scale', colorbar_scale)\n",
    "                        colorbar_limits = custom_opts.get('colorbar_limits', colorbar_limits)\n",
    "\n",
    "                    # Time clipping\n",
    "                    clipped_results = apply_time_range(trange, datetime_array, *variables_data)\n",
    "                    time_data_clipped = clipped_results[0]\n",
    "                    variables_data_clipped = clipped_results[1:]\n",
    "\n",
    "                    plot_ax = ax_right if is_right else ax\n",
    "                    for var_data, label, color, lw, ls in zip(variables_data_clipped, var_legend_labels,\n",
    "                                                              colors_list, line_widths, line_styles):\n",
    "                        line, = plot_ax.plot(time_data_clipped, var_data, label=label, color=color,\n",
    "                                             linewidth=lw, linestyle=ls)\n",
    "                        legend_handles.append(line)\n",
    "                        legend_labels.append(label)\n",
    "                    ylabel = y_axis_labels[0].replace('\\\\n', '\\n')\n",
    "                    plot_ax.set_ylabel(ylabel)\n",
    "                    plot_ax.set_yscale(y_axis_scaling)\n",
    "\n",
    "                    # Apply y-axis limits\n",
    "                    if y_axis_limits is not None:\n",
    "                        plot_ax.set_ylim(y_axis_limits)\n",
    "\n",
    "                elif plot_type == 'spectral':\n",
    "                    # Unpack data\n",
    "                    (plot_type, variables_data, var_names, datetime_array,\n",
    "                     y_axis_labels, var_legend_labels, colors_list, y_axis_scaling,\n",
    "                     y_axis_limits, line_widths, line_styles, additional_data,\n",
    "                     colorbar_map, colorbar_scale, colorbar_limits) = data\n",
    "\n",
    "                    # Debug prints for colorbar limits\n",
    "                    print(f\"Variable '{variable}' colorbar_limits before custom options: {colorbar_limits}\")\n",
    "                    if variable in plotbot_manager.custom_options:\n",
    "                        print(f\"Custom options for '{variable}': {plotbot_manager.custom_options[variable]}\")\n",
    "\n",
    "                    # Apply custom options if they exist\n",
    "                    if variable in plotbot_manager.custom_options:\n",
    "                        custom_opts = plotbot_manager.custom_options[variable]\n",
    "                        plot_type = custom_opts.get('plot_type', plot_type)\n",
    "                        variables_data = custom_opts.get('variables_data', variables_data)\n",
    "                        var_names = custom_opts.get('var_names', var_names)\n",
    "                        datetime_array = custom_opts.get('datetime_array', datetime_array)\n",
    "                        y_axis_labels = custom_opts.get('y_label', y_axis_labels)\n",
    "                        var_legend_labels = custom_opts.get('legend_label', var_legend_labels)\n",
    "                        colors_list = custom_opts.get('line_color', colors_list)\n",
    "                        y_axis_scaling = custom_opts.get('y_scale', y_axis_scaling)\n",
    "                        y_axis_limits = custom_opts.get('y_lim', y_axis_limits)\n",
    "                        line_widths = custom_opts.get('line_width', line_widths)\n",
    "                        line_styles = custom_opts.get('line_style', line_styles)\n",
    "                        additional_data = custom_opts.get('additional_data', additional_data)\n",
    "                        colorbar_map = custom_opts.get('colorbar_map', colorbar_map)\n",
    "                        colorbar_scale = custom_opts.get('colorbar_scale', colorbar_scale)\n",
    "                        colorbar_limits = custom_opts.get('colorbar_limits', colorbar_limits)\n",
    "\n",
    "                    # Time clipping\n",
    "                    clipped_results = apply_time_range(trange, datetime_array, *variables_data)\n",
    "                    time_data_clipped = clipped_results[0]\n",
    "                    variables_data_clipped = clipped_results[1:]\n",
    "\n",
    "                    time_indices = time_clip(datetime_array, trange[0], trange[1])\n",
    "                    additional_data_clipped = additional_data[time_indices]\n",
    "\n",
    "                    # Handle colorbar scaling\n",
    "                    if colorbar_scale == 'log':\n",
    "                        norm = colors.LogNorm(vmin=colorbar_limits[0], vmax=colorbar_limits[1]) if colorbar_limits else colors.LogNorm()\n",
    "                    elif colorbar_scale == 'linear':\n",
    "                        norm = colors.Normalize(vmin=colorbar_limits[0], vmax=colorbar_limits[1]) if colorbar_limits else None\n",
    "                    else:\n",
    "                        norm = None\n",
    "                    cmap = colorbar_map if colorbar_map else None\n",
    "\n",
    "                    \n",
    "                    # im = ax.pcolormesh(time_data_clipped, additional_data_clipped, variables_data_clipped[0],\n",
    "                    #                    cmap=cmap, shading='auto', norm=norm)\n",
    "                    \n",
    "                    im = plot_efficient_spectrogram(\n",
    "                        ax=ax,\n",
    "                        time_data=time_data_clipped,\n",
    "                        energy_data=additional_data_clipped,\n",
    "                        intensity_data=variables_data_clipped[0],\n",
    "                        fig=fig,\n",
    "                        cmap=cmap,\n",
    "                        norm=norm\n",
    "                    )\n",
    "\n",
    "                    ylabel = y_axis_labels[0].replace('\\\\n', '\\n')\n",
    "                    ax.set_ylabel(ylabel)\n",
    "                    pos = ax.get_position()\n",
    "                    cax = fig.add_axes([pos.x1 + 0.01, pos.y0, 0.02, pos.height])\n",
    "                    cbar = plt.colorbar(im, cax=cax)\n",
    "                    cbar.set_label(r'Intensity')\n",
    "\n",
    "                    # Apply y-axis limits and scaling\n",
    "                    if y_axis_limits is not None:\n",
    "                        ax.set_ylim(y_axis_limits)\n",
    "                    ax.set_yscale(y_axis_scaling)\n",
    "\n",
    "                elif plot_type == 'scatter':\n",
    "                    # Unpack data\n",
    "                    (plot_type, variables_data, var_names, datetime_array,\n",
    "                     y_axis_label, var_legend_labels, colors_list, marker_styles,\n",
    "                     marker_sizes, alphas, y_axis_scaling, y_axis_limits) = data\n",
    "\n",
    "                    # Apply custom options if they exist\n",
    "                    if variable in plotbot_manager.custom_options:\n",
    "                        custom_opts = plotbot_manager.custom_options[variable]\n",
    "                        plot_type = custom_opts.get('plot_type', plot_type)\n",
    "                        variables_data = custom_opts.get('variables_data', variables_data)\n",
    "                        var_names = custom_opts.get('var_names', var_names)\n",
    "                        datetime_array = custom_opts.get('datetime_array', datetime_array)\n",
    "                        y_axis_label = custom_opts.get('y_label', y_axis_label)\n",
    "                        var_legend_labels = custom_opts.get('legend_label', var_legend_labels)\n",
    "                        colors_list = custom_opts.get('line_color', colors_list)\n",
    "                        marker_styles = custom_opts.get('marker_style', marker_styles)\n",
    "                        marker_sizes = custom_opts.get('marker_size', marker_sizes)\n",
    "                        alphas = custom_opts.get('alpha', alphas)\n",
    "                        y_axis_scaling = custom_opts.get('y_scale', y_axis_scaling)\n",
    "                        y_axis_limits = custom_opts.get('y_lim', y_axis_limits)\n",
    "\n",
    "                    # Time clipping\n",
    "                    clipped_results = apply_time_range(trange, datetime_array, *variables_data)\n",
    "                    time_data_clipped = clipped_results[0]\n",
    "                    variables_data_clipped = clipped_results[1:]\n",
    "\n",
    "                    plot_ax = ax_right if is_right else ax\n",
    "                    for var_data, label, color, marker_style, marker_size, alpha in zip(\n",
    "                            variables_data_clipped, var_legend_labels, colors_list,\n",
    "                            marker_styles, marker_sizes, alphas):\n",
    "                        sc = plot_ax.scatter(time_data_clipped, var_data, label=label, color=color,\n",
    "                                             marker=marker_style, s=marker_size, alpha=alpha)\n",
    "                        legend_handles.append(sc)\n",
    "                        legend_labels.append(label)\n",
    "                    plot_ax.set_ylabel(y_axis_label)\n",
    "                    plot_ax.set_yscale(y_axis_scaling)\n",
    "                    if y_axis_limits is not None:\n",
    "                        plot_ax.set_ylim(y_axis_limits)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Unknown plot type: {plot_type}\")\n",
    "                    continue\n",
    "\n",
    "        # Create a single legend for the subplot\n",
    "        if legend_handles:\n",
    "            if has_right_axis:\n",
    "                leg = ax.legend(legend_handles, legend_labels, loc='center left', bbox_to_anchor=(1.095, 0.5))\n",
    "                for lh in legend_handles: \n",
    "                    lh.set_alpha(1)\n",
    "            else:\n",
    "                leg = ax.legend(legend_handles, legend_labels, loc='center left', bbox_to_anchor=(1.02, 0.5))\n",
    "             #   for lh in leg.legend_handles: \n",
    "                for lh in legend_handles: \n",
    "                    lh.set_alpha(1)\n",
    "\n",
    "        # Remove extra space on x-axis\n",
    "        ax.margins(x=0)\n",
    "        if ax_right:\n",
    "            ax_right.margins(x=0)\n",
    "\n",
    "    # Set common x-label\n",
    "    axs[-1].set_xlabel('Time')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41ed8d-e453-4034-8e87-5b94594e075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the midpoint for the colormap with log normalization\n",
    "class MidpointLogNormalize(colors.LogNorm):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        super().__init__(vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        if self.midpoint is None:\n",
    "            return super().__call__(value, clip)\n",
    "        log_vmin = np.log10(self.vmin)\n",
    "        log_vmax = np.log10(self.vmax)\n",
    "        log_midpoint = np.log10(self.midpoint)\n",
    "        x, y = [log_vmin, log_midpoint, log_vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(np.log10(value), x, y))\n",
    "    \n",
    "# Define the midpoint for the colormap\n",
    "class MidpointNormalize(colors.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        super().__init__(vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "# Create a colorbar with the center around 1\n",
    "#norm = MidpointNormalize(vmin=0, vmax=2, midpoint=1)\n",
    "\n",
    "# Create a colorbar with the center around 1 using log normalization\n",
    "#norm = MidpointLogNormalize(vmin=0.1, vmax=100, midpoint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340110e9-fcb6-4a05-851a-062618384a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showdahodo(trange, var1, var2, color_var=None, norm_ = None, xlim_ = None, ylim_ = None, \n",
    "               fname = None, s_ = None, alpha_ = None, xlabel_ = None, ylabel_ = None, \n",
    "               clabel_ = None, xlog_ = None, ylog_ = None, cmap_ = None, sort = None, \n",
    "               invsort = None, lumsort = None, brazil = None, corr = None, wvpow = None, \n",
    "               rsun = None, noshow = None, face_c = None, face_a = None, identity_line = None):\n",
    "    \"\"\"\n",
    "    Run \"help(showdahodo)\" for more information!\n",
    "\n",
    "    Create a hodogram plot of two variables, optionally colored by a third variable.\n",
    "    Also calculates and displays the correlation coefficient and plots a trend line.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def check_lengths(name, original_len, clipped_len, resampled_len=None, resampling_done=False):\n",
    "        \n",
    "        # print(f\"{name} original length: {original_len}\")\n",
    "        # print(f\"{name} length after time clipping: {clipped_len}\")\n",
    "        # if resampled_len is not None:\n",
    "        #     print(f\"{name} length after resampling: {resampled_len}\")\n",
    "        #     if resampling_done:\n",
    "        #         print(f\"{name} resampling successful!\")\n",
    "        #     else:\n",
    "        #         print(f\"{name} resampling not needed.\")\n",
    "        # print()  # For better readability\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # Get data for var1 and var2\n",
    "    data1 = zipped_data[var1]\n",
    "    data2 = zipped_data[var2]\n",
    "\n",
    "    # Extract data and time arrays\n",
    "    values1_full = np.array(data1[1][0])  # Data array is now at index 1\n",
    "    time1_full = np.array(data1[3])       # Time array is now at index 3\n",
    "    values2_full = np.array(data2[1][0])  # Data array is now at index 1\n",
    "    time2_full = np.array(data2[3])       # Time array is now at index 3\n",
    "\n",
    "    if wvpow is not None:\n",
    "        color1_time_full = datetime_wvpow\n",
    "        color2_time_full = datetime_wvpow\n",
    "        color1_values_full = wvpow_LH_spi\n",
    "        color2_values_full = wvpow_RH_spi\n",
    "\n",
    "\n",
    "    # Save original lengths\n",
    "    time1_original_len = len(time1_full)\n",
    "    time2_original_len = len(time2_full)\n",
    "\n",
    "    # Apply time range using apply_time_range\n",
    "    time1_clipped, values1_clipped = apply_time_range(trange, time1_full, values1_full)\n",
    "    time2_clipped, values2_clipped = apply_time_range(trange, time2_full, values2_full)\n",
    "\n",
    "    time1_clipped_len = len(time1_clipped)\n",
    "    time2_clipped_len = len(time2_clipped)\n",
    "    # Prepare color data\n",
    "    if color_var is not None:\n",
    "        if color_var == 'wvpow':\n",
    "            color1_time_full = datetime_wvpow\n",
    "            color2_time_full = datetime_wvpow\n",
    "            color1_values_full = wvpow_LH_spi\n",
    "            color2_values_full = wvpow_RH_spi\n",
    "            color1_time_original_len = len(color1_time_full)\n",
    "            color2_time_original_len = len(color2_time_full)\n",
    "            # Apply time range\n",
    "            color1_time_clipped, color1_values_clipped = apply_time_range(trange, color1_time_full, color1_values_full)\n",
    "            color1_time_clipped_len = len(color1_time_clipped)\n",
    "            # Apply time range\n",
    "            color2_time_clipped, color2_values_clipped = apply_time_range(trange, color2_time_full, color2_values_full)\n",
    "            color2_time_clipped_len = len(color2_time_clipped)\n",
    "        else:\n",
    "            color_data = zipped_data[color_var]\n",
    "            color_values_full = np.array(color_data[1][0])  # Data array is now at index 1\n",
    "            color_time_full = np.array(color_data[3])       # Time array is now at index 3\n",
    "            color_time_original_len = len(color_time_full)\n",
    "\n",
    "            # Apply time range\n",
    "            color_time_clipped, color_values_clipped = apply_time_range(trange, color_time_full, color_values_full)\n",
    "            color_time_clipped_len = len(color_time_clipped)\n",
    "    else:\n",
    "        color_time_clipped = None\n",
    "        color_values_clipped = None\n",
    "        color_time_original_len = 0\n",
    "        color_time_clipped_len = 0\n",
    "\n",
    "    # Determine which time series has the lowest sampling rate\n",
    "    # We'll use the one with the fewest data points as target_times\n",
    "    lengths = {\n",
    "        'time1': time1_clipped_len,\n",
    "        'time2': time2_clipped_len,\n",
    "        'color_time': color_time_clipped_len if color_time_clipped is not None else float('inf')\n",
    "    }    \n",
    "    min_length = min(lengths.values())\n",
    "    if lengths['time1'] == min_length:\n",
    "        target_times = time1_clipped\n",
    "    elif lengths['time2'] == min_length:\n",
    "        target_times = time2_clipped\n",
    "    else:\n",
    "        target_times = color_time_clipped\n",
    "\n",
    "    # Check if time arrays are equal\n",
    "    time_arrays_equal = np.array_equal(time1_clipped, target_times) and np.array_equal(time2_clipped, target_times)\n",
    "    if color_var is not None and color_time_clipped is not None:\n",
    "        time_arrays_equal = time_arrays_equal and np.array_equal(color_time_clipped, target_times)\n",
    "\n",
    "    # Decide whether resampling is needed\n",
    "    if time_arrays_equal:\n",
    "        # No resampling needed\n",
    "        values1 = values1_clipped\n",
    "        values2 = values2_clipped\n",
    "        if color_var is not None:\n",
    "            color_values = color_values_clipped\n",
    "        else:\n",
    "            color_values = None\n",
    "\n",
    "        # Use clipped lengths as resampled lengths\n",
    "        resampled_len = len(target_times)\n",
    "        check_lengths('values1', time1_original_len, time1_clipped_len, resampled_len, resampling_done=False)\n",
    "        check_lengths('values2', time2_original_len, time2_clipped_len, resampled_len, resampling_done=False)\n",
    "        if color_var is not None:\n",
    "            check_lengths('color_values', color_time_original_len, color_time_clipped_len, resampled_len, resampling_done=False)\n",
    "    else:\n",
    "        # Resampling is needed\n",
    "        if not np.array_equal(time1_clipped, target_times):\n",
    "            #values1 = downsample_time_based(time1_clipped, values1_clipped, target_times) #this is for downsampling via interpolation\n",
    "            #values1 = downsample_to_match(time1_clipped, values1_clipped, target_times)\n",
    "            values1 = downsample_to_min_ind(time1_clipped, values1_clipped, target_times)\n",
    "            resampling_done1 = True\n",
    "        else:\n",
    "            values1 = values1_clipped\n",
    "            resampling_done1 = False\n",
    "\n",
    "        if not np.array_equal(time2_clipped, target_times):\n",
    "            #values2 = downsample_time_based(time2_clipped, values2_clipped, target_times)\n",
    "            #values2 = downsample_to_match(time2_clipped, values2_clipped, target_times)\n",
    "            values2 = downsample_to_min_ind(time2_clipped, values2_clipped, target_times)\n",
    "\n",
    "            resampling_done2 = True\n",
    "        else:\n",
    "            values2 = values2_clipped\n",
    "            resampling_done2 = False\n",
    "\n",
    "        if color_var is not None and color_time_clipped is not None:\n",
    "            if not np.array_equal(color_time_clipped, target_times):\n",
    "                #color_values = downsample_time_based(color_time_clipped, color_values_clipped, target_times)\n",
    "                #color_values = downsample_to_match(color_time_clipped, color_values_clipped, target_times)\n",
    "                color_values = downsample_to_min_ind(color_time_clipped, color_values_clipped, target_times)                \n",
    "                resampling_done_color = True\n",
    "            else:\n",
    "                color_values = color_values_clipped\n",
    "                resampling_done_color = False\n",
    "        else:\n",
    "            color_values = None\n",
    "            resampling_done_color = False\n",
    "\n",
    "        # Use resampled lengths\n",
    "        resampled_len = len(target_times)\n",
    "        check_lengths('values1', time1_original_len, time1_clipped_len, resampled_len, resampling_done=resampling_done1)\n",
    "        check_lengths('values2', time2_original_len, time2_clipped_len, resampled_len, resampling_done=resampling_done2)\n",
    "        if color_var is not None:\n",
    "            check_lengths('color_values', color_time_original_len, color_time_clipped_len, resampled_len, resampling_done=resampling_done_color)\n",
    "\n",
    "    # Prepare colors\n",
    "    if color_var is None:\n",
    "        colors = mdates.date2num(target_times) - mdates.date2num(target_times[0])\n",
    "        color_label = 'Time'\n",
    "    else:\n",
    "        colors = color_values\n",
    "        if clabel_ is not None:\n",
    "            color_label = clabel_\n",
    "        else:\n",
    "            color_label = color_data[5][0] if color_data[0] != 'scatter' else color_data[5]  # legend label for color variable\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if s_ is not None:\n",
    "        s_ = s_\n",
    "    else:\n",
    "        s_ = 20\n",
    "        \n",
    "    if alpha_ is not None:\n",
    "        alpha_ = alpha_\n",
    "    else:\n",
    "        alpha_ = .7\n",
    "    if norm_ is not None:\n",
    "        norm_ = norm_\n",
    "    else:\n",
    "        norm_ = Normalize()\n",
    "    if cmap_ is not None:\n",
    "        cmap_ = cmap_\n",
    "    else:\n",
    "        cmap_ = 'plasma'\n",
    "        \n",
    "    if sort is not None:\n",
    "    #sort color by ascending order so highest color value plotted last\n",
    "        sort_c = np.argsort(colors)\n",
    "        values1 = values1[sort_c]\n",
    "        values2 = values2[sort_c]\n",
    "        colors = colors[sort_c]\n",
    "        \n",
    "    if invsort is not None:\n",
    "    #sort color by descending order so lowest color value plotted last\n",
    "        sort_c = np.argsort(colors)[::-1]\n",
    "        values1 = values1[sort_c]\n",
    "        values2 = values2[sort_c]\n",
    "        colors = colors[sort_c]\n",
    "    \n",
    "    if lumsort is not None:\n",
    "        #sort color by plotting lightest color last\n",
    "        # Get the colormap instance\n",
    "        colormap = get_cmap(cmap_)\n",
    "        # Calculate RGB values for each data point\n",
    "        colors_rbg = colormap(norm(colors))\n",
    "        # Calculate luminance (relative brightness) of each color\n",
    "        luminance = 0.2126 * colors_rbg[:, 0] + 0.7152 * colors_rbg[:, 1] + 0.0722 * colors_rbg[:, 2]\n",
    "        # Sort data by luminance\n",
    "        sort_c = np.argsort(luminance)  # Darker colors first, lighter colors last\n",
    "        sort_c = np.argsort(colors)[::-1]\n",
    "        values1 = values1[sort_c]\n",
    "        values2 = values2[sort_c]\n",
    "        colors = colors[sort_c]\n",
    "        colors = np.log10(colors)\n",
    "    if face_c is not None:\n",
    "        ax.patch.set_facecolor(face_c)\n",
    "    if face_a is not None:\n",
    "        ax.patch.set_alpha(face_a)\n",
    "    \n",
    "    scatter = plt.scatter(values1, values2, c=colors, cmap=cmap_, norm=norm_, s=s_, alpha = alpha_)\n",
    "    cbar = plt.colorbar(scatter, label=color_label)\n",
    "    cbar.solids.set(alpha=1)\n",
    "    \n",
    "    corr_title = ''\n",
    "    if corr is not None:\n",
    "        if ((xlog_ is not None) and (ylog_ is not None)) or brazil is not None:\n",
    "            #Calculate correlation coefficient\n",
    "           # correlation_coefficient, p_value = stats.pearsonr(np.log10(values1), np.log10(values2))\n",
    "            # Calculate trend line\n",
    "           # z = np.polyfit(np.log10(values1), np.log10(values2), 1)\n",
    "           # p = np.poly1d(z)\n",
    "           # plt.plot(np.log10(values1), p(np.log10(values1)), \".k\", alpha=0.5)  # Add trend line\n",
    "            coeff = corr_nan(np.log10(values1),np.log10(values2))\n",
    "            corr_title = f'Corr Coeff: {coeff:.2f}, '\n",
    "            #plt.title(f'log({var1}) vs log({var2})\\nCorrelation Coefficient: {coeff:.2f}')\n",
    "        else:\n",
    "            # Calculate correlation coefficient\n",
    "            correlation_coefficient, p_value = stats.pearsonr(values1, values2)\n",
    "            # Calculate trend line\n",
    "            z = np.polyfit(values1, values2, 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(values1, p(values1), \".k\", alpha=0.5)  # Add trend line\n",
    "            corr_title = f'Corr Coef: {correlation_coefficient:.2f}, '\n",
    "            #plt.title(f'{var1} vs {var2}\\nCorrelation Coefficient: {correlation_coefficient:.2f}')\n",
    "\n",
    "    if color_var is None:\n",
    "        # Convert colorbar ticks to datetime\n",
    "        cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(\n",
    "            lambda x, p: (target_times[0] + timedelta(days=x)).strftime('%Y-%m-%d/%H:%M:%S')\n",
    "        ))\n",
    "    \n",
    "    if xlabel_ is not None:\n",
    "        xlabel = xlabel_\n",
    "    else:\n",
    "         # Set axis labels based on plot type\n",
    "         # data[4] = yaxis title, data[5] = legend\n",
    "        xlabel = data1[5][0] if data1[0] != 'scatter' else data1[5]\n",
    "    if ylabel_ is not None:\n",
    "        ylabel = ylabel_\n",
    "    else:\n",
    "        ylabel = data2[5][0] if data2[0] != 'scatter' else data2[5]\n",
    "\n",
    "    plt.xlabel(xlabel, fontsize = 16)\n",
    "    plt.ylabel(ylabel, fontsize = 16)\n",
    "    \n",
    "    if xlim_ is not None:\n",
    "        xlim = xlim_\n",
    "        plt.xlim(xlim)\n",
    "    if ylim_ is not None:\n",
    "        ylim = ylim_\n",
    "        plt.ylim(ylim)\n",
    "        \n",
    "    if brazil is not None:\n",
    "    #add instability threshold curves and plot on loglog scale\n",
    "        #step =  np.nanmin(values1_clipped)\n",
    "        beta_par       = np.arange(0, 1000, 1e-4)\n",
    "        trat_parfire = 1-(.47/(beta_par - .59)**.53)\n",
    "        trat_oblfire = 1-(1.4/(beta_par + .11))\n",
    "        trat_protcyc = 1+(.43/(beta_par + .0004)**.42)\n",
    "        trat_mirror = 1+(.77/(beta_par + .016)**.76)\n",
    "        plt.plot(beta_par,trat_parfire,color='black',linestyle='dashed')\n",
    "        plt.plot(beta_par,trat_oblfire,color='grey',linestyle='dashed')\n",
    "        plt.plot(beta_par,trat_protcyc,color='black',linestyle='dotted')\n",
    "        plt.plot(beta_par,trat_mirror,color='grey',linestyle='dotted')\n",
    "        plt.loglog()\n",
    "        \n",
    "    if xlog_ is not None:\n",
    "        plt.xscale('log')\n",
    "    if ylog_ is not None:\n",
    "        plt.yscale('log')\n",
    "        \n",
    "    \n",
    "    if identity_line is not None:\n",
    "        ax.axline((0, 0), slope=1, c = 'black', linestyle = '--')\n",
    "    \n",
    "    tname=f\"{trange[0]}\" + ' - ' + f\"{trange[-1]}\"\n",
    "    \n",
    "    rsun_title = ''\n",
    "    if rsun is not None:\n",
    "        dist_time_clipped, dist_values_clipped = apply_time_range(trange, datetime_spi, sun_dist_rsun)    \n",
    "        sun_dist_rsun_clipped_avg = np.round(np.average(dist_values_clipped),1)\n",
    "        rsun_title = f\"Rs = {sun_dist_rsun_clipped_avg}, \"\n",
    "       # plt.title(f\"Rs = {sun_dist_rsun_clipped_avg},  \" +  tname)\n",
    "    #plt.title(f'Hodogram: {var1} vs {var2}\\nCorrelation Coefficient: {correlation_coefficient:.2f}')\n",
    "    #plt.grid(True)\n",
    "    \n",
    "    plt.title(rsun_title + corr_title + tname, fontsize = 12)\n",
    "    \n",
    "    if fname is not None:\n",
    "        plt.savefig(f\"{fname}_brazil.png\", bbox_inches='tight')\n",
    "    \n",
    "    if noshow is None:\n",
    "        plt.show()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc41e72-3042-4a26-9f9b-dae5c9fe08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def showdahodo_brazil_wvpow(trange, xlim_ = None, ylim_ = None, fname = None, s_ = None, alpha_ = None, \n",
    "                            vmin_ = None, vmax_ = None, rsun = None, sort = None, invsort = None, norm_ = None):\n",
    "\n",
    "    time1_full = datetime_spi\n",
    "    values1_full = beta_ppar_spi\n",
    "    time2_full = datetime_spi\n",
    "    values2_full = Anisotropy\n",
    "    color1_time_full = datetime_spi\n",
    "    color2_time_full = datetime_spi\n",
    "    color1_values_full = wvpow_LH_spi\n",
    "    color2_values_full = wvpow_RH_spi\n",
    "\n",
    "    # Apply time range using apply_time_range\n",
    "    time1_clipped, values1_clipped = apply_time_range(trange, time1_full, values1_full)\n",
    "    time2_clipped, values2_clipped = apply_time_range(trange, time2_full, values2_full)\n",
    "    color1_time_clipped, color1_values_clipped = apply_time_range(trange, color1_time_full, color1_values_full)\n",
    "    color2_time_clipped, color2_values_clipped = apply_time_range(trange, color2_time_full, color2_values_full)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ytitle = r'Proton $T_\\perp/T_\\parallel$'\n",
    "    xtitle = r'Proton $\\beta_\\parallel$'\n",
    "    ctitle1 = 'Log(LH Wave Pow)'\n",
    "    ctitle2 = 'Log(RH Wave Pow)'\n",
    "    \n",
    "    if s_ is not None:\n",
    "        s_ = s_\n",
    "    else:\n",
    "        s_ = 20\n",
    "        \n",
    "    if alpha_ is not None:\n",
    "        alpha_ = alpha_\n",
    "    else:\n",
    "        alpha_ = .7\n",
    "        \n",
    "    if sort is not None:\n",
    "    #sort color by ascending order so highest color value plotted last\n",
    "        sort_c1 = np.argsort(color1_values_clipped)\n",
    "        values1_clipped_c1 = values1_clipped[sort_c1]\n",
    "        values2_clipped_c1 = values2_clipped[sort_c1]\n",
    "        color1_values_clipped = color1_values_clipped[sort_c1]\n",
    "        sort_c2 = np.argsort(color2_values_clipped)\n",
    "        values1_clipped_c2 = values1_clipped[sort_c2]\n",
    "        values2_clipped_c2 = values2_clipped[sort_c2]\n",
    "        color2_values_clipped = color2_values_clipped[sort_c2]\n",
    "    else:\n",
    "        values1_clipped_c1 = values1_clipped\n",
    "        values2_clipped_c1 = values2_clipped\n",
    "        values1_clipped_c2 = values1_clipped\n",
    "        values2_clipped_c2 = values2_clipped\n",
    "\n",
    "\n",
    "    if invsort is not None:\n",
    "    #sort color by descending order so lowest color value plotted last\n",
    "        sort_c1 = np.argsort(color1_values_clipped)[::-1]\n",
    "        values1_clipped_c1 = values1_clipped[sort_c1]\n",
    "        values2_clipped_c1 = values2_clipped[sort_c1]\n",
    "        color1_values_clipped = color1_values_clipped[sort_c1]\n",
    "        sort_c2 = np.argsort(color2_values_clipped)[::-1]\n",
    "        values1_clipped_c2 = values1_clipped[sort_c2]\n",
    "        values2_clipped_c2 = values2_clipped[sort_c2]\n",
    "        color2_values_clipped = color2_values_clipped[sort_c2]\n",
    "    else:\n",
    "        values1_clipped_c1 = values1_clipped\n",
    "        values2_clipped_c1 = values2_clipped\n",
    "        values1_clipped_c2 = values1_clipped\n",
    "        values2_clipped_c2 = values2_clipped\n",
    "    if norm_ is not None:\n",
    "        norm_ = norm_\n",
    "    else:\n",
    "        norm_ = colors.LogNorm()\n",
    "\n",
    "    if vmin_ is not None:\n",
    "        s1=plt.scatter(values1_clipped_c1, values2_clipped_c1, c = color1_values_clipped, cmap='Blues_r', s=s_, alpha=alpha_, vmin = vmin_, vmax = vmax_, norm = norm_)\n",
    "        s2=plt.scatter(values1_clipped_c2, values2_clipped_c2, c = color2_values_clipped, cmap='Reds_r', s=s_, alpha=alpha_, vmin = vmin_, vmax = vmax_, norm = norm_)\n",
    "    else:\n",
    "        s1=plt.scatter(values1_clipped_c1, values2_clipped_c1, c = color1_values_clipped, cmap='Blues', s=s_, alpha=alpha_, norm = norm_)\n",
    "        s2=plt.scatter(values1_clipped_c2, values2_clipped_c2, c = color2_values_clipped, cmap='Reds', s=s_, alpha=alpha_, norm = norm_)\n",
    "        \n",
    "    if xlim_ is not None:\n",
    "        plt.xlim(xlim_)\n",
    "    if ylim_ is not None:\n",
    "        plt.ylim(ylim_)\n",
    "        \n",
    "        \n",
    "    plt.ylabel(ytitle, fontsize = 16)\n",
    "    plt.xlabel(xtitle, fontsize = 16)\n",
    "    plt.loglog()\n",
    "\n",
    "    ax.patch.set_facecolor('mediumspringgreen')\n",
    "    ax.patch.set_alpha(0.3)\n",
    "\n",
    "    cbar1 = plt.colorbar(s1)\n",
    "    cbar1.set_label(ctitle1)\n",
    "    cbar2 = plt.colorbar(s2)\n",
    "    cbar2.set_label(ctitle2)\n",
    "    cbar1.solids.set(alpha=1)\n",
    "    cbar2.solids.set(alpha=1)\n",
    "\n",
    "    fig.set_size_inches(15, 10)\n",
    "    \n",
    "    tname=f\"{trange[0]}\" + ' - ' + f\"{trange[-1]}\"\n",
    "    rsun_title = ''\n",
    "    if rsun is not None:\n",
    "        dist_time_clipped, dist_values_clipped = apply_time_range(trange, datetime_spi, sun_dist_rsun)    \n",
    "        sun_dist_rsun_clipped_avg = np.round(np.average(dist_values_clipped),1)\n",
    "        rsun_title = f\"Rs = {sun_dist_rsun_clipped_avg}, \"\n",
    "    \n",
    "\n",
    "    plt.title(rsun_title + tname, fontsize = 12)\n",
    "    \n",
    "    #step =  min(values1_clipped)    \n",
    "    beta_par       = np.arange(0, 1000, 1e-4)\n",
    "    trat_parfire = 1-(.47/(beta_par - .59)**.53)\n",
    "    trat_oblfire = 1-(1.4/(beta_par + .11))\n",
    "    trat_protcyc = 1+(.43/(beta_par + .0004)**.42)\n",
    "    trat_mirror = 1+(.77/(beta_par + .016)**.76)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.plot(beta_par,trat_parfire,color='black',linestyle='dashed')\n",
    "    plt.plot(beta_par,trat_oblfire,color='grey',linestyle='dashed')\n",
    "    plt.plot(beta_par,trat_protcyc,color='black',linestyle='dotted')\n",
    "    plt.plot(beta_par,trat_mirror,color='grey',linestyle='dotted')\n",
    "    \n",
    "    if fname is not None:\n",
    "        plt.savefig(f\"{fname}_brazil_wvpow.png\", bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5df18-e6a7-47e5-84d6-2ef4a6d469e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix2datetime(ut_arr) : \n",
    "    \"\"\"Convert 1D array of unix timestamps (float) to `datetime.datetime`\"\"\"\n",
    "    return np.array([datetime.utcfromtimestamp(ut) for ut in ut_arr])\n",
    "\n",
    "def lista_hista(times, tau, plot = None):\n",
    "    #input array of datetimes for histagram and \n",
    "    #temporal bin size tau in s\n",
    "    \n",
    "    times_numeric = np.array([dt.timestamp() for dt in times])\n",
    "    tlen = times_numeric[-1] - times_numeric[0] #total length of time in s\n",
    "    nbins = round(tlen/tau) \n",
    "    c, b, f = plt.hist(times, bins=nbins, histtype='step')\n",
    "\n",
    "    # Calculate bin centers\n",
    "    bin_centers = b[:-1] + np.diff(b) / 2\n",
    "\n",
    "    # Convert bin centers to datetime objects using num2date\n",
    "    bin_centers_datetime = [num2date(center) for center in bin_centers]\n",
    "    bin_centers_datetime = np.asarray(bin_centers_datetime)\n",
    "    if plot is not None:\n",
    "        # Plot the histogram using plt.plot\n",
    "        plt.plot(bin_centers_datetime, c)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.title('Histogram of Datetimes')\n",
    "        plt.show()\n",
    "    \n",
    "    return c, bin_centers_datetime\n",
    "\n",
    "\n",
    "# Assuming `hardham_datetime` is an array of datetime objects\n",
    "# First, use plt.hist to get counts and bins\n",
    "def lista_hista2d(times, vals, tau, density = None, cmax = None, range = None, cmap = None):\n",
    "    #input array of datetimes for histagram and \n",
    "    #temporal bin size tau in s\n",
    "    #h_RH,bx,by, f = ax.hist2d(wvpow_times, np.log10(wvpow_RH),density=True, cmax=1e-3,bins=300, range = [[wvpow_times[0],wvpow_times[-1]],[-3,2]],cmap='Reds')\n",
    "    if density is not None:\n",
    "        density = density\n",
    "    if cmax is not None:\n",
    "        cmax = cmax\n",
    "    if range is not None:\n",
    "        range = range\n",
    "    if cmap is not None:\n",
    "        cmap = cmap\n",
    "    \n",
    "    times_numeric = np.array([dt.timestamp() for dt in times])\n",
    "    tlen = times_numeric[-1] - times_numeric[0] #total length of time in s\n",
    "    nbins = round(tlen/tau) \n",
    "    h, bx , by, f = plt.hist2d(times_numeric, vals, bins = nbins,\n",
    "                              density = density, cmax = cmax, range = range, cmap = cmap)\n",
    "\n",
    "    # Calculate bin centers\n",
    "    bin_centers_times = bx[:-1] + np.diff(bx) / 2\n",
    "\n",
    "    # Convert bin centers to datetime objects using num2date\n",
    "    bin_centers_datetime = unix2datetime(bin_centers_times)\n",
    "    bin_centers_datetime = np.asarray(bin_centers_datetime)\n",
    "    bin_centers_datetime_utc = np.array([dt.replace(tzinfo=timezone.utc) for dt in bin_centers_datetime])\n",
    "\n",
    "    bin_centers_vals = by[:-1] + np.diff(by) / 2\n",
    "\n",
    "    #if plot is not None:\n",
    "     #   plt.pcolormesh(bin_centers_datetime_utc, bin_centers_vals , h.T, cmap = cmap)\n",
    "    \n",
    "    return h.T, bin_centers_datetime_utc, bin_centers_vals \n",
    "\n",
    "\n",
    "\n",
    "def read_pickle(fname):\n",
    "    with open(f'{fname}.pkl', 'rb') as handle:\n",
    "        x = pickle.load(handle)\n",
    "    return x\n",
    "\n",
    "#Proton temperature anisotropy from Temperature Tensor\n",
    "def find_Tanisotropy(T_XX,T_YY,T_ZZ,T_XY,T_XZ,T_YZ):\n",
    "    #Access tensor elements -- The temperature is an array of 9 elements. We want to find out how much temp is aligned parallel or perp to the mag field.\n",
    "    T_YX = T_XY\n",
    "    T_ZX = T_XZ\n",
    "    T_ZY = T_YZ\n",
    "\n",
    "    #Access magnetic field in span-I coordinates\n",
    "    B_spi = get_data('psp_spi_sf00_MAGF_INST')\n",
    "    B_X = B_spi.y[:,0]\n",
    "    B_Y = B_spi.y[:,1]\n",
    "    B_Z = B_spi.y[:,2]\n",
    "    B_mag_XYZ = np.sqrt(B_X**2 + B_Y**2 + B_Z**2)\n",
    "\n",
    "    #Project Tensor onto B field, find perpendicular and parallel components\n",
    "    T_parallel=[]\n",
    "    T_perpendicular=[]\n",
    "    Anisotropy=[]\n",
    "    for hamepoch_idx in range(len(T_XX)):  #Calculates Tperp and Tpar from the projection of the magnetic field vector\n",
    "        i = np.argmin(np.abs(spi_epoch - hardham_list[hamepoch_idx]))\n",
    "        Sum_1=B_X[i]*B_X[i]*T_XX[hamepoch_idx]\n",
    "        Sum_2=B_X[i]*B_Y[i]*T_XY[hamepoch_idx]\n",
    "        Sum_3=B_X[i]*B_Z[i]*T_XZ[hamepoch_idx]\n",
    "        Sum_4=B_Y[i]*B_X[i]*T_YX[hamepoch_idx]\n",
    "        Sum_5=B_Y[i]*B_Y[i]*T_YY[hamepoch_idx]\n",
    "        Sum_6=B_Y[i]*B_Z[i]*T_YZ[hamepoch_idx]\n",
    "        Sum_7=B_Z[i]*B_X[i]*T_ZX[hamepoch_idx]\n",
    "        Sum_8=B_Z[i]*B_Y[i]*T_ZY[hamepoch_idx]\n",
    "        Sum_9=B_Z[i]*B_Z[i]*T_ZZ[hamepoch_idx]    \n",
    "        T_para=((Sum_1+Sum_2+Sum_3+Sum_4+Sum_5+Sum_6+Sum_7+Sum_8+Sum_9)/(B_mag_XYZ[i])**2)\n",
    "        Trace_Temp=(T_XX[hamepoch_idx]+T_YY[hamepoch_idx]+T_ZZ[hamepoch_idx])\n",
    "        T_perp=(Trace_Temp-T_para)/2.0\n",
    "        T_parallel.append((Sum_1+Sum_2+Sum_3+Sum_4+Sum_5+Sum_6+Sum_7+Sum_8+Sum_9)/(B_mag_XYZ[i])**2)\n",
    "        T_perpendicular.append(T_perp)\n",
    "        Anisotropy.append(T_perp/T_para)\n",
    "\n",
    "    return np.array(T_perpendicular), np.array(T_parallel), np.array(Anisotropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
