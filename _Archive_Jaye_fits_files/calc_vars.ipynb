{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00627c48-53a9-46f0-8857-ae2da919c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mag_rtn_4sa is not None:\n",
    "        ## Defining & Calculating Variables\n",
    "    #==============================================================\n",
    "    # 🧲 MAG_RTN_4SA Data Preparation\n",
    "    #==============================================================\n",
    "\n",
    "    # Get magnetic field and time data for RTN coordinates at 4 samples per cycle\n",
    "    mag_data_RTN_4sa = get_data('psp_fld_l2_mag_RTN_4_Sa_per_Cyc')\n",
    "    mag_time_RTN_4sa, mag_field_RTN_4sa = mag_data_RTN_4sa.times, mag_data_RTN_4sa.y\n",
    "\n",
    "        # Access components of magnetic field in RTN coordinates at 4 samples per second\n",
    "    split_vec('psp_fld_l2_mag_RTN_4_Sa_per_Cyc')\n",
    "    br_RTN_4sa = get_data('psp_fld_l2_mag_RTN_4_Sa_per_Cyc_x')\n",
    "    bt_RTN_4sa = get_data('psp_fld_l2_mag_RTN_4_Sa_per_Cyc_y')\n",
    "    bn_RTN_4sa = get_data('psp_fld_l2_mag_RTN_4_Sa_per_Cyc_z')\n",
    "    bmag_RTN_4sa = np.sqrt(br_RTN_4sa.y**2 + bt_RTN_4sa.y**2 + bn_RTN_4sa.y**2)\n",
    "\n",
    "    br_RTN_4sa = br_RTN_4sa.y\n",
    "    bt_RTN_4sa = bt_RTN_4sa.y\n",
    "    bn_RTN_4sa = bn_RTN_4sa.y\n",
    "\n",
    "        # Converting the magnetic field time to a timezone-aware datetime object\n",
    "    datetime_mag_RTN_4sa = np.array([dt.replace(tzinfo=timezone.utc) for dt in time_datetime(mag_time_RTN_4sa)])\n",
    "\n",
    "        # Calculate magnetic pressure\n",
    "    mu_0 = constants.mu_0  # Permeability of free space\n",
    "    pmag_RTN_4sa = (bmag_RTN_4sa**2) / (2 * mu_0)\n",
    "\n",
    "        # Convert to nPa (nanoPascals) to match your other pressure units\n",
    "    pmag_RTN_4sa = pmag_RTN_4sa * 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013016a-dca8-41f4-a5b9-bfc299a4b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mag_rtn is not None:\n",
    "        # # Get magnetic field and time data for RTN coordinates at full resolution\n",
    "    mag_data_RTN = get_data('psp_fld_l2_mag_RTN')\n",
    "    mag_time_RTN, mag_field_RTN = mag_data_RTN.times, mag_data_RTN.y\n",
    "\n",
    "    # # Access components of magnetic field in RTN coordinates at full resolution\n",
    "    split_vec('psp_fld_l2_mag_RTN')\n",
    "    br_RTN = get_data('psp_fld_l2_mag_RTN_x')\n",
    "    bt_RTN = get_data('psp_fld_l2_mag_RTN_y')\n",
    "    bn_RTN = get_data('psp_fld_l2_mag_RTN_z')\n",
    "    bmag_RTN = np.sqrt(br_RTN.y**2 + bt_RTN.y**2 + bn_RTN.y**2)\n",
    "\n",
    "    br_RTN = br_RTN.y\n",
    "    bt_RTN = bt_RTN.y\n",
    "    bn_RTN = bn_RTN.y\n",
    "\n",
    "    # # Converting the magnetic field time to a timezone-aware datetime object\n",
    "    datetime_mag_RTN = np.array([dt.replace(tzinfo=timezone.utc) for dt in time_datetime(mag_time_RTN)])\n",
    "    \n",
    "    mu_0 = constants.mu_0  # Permeability of free space\n",
    "    # # Calculate magnetic pressure\n",
    "    pmag_RTN = (bmag_RTN**2) / (2 * mu_0)\n",
    "\n",
    "    # # Convert to nPa (nanoPascals) to match your other pressure units\n",
    "    pmag_RTN = pmag_RTN * 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b0b29-f161-4f7f-81ea-e63ebb85ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spe_sf0_pad is not None:\n",
    "            #==============================================================\n",
    "    # Strahl Data Preparation (PAD)\n",
    "    #==============================================================\n",
    "\n",
    "    #Access electron strahl pitch-angle distribution (PAD)\n",
    "    #epad_data is a data structure\n",
    "    epad_data = get_data('psp_spe_EFLUX_VS_PA_E')\n",
    "    epad_PA = get_data('psp_spe_PITCHANGLE')\n",
    "\n",
    "    #access the times by doing whatever you named it.times\n",
    "    epad_times=epad_data.times\n",
    "\n",
    "    #values are always stored in whatever you named it.y\n",
    "    epad_vals = epad_data.y\n",
    "\n",
    "    #Pulling in the epad_PA values by adding \"whatever you named it\".y\n",
    "    epad_PA_vals = epad_PA.y\n",
    "\n",
    "    #Note for E1-E9, electron strahl energy bin is 8, for E10 and above, it's 12\n",
    "    #strahl_energy_index = ebin\n",
    "        #energy_index = strahl_energy_index\n",
    "    if (time_double(trange[0]) < time_double('2021-11-15')):\n",
    "        strahl_energy_index = 8\n",
    "    else:\n",
    "        strahl_energy_index = 10\n",
    "    \n",
    "    energy_index = strahl_energy_index\n",
    "\n",
    "    #This takes whatever energy slice we're looking at and accesses that flux information\n",
    "    #This is a 3D data cube\n",
    "    #The \":\" means take ALL of that row and ALL of that column, and we're taking that whole energy plane\n",
    "    epad_strahl = epad_vals[:,:,energy_index]\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------\\\\\n",
    "    # Convert Unix timestamps to timezone-aware datetime objects in UTC\n",
    "\n",
    "    # Most space science data records time as Unix timestamps. A Unix timestamp is the number of seconds \n",
    "    # that have elapsed since January 1, 1970 (also known as the \"epoch\"). These timestamps are in Coordinated Universal Time (UTC) \n",
    "    # but are considered \"naive\" datetime objects in Python because they lack explicit timezone information.\n",
    "\n",
    "    # To work with these timestamps effectively, especially when combining data from different sources or performing timezone-aware calculations, \n",
    "    # it's important to convert them into timezone-aware datetime objects.\n",
    "\n",
    "    # Here's what the code does:\n",
    "    # 1. Use 'time_datetime(epad_times)' to convert the array of Unix timestamps 'epad_times' into Python datetime objects.\n",
    "    #    This function returns a list of naive datetime objects (without timezone information).\n",
    "    # 2. Use a list comprehension to iterate over each datetime object 'dt' in the list:\n",
    "    #    - For each 'dt', call 'dt.replace(tzinfo=timezone.utc)' to assign the UTC timezone.\n",
    "    #    - This creates a new datetime object that is timezone-aware.\n",
    "    # 3. Convert the list of timezone-aware datetime objects into a NumPy array using 'np.array()' for efficient numerical operations.\n",
    "\n",
    "    # The result is 'datetime_spe', a NumPy array of timezone-aware datetime objects in UTC.\n",
    "    datetime_spe = np.array([dt.replace(tzinfo=timezone.utc) for dt in time_datetime(epad_times)])\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------//\n",
    "\n",
    "    # Prepare the time array to match the dimensions required by pcolormesh.\n",
    "    # pcolormesh requires the X (time rows), Y (pitch angle columns), and color (flux) arrays to have the same shape.\n",
    "    # Our flux data 'epad_strahl' has shape (number of time steps, number of pitch angle bins).\n",
    "    # We expand 'datetime_spe' from shape (N,) to (N, 1) to make it a 2D column vector.\n",
    "\n",
    "    # This creates a column vector with N rows (where N is the number of time steps) and 1 column.\n",
    "    # Then, we repeat each time value across all pitch angle bins to create 'times_spe_repeat' with shape (N, M).\n",
    "    # This aligns the time array with the flux data for proper plotting.\n",
    "\n",
    "    # i.e., We create 'times_spe_repeat' by repeating 'datetime_spe' across pitch angle bins.\n",
    "    # This creates a new array with the same number of columns as the number of pitch angle bins.\n",
    "\n",
    "    #Pro-Tip, in python matrices are indexed as ⭐️ (rows, columns) ⭐️\n",
    "\n",
    "    # So, starting with (e.g.):\n",
    "    # [T1, T2, T3]  # A 1D array (row vector) of timestamps\n",
    "\n",
    "    # We expand it to (N, 1): <--- (using np.expand_dims) \n",
    "    # [[T1],\n",
    "    #  [T2],\n",
    "    #  [T3]]  # A 2D column vector\n",
    "\n",
    "    # We then repeat each of these values across all pitch angle bins to get: <--- (using np.repeat) (Time steps, 12 PA bins)\n",
    "    # [T1, T1, T1, T1, T1, T1, T1, T1, T1, T1, T1, T1]  <--- 12 PA Bins for Time step 1\n",
    "    # [T2, T2, T2, T2, T2, T2, T2, T2, T2, T2, T2, T2]  <--- 12 PA Bins for Time step 2\n",
    "    # [T3, T3, T3, T3, T3, T3, T3, T3, T3, T3, T3, T3]  <--- 12 PA Bins for Time step 3\n",
    "    # ... <--- (continues for all time steps)\n",
    "\n",
    "    # This creates a 2D array where each row corresponds to a timestamp and each column represents a pitch angle bin.\n",
    "\n",
    "    times_spe_repeat = np.repeat(\n",
    "        np.expand_dims(datetime_spe, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        epad_strahl.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "\n",
    "    # Apply logarithmic transformation to the electron PAD data\n",
    "    log_epad_strahl = np.log10(epad_strahl)\n",
    "\n",
    "    #compute centroids\n",
    "    centroids_epad = np.ma.average(epad_PA_vals, \n",
    "                        weights=epad_strahl, \n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73ffaa-141e-4b72-98f8-f589fb7c74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spi_sf00_mom is not None:\n",
    "    #==============================================================\n",
    "    # Proton Calculations\n",
    "    #==============================================================\n",
    "    \n",
    "    #Proton temperature anisotropy from Temperature Tensor\n",
    "\n",
    "    #Access tensor elements -- The temperature is an array of 9 elements. We want to find out how much temp is aligned parallel or perp to the mag field.\n",
    "    T_Tens = get_data('psp_spi_sf00_T_TENSOR_INST') #Obtains the values from the tensor array\n",
    "    T_XX = T_Tens.y[:,0]\n",
    "    T_YY = T_Tens.y[:,1]\n",
    "    T_ZZ = T_Tens.y[:,2]\n",
    "    T_XY = T_Tens.y[:,3]\n",
    "    T_XZ = T_Tens.y[:,4]\n",
    "    T_YZ = T_Tens.y[:,5]\n",
    "\n",
    "    T_YX = T_XY\n",
    "    T_ZX = T_XZ\n",
    "    T_ZY = T_YZ\n",
    "\n",
    "    #Access magnetic field in span-I coordinates\n",
    "    B_spi = get_data('psp_spi_sf00_MAGF_INST')\n",
    "    B_X = B_spi.y[:,0]\n",
    "    B_Y = B_spi.y[:,1]\n",
    "    B_Z = B_spi.y[:,2]\n",
    "    B_mag_XYZ = np.sqrt(B_X**2 + B_Y**2 + B_Z**2)\n",
    "\n",
    "    #Project Tensor onto B field, find perpendicular and parallel components\n",
    "    T_parallel=[]\n",
    "    T_perpendicular=[]\n",
    "    Anisotropy=[]\n",
    "    for i in range(len(B_X)):  #Calculates Tperp and Tpar from the projection of the magnetic field vector\n",
    "        Sum_1=B_X[i]*B_X[i]*T_XX[i]\n",
    "        Sum_2=B_X[i]*B_Y[i]*T_XY[i]\n",
    "        Sum_3=B_X[i]*B_Z[i]*T_XZ[i]\n",
    "        Sum_4=B_Y[i]*B_X[i]*T_YX[i]\n",
    "        Sum_5=B_Y[i]*B_Y[i]*T_YY[i]\n",
    "        Sum_6=B_Y[i]*B_Z[i]*T_YZ[i]\n",
    "        Sum_7=B_Z[i]*B_X[i]*T_ZX[i]\n",
    "        Sum_8=B_Z[i]*B_Y[i]*T_ZY[i]\n",
    "        Sum_9=B_Z[i]*B_Z[i]*T_ZZ[i]    \n",
    "        T_para=((Sum_1+Sum_2+Sum_3+Sum_4+Sum_5+Sum_6+Sum_7+Sum_8+Sum_9)/(B_mag_XYZ[i])**2)\n",
    "        Trace_Temp=(T_XX[i]+T_YY[i]+T_ZZ[i])\n",
    "        T_perp=(Trace_Temp-T_para)/2.0\n",
    "        T_parallel.append((Sum_1+Sum_2+Sum_3+Sum_4+Sum_5+Sum_6+Sum_7+Sum_8+Sum_9)/(B_mag_XYZ[i])**2)\n",
    "        T_perpendicular.append(T_perp)\n",
    "        Anisotropy.append(T_perp/T_para)\n",
    "\n",
    "    #Proton Analysis: Velocity, Energy Flux, and Temperature Anisotropy\n",
    "    #==============================================================\n",
    "\n",
    "    # This section performs the following operations on the proton data:\n",
    "    # 1. Extracts and calculates velocity components (Vr, Vt, Vn) and magnitude\n",
    "    # 2. Accesses proton energy flux data for energy, theta, and phi\n",
    "    # 3. Extracts times, energy flux, and energy values\n",
    "    # 4. Computes centroids for energy, theta, and phi\n",
    "    # 5. Prepares time arrays for plotting\n",
    "    # 6. Calculates temperature anisotropy from temperature tensor and magnetic field data\n",
    "    # Note: Commented out code for interpolation to sf00 fit file cadence\n",
    "\n",
    "    #Access components of velocity\n",
    "    split_vec('psp_spi_sf00_VEL_RTN_SUN') #split_vec saparates the three components out, we're in RTN so \"x\" becomes our \"r\" and so on...\n",
    "    vr_ = get_data('psp_spi_sf00_VEL_RTN_SUN_x')\n",
    "    vr = vr_.y\n",
    "    vt_ = get_data('psp_spi_sf00_VEL_RTN_SUN_y')\n",
    "    vt = vt_.y\n",
    "    vn_ = get_data('psp_spi_sf00_VEL_RTN_SUN_z')\n",
    "    vn = vn_.y\n",
    "    vmag = np.sqrt(vr**2 + vt **2 + vn**2) #Here we're defining the magnitude of this vector, the data is stored in the .y component\n",
    "\n",
    "    #In the future we'll create a function that computes the magnitude.\n",
    "\n",
    "    #access proton energy flux vs energy -- For the ions we want to look at the energy, theta and phi planes...\n",
    "    spi_eflux_v_energy = get_data('psp_spi_sf00_EFLUX_VS_ENERGY')\n",
    "    spi_eflux_v_theta = get_data('psp_spi_sf00_EFLUX_VS_THETA')\n",
    "    spi_eflux_v_phi = get_data('psp_spi_sf00_EFLUX_VS_PHI')\n",
    "\n",
    "    spi_times = spi_eflux_v_energy.times #extracting the times\n",
    "\n",
    "    spi_nrg_flux = spi_eflux_v_energy.y #energy flux vs energy, this is the data that will go into the color bar\n",
    "\n",
    "    # Apply logarithmic transformation to the spi nrg flux vs nrg data\n",
    "    log_spi_nrg_flux = np.log10(spi_nrg_flux)\n",
    "\n",
    "    spi_nrg_vals = spi_eflux_v_energy.v #energy flux vs the actual energy values, this is the y axis. / The energy values for the  32 channels coppied at every time step (they do not change)\n",
    "\n",
    "    #When we're looking at eflux vs e, at each time step we have the amount of flux and at what energy did we measure that flux.\n",
    "    #Energy vs phi, we have the amount of flux and at which phi value did we measure that flux.\n",
    "\n",
    "    spi_nrg_flux_theta = spi_eflux_v_theta.y\n",
    "    log_spi_nrg_flux_theta = np.log10(spi_nrg_flux_theta)\n",
    "    spi_nrg_vals_theta = spi_eflux_v_theta.v\n",
    "    spi_nrg_flux_phi = spi_eflux_v_phi.y\n",
    "    log_spi_nrg_flux_phi = np.log10(spi_nrg_flux_phi)\n",
    "    spi_nrg_vals_phi = spi_eflux_v_phi.v\n",
    "\n",
    "    datetime_spi = time_datetime(spi_times)\n",
    "    print(datetime_spi[0])\n",
    "    print(datetime_spi[-1])\n",
    "\n",
    "    times_spi_repeat = np.repeat(np.expand_dims(datetime_spi,1),32,1)\n",
    "    times_spi_repeat_angle = np.repeat(np.expand_dims(datetime_spi,1),8,1) \n",
    "\n",
    "    #compute centroids\n",
    "    centroids_spi_nrg = np.ma.average(spi_nrg_vals, \n",
    "                           weights=spi_nrg_flux, \n",
    "                           axis=1)\n",
    "    centroids_spi_theta = np.ma.average(spi_nrg_vals_theta, \n",
    "                           weights=spi_nrg_flux_theta, \n",
    "                           axis=1)\n",
    "    centroids_spi_phi = np.ma.average(spi_nrg_vals_phi, \n",
    "                           weights=spi_nrg_flux_phi, \n",
    "                           axis=1)\n",
    "\n",
    "\n",
    "    # Proton Plasma Parameters Calculation\n",
    "    #==============================================================\n",
    "    # This cell calculates various plasma parameters for protons:\n",
    "    # 1. Extracts plasma density and magnetic field data\n",
    "    # 2. Calculates magnetic field magnitude\n",
    "    # 3. Computes Alfvén speed\n",
    "    # 4. Calculates parallel and perpendicular plasma beta\n",
    "    # 5. Computes parallel and perpendicular plasma pressure\n",
    "    # Note: All calculations use data from SPAN-Ion (SPI) instrument\n",
    "\n",
    "    dens_spi = get_data('psp_spi_sf00_DENS') #the density of the plasma measured by span ion in units of cm^-3\n",
    "    b_spi = get_data('psp_spi_sf00_MAGF_INST') #the magnetic field projected in the instrument frame at the same cadence as span ion\n",
    "\n",
    "    # Calculate the magnitude of b_spi\n",
    "    b_mag_spi = np.sqrt(np.sum(b_spi.y**2, axis=1))\n",
    "\n",
    "    #The alfven speed\n",
    "    v_alfven_spi = 21.8 * b_mag_spi/ np.sqrt(dens_spi.y) #alfven speed computed using spi density\n",
    "\n",
    "    #The velocity of the solar wind\n",
    "    v_sw_spi = np.sqrt(vr**2 + vt**2 + vn**2)\n",
    "\n",
    "    # The Alfvén Mach number\n",
    "    M_alfven_spi = v_sw_spi / v_alfven_spi\n",
    "\n",
    "    #The Temperature\n",
    "    temp_spi = get_data('psp_spi_sf00_TEMP')\n",
    "\n",
    "    #The definition of beta is the particle pressure over the magnetic field pressure\n",
    "    #p is for proton and this is the parallel and perpendicular beta\n",
    "    beta_ppar_spi = (4.03E-11*dens_spi.y*T_parallel)/(1e-5*b_mag_spi)**2\n",
    "    beta_pperp_spi = (4.03E-11*dens_spi.y*T_perpendicular)/(1e-5*b_mag_spi)**2\n",
    "    beta_p_spi = (4.03E-11*dens_spi.y*temp_spi.y)/(1e-5*b_mag_spi)**2\n",
    "\n",
    "\n",
    "    pressure_ppar_spi = 1.602E-4*dens_spi.y*T_parallel #the original number was 1.602E-13, this is the converstion factor for electron vots per cubic CM to pascals, then we multiplied it by 1E9 (1*10^9) to convert to nanopascals \n",
    "    pressure_pperp_spi = 1.602E-4*dens_spi.y*T_perpendicular\n",
    "    pressure_p_spi = 1.602E-4*temp_spi.y*dens_spi.y \n",
    "    #Pperp over ppar eventually\n",
    "\n",
    "    # Calculate magnetic pressure using Bmag_spi cadence (for tot pressure calc)\n",
    "    mu_0 = constants.mu_0  # Permeability of free space\n",
    "    pmag_spi = (b_mag_spi**2) / (2 * mu_0)\n",
    "        # Convert to nPa (nanoPascals) to match your other pressure units\n",
    "    pmag_spi = pmag_spi * 1e9\n",
    "    #total pressure of magnetic plus proton (later add more terms like alpha and elec)\n",
    "    pressure_tot_mag_p = pressure_p_spi + pmag_spi\n",
    "\n",
    "\n",
    "    #dostance from sun\n",
    "    sun_dist_ = get_data('psp_spi_sf00_SUN_DIST')\n",
    "    sun_dist_km = sun_dist_.y\n",
    "    sun_dist_rsun = sun_dist_km/695700.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec0adc-da3a-4612-8019-a410944828cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spi_sf0a_mom is not None:\n",
    "    #==============================================================\n",
    "    # Proton Calculations\n",
    "    #==============================================================\n",
    "    \n",
    "    #Proton temperature anisotropy from Temperature Tensor\n",
    "\n",
    "    #Access tensor elements -- The temperature is an array of 9 elements. We want to find out how much temp is aligned parallel or perp to the mag field.\n",
    "    T_Tens_sf0a = get_data('psp_spi_sf0a_T_TENSOR_INST') #Obtains the values from the tensor array\n",
    "    T_XX_sf0a = T_Tens_sf0a.y[:,0]\n",
    "    T_YY_sf0a = T_Tens_sf0a.y[:,1]\n",
    "    T_ZZ_sf0a = T_Tens_sf0a.y[:,2]\n",
    "    T_XY_sf0a = T_Tens_sf0a.y[:,3]\n",
    "    T_XZ_sf0a = T_Tens_sf0a.y[:,4]\n",
    "    T_YZ_sf0a = T_Tens_sf0a.y[:,5]\n",
    "\n",
    "    T_YX_sf0a = T_XY_sf0a\n",
    "    T_ZX_sf0a = T_XZ_sf0a\n",
    "    T_ZY_sf0a = T_YZ_sf0a\n",
    "\n",
    "    #Access magnetic field in span-I coordinates\n",
    "    B_spi_sf0a = get_data('psp_spi_sf0a_MAGF_INST')\n",
    "    B_X_sf0a = B_spi_sf0a.y[:,0]\n",
    "    B_Y_sf0a = B_spi_sf0a.y[:,1]\n",
    "    B_Z_sf0a = B_spi_sf0a.y[:,2]\n",
    "    B_mag_XYZ_sf0a = np.sqrt(B_X_sf0a**2 + B_Y_sf0a**2 + B_Z_sf0a**2)\n",
    "\n",
    "    #Project Tensor onto B field, find perpendicular and parallel components\n",
    "    T_parallel_sf0a=[]\n",
    "    T_perpendicular_sf0a=[]\n",
    "    Anisotropy_sf0a=[]\n",
    "    for i in range(len(B_X_sf0a)):  #Calculates Tperp and Tpar from the projection of the magnetic field vector\n",
    "        Sum_1_sf0a=B_X_sf0a[i]*B_X_sf0a[i]*T_XX_sf0a[i]\n",
    "        Sum_2_sf0a=B_X_sf0a[i]*B_Y_sf0a[i]*T_XY_sf0a[i]\n",
    "        Sum_3_sf0a=B_X_sf0a[i]*B_Z_sf0a[i]*T_XZ_sf0a[i]\n",
    "        Sum_4_sf0a=B_Y_sf0a[i]*B_X_sf0a[i]*T_YX_sf0a[i]\n",
    "        Sum_5_sf0a=B_Y_sf0a[i]*B_Y_sf0a[i]*T_YY_sf0a[i]\n",
    "        Sum_6_sf0a=B_Y_sf0a[i]*B_Z_sf0a[i]*T_YZ_sf0a[i]\n",
    "        Sum_7_sf0a=B_Z_sf0a[i]*B_X_sf0a[i]*T_ZX_sf0a[i]\n",
    "        Sum_8_sf0a=B_Z_sf0a[i]*B_Y_sf0a[i]*T_ZY_sf0a[i]\n",
    "        Sum_9_sf0a=B_Z_sf0a[i]*B_Z_sf0a[i]*T_ZZ_sf0a[i]    \n",
    "        T_para_sf0a=((Sum_1_sf0a+Sum_2_sf0a+Sum_3_sf0a+Sum_4_sf0a+Sum_5_sf0a+Sum_6_sf0a+Sum_7_sf0a+Sum_8_sf0a+Sum_9_sf0a)/(B_mag_XYZ_sf0a[i])**2)\n",
    "        Trace_Temp_sf0a=(T_XX_sf0a[i]+T_YY_sf0a[i]+T_ZZ_sf0a[i])\n",
    "        T_perp_sf0a=(Trace_Temp_sf0a-T_para_sf0a)/2.0\n",
    "        T_parallel_sf0a.append((Sum_1_sf0a+Sum_2_sf0a+Sum_3_sf0a+Sum_4_sf0a+Sum_5_sf0a+Sum_6_sf0a+Sum_7_sf0a+Sum_8_sf0a+Sum_9_sf0a)/(B_mag_XYZ_sf0a[i])**2)\n",
    "        T_perpendicular_sf0a.append(T_perp_sf0a)\n",
    "        Anisotropy_sf0a.append(T_perp_sf0a/T_para_sf0a)\n",
    "\n",
    "    #Proton Analysis: Velocity, Energy Flux, and Temperature Anisotropy\n",
    "    #==============================================================\n",
    "\n",
    "    # This section performs the following operations on the proton data:\n",
    "    # 1. Extracts and calculates velocity components (Vr, Vt, Vn) and magnitude\n",
    "    # 2. Accesses proton energy flux data for energy, theta, and phi\n",
    "    # 3. Extracts times, energy flux, and energy values\n",
    "    # 4. Computes centroids for energy, theta, and phi\n",
    "    # 5. Prepares time arrays for plotting\n",
    "    # 6. Calculates temperature anisotropy from temperature tensor and magnetic field data\n",
    "    # Note: Commented out code for interpolation to sf00 fit file cadence\n",
    "\n",
    "    #Access components of velocity\n",
    "    split_vec('psp_spi_sf0a_VEL_RTN_SUN') #split_vec saparates the three components out, we're in RTN so \"x\" becomes our \"r\" and so on...\n",
    "    vr_sf0a_ = get_data('psp_spi_sf0a_VEL_RTN_SUN_x')\n",
    "    vr_sf0a = vr_sf0a_.y\n",
    "    vt_sf0a_ = get_data('psp_spi_sf0a_VEL_RTN_SUN_y')\n",
    "    vt_sf0a = vt_sf0a_.y\n",
    "    vn_sf0a_ = get_data('psp_spi_sf0a_VEL_RTN_SUN_z')\n",
    "    vn_sf0a = vn_sf0a_.y\n",
    "    vmag_sf0a = np.sqrt(vr_sf0a**2 + vt_sf0a **2 + vn_sf0a**2) #Here we're defining the magnitude of this vector, the data is stored in the .y component\n",
    "\n",
    "    #In the future we'll create a function that computes the magnitude.\n",
    "\n",
    "    #access proton energy flux vs energy -- For the ions we want to look at the energy, theta and phi planes...\n",
    "    spi_sf0a_eflux_v_energy = get_data('psp_spi_sf0a_EFLUX_VS_ENERGY')\n",
    "    spi_sf0a_eflux_v_theta = get_data('psp_spi_sf0a_EFLUX_VS_THETA')\n",
    "    spi_sf0a_eflux_v_phi = get_data('psp_spi_sf0a_EFLUX_VS_PHI')\n",
    "\n",
    "    spi_sf0a_times = spi_sf0a_eflux_v_energy.times #extracting the times\n",
    "\n",
    "    spi_sf0a_nrg_flux = spi_sf0a_eflux_v_energy.y #energy flux vs energy, this is the data that will go into the color bar\n",
    "\n",
    "    # Apply logarithmic transformation to the spi nrg flux vs nrg data\n",
    "    log_spi_sf0a_nrg_flux = np.log10(spi_sf0a_nrg_flux)\n",
    "\n",
    "    spi_sf0a_nrg_vals = spi_sf0a_eflux_v_energy.v #energy flux vs the actual energy values, this is the y axis. / The energy values for the  32 channels coppied at every time step (they do not change)\n",
    "\n",
    "    #When we're looking at eflux vs e, at each time step we have the amount of flux and at what energy did we measure that flux.\n",
    "    #Energy vs phi, we have the amount of flux and at which phi value did we measure that flux.\n",
    "\n",
    "    spi_sf0a_nrg_flux_theta = spi_sf0a_eflux_v_theta.y\n",
    "    log_spi_sf0a_nrg_flux_theta = np.log10(spi_sf0a_nrg_flux_theta)\n",
    "    spi_sf0a_nrg_vals_theta = spi_sf0a_eflux_v_theta.v\n",
    "    spi_sf0a_nrg_flux_phi = spi_sf0a_eflux_v_phi.y\n",
    "    log_spi_sf0a_nrg_flux_phi = np.log10(spi_sf0a_nrg_flux_phi)\n",
    "    spi_sf0a_nrg_vals_phi = spi_sf0a_eflux_v_phi.v\n",
    "\n",
    "    datetime_spi_sf0a = time_datetime(spi_sf0a_times)\n",
    "    print(datetime_spi_sf0a[0])\n",
    "    print(datetime_spi_sf0a[-1])\n",
    "\n",
    "    times_spi_sf0a_repeat = np.repeat(np.expand_dims(datetime_spi_sf0a,1),32,1)\n",
    "    times_spi_sf0a_repeat_angle = np.repeat(np.expand_dims(datetime_spi_sf0a,1),8,1) \n",
    "\n",
    "    #compute centroids\n",
    "    centroids_spi_sf0a_nrg = np.ma.average(spi_sf0a_nrg_vals, \n",
    "                           weights=spi_sf0a_nrg_flux, \n",
    "                           axis=1)\n",
    "    centroids_spi_sf0a_theta = np.ma.average(spi_sf0a_nrg_vals_theta, \n",
    "                           weights=spi_sf0a_nrg_flux_theta, \n",
    "                           axis=1)\n",
    "    centroids_spi_sf0a_phi = np.ma.average(spi_sf0a_nrg_vals_phi, \n",
    "                           weights=spi_sf0a_nrg_flux_phi, \n",
    "                           axis=1)\n",
    "\n",
    "\n",
    "    # Proton Plasma Parameters Calculation\n",
    "    #==============================================================\n",
    "    # This cell calculates various plasma parameters for protons:\n",
    "    # 1. Extracts plasma density and magnetic field data\n",
    "    # 2. Calculates magnetic field magnitude\n",
    "    # 3. Computes Alfvén speed\n",
    "    # 4. Calculates parallel and perpendicular plasma beta\n",
    "    # 5. Computes parallel and perpendicular plasma pressure\n",
    "    # Note: All calculations use data from SPAN-Ion (spi_sf0a) instrument\n",
    "\n",
    "    dens_spi_sf0a = get_data('psp_spi_sf0a_DENS') #the density of the plasma measured by span ion in units of cm^-3\n",
    "    b_spi_sf0a = get_data('psp_spi_sf0a_MAGF_INST') #the magnetic field projected in the instrument frame at the same cadence as span ion\n",
    "\n",
    "    # Calculate the magnitude of b_spi_sf0a\n",
    "    b_mag_spi_sf0a = np.sqrt(np.sum(b_spi_sf0a.y**2, axis=1))\n",
    "\n",
    "    #The alfven speed\n",
    "    v_alfven_spi_sf0a = 21.8 * b_mag_spi_sf0a/ np.sqrt(dens_spi_sf0a.y) #alfven speed computed using spi_sf0a density\n",
    "\n",
    "\n",
    "    # The Alfvén Mach number (alphas)\n",
    "    M_alfven_spi_sf0a = vmag_sf0a / v_alfven_spi_sf0a\n",
    "\n",
    "    #The Temperature\n",
    "    temp_spi_sf0a = get_data('psp_spi_sf0a_TEMP')\n",
    "\n",
    "    #The definition of beta is the particle pressure over the magnetic field pressure\n",
    "    #p is for proton and this is the parallel and perpendicular beta\n",
    "    beta_ppar_spi_sf0a = (4.03E-11*dens_spi_sf0a.y*T_parallel_sf0a)/(1e-5*b_mag_spi_sf0a)**2\n",
    "    beta_pperp_spi_sf0a = (4.03E-11*dens_spi_sf0a.y*T_perpendicular_sf0a)/(1e-5*b_mag_spi_sf0a)**2\n",
    "    beta_p_spi_sf0a = (4.03E-11*dens_spi_sf0a.y*temp_spi_sf0a.y)/(1e-5*b_mag_spi_sf0a)**2\n",
    "\n",
    "\n",
    "    pressure_ppar_spi_sf0a = 1.602E-4*dens_spi_sf0a.y*T_parallel_sf0a #the original number was 1.602E-13, this is the converstion factor for electron vots per cubic CM to pascals, then we multiplied it by 1E9 (1*10^9) to convert to nanopascals \n",
    "    pressure_pperp_spi_sf0a = 1.602E-4*dens_spi_sf0a.y*T_perpendicular_sf0a\n",
    "    pressure_p_spi_sf0a = 1.602E-4*temp_spi_sf0a.y*dens_spi_sf0a.y \n",
    "    #Pperp over ppar eventually\n",
    "\n",
    "    # Calculate magnetic pressure using Bmag_spi_sf0a cadence (for tot pressure calc)\n",
    "    mu_0 = constants.mu_0  # Permeability of free space\n",
    "    pmag_spi_sf0a = (b_mag_spi_sf0a**2) / (2 * mu_0)\n",
    "        # Convert to nPa (nanoPascals) to match your other pressure units\n",
    "    pmag_spi_sf0a = pmag_spi_sf0a * 1e9\n",
    "    #total pressure of magnetic plus proton (later add more terms like alpha and elec)\n",
    "    pressure_tot_mag_p_sf0a = pressure_p_spi_sf0a + pmag_spi_sf0a\n",
    "\n",
    "\n",
    "    #dostance from sun\n",
    "    sun_dist_sf0a_ = get_data('psp_spi_sf0a_SUN_DIST')\n",
    "    sun_dist_sf0a_km = sun_dist_sf0a_.y\n",
    "    sun_dist_sf0a_rsun = sun_dist_sf0a_km/695700.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e50d25-8991-4ea0-a1aa-1b211e31e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (spi_sf00_mom and spi_sf0a_mom) is not None:\n",
    "    divide('psp_spi_sf0a_DENS','psp_spi_sf00_DENS','Na/Np')\n",
    "    na_div_np_ = get_data('Na/Np')\n",
    "    na_div_np = na_div_np_.y\n",
    "    \n",
    "    #calculate alpha-proton drift speed as |Valphas|-|Vsw|. \n",
    "    store_data('|Vsw|',data={'x':vr_.times, 'y': v_sw_spi})\n",
    "    store_data('|Valpha|',data={'x':vr_sf0a_.times, 'y': vmag_sf0a})\n",
    "    \n",
    "    #Note alphas are slower cadence than the protons, so need to interpolate first.\n",
    "    tinterpol('|Vsw|','|Valpha|',newname = 'va-vp')\n",
    "    \n",
    "    vdrift_ap_ = get_data('va-vp')\n",
    "    vdrift_ap = vdrift_ap_.y\n",
    "    \n",
    "    store_data('valfven_spi', data={'x':vr_.times, 'y': v_alfven_spi})\n",
    "    #normalize by alfven speed\n",
    "    divide('va-vp','valfven_spi','va-vp/vA')\n",
    "    \n",
    "    vdrift_ap_va_ = get_data('va-vp/vA')\n",
    "    vdrift_ap_va = vdrift_ap_va_.y \n",
    "    \n",
    "\n",
    "    #calculate Alpha/proton temperature ratio\n",
    "    divide('psp_spi_sf0a_TEMP','psp_spi_sf00_TEMP','Ta/Tp')\n",
    "    ta_div_tp_ = get_data('Ta/Tp')\n",
    "    ta_div_tp = ta_div_tp_.y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ba7bb-91b3-4e11-b6a3-f6458e467df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wvpow is not None:\n",
    "    wvpow_LH_dat = get_data('wavePower_LH')\n",
    "    wvpow_times = wvpow_LH_dat.times\n",
    "    datetime_wvpow = time_datetime(wvpow_times)\n",
    "    wvpow_LH = wvpow_LH_dat.y\n",
    "    wvpow_RH_dat = get_data('wavePower_RH')\n",
    "    wvpow_RH = wvpow_RH_dat.y\n",
    "    wvpow_LH_spi = downsample_time_based(datetime_wvpow, wvpow_LH, datetime_spi)\n",
    "    wvpow_RH_spi = downsample_time_based(datetime_wvpow, wvpow_RH, datetime_spi)\n",
    "    \n",
    "    #histograms\n",
    "    tau_30s = 30 #30 sec\n",
    "    h_RH_30s, bin_centers_datetime_utc_RH_30s, bin_centers_vals_RH_30s=lista_hista2d(datetime_wvpow,np.log10(wvpow_RH),density=True, cmax=1e-3,tau=tau_30s, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Reds')\n",
    "    h_LH_30s, bin_centers_datetime_utc_LH_30s, bin_centers_vals_LH_30s=lista_hista2d(datetime_wvpow,np.log10(wvpow_LH),density=True, cmax=1e-3,tau=tau_30s, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Blues')\n",
    "    \n",
    "    tau_2m = 2*60 #2 min\n",
    "    h_RH_2m, bin_centers_datetime_utc_RH_2m, bin_centers_vals_RH_2m=lista_hista2d(datetime_wvpow,np.log10(wvpow_RH),density=True, cmax=1e-3,tau=tau_2m, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Reds')\n",
    "    h_LH_2m, bin_centers_datetime_utc_LH_2m, bin_centers_vals_LH_2m=lista_hista2d(datetime_wvpow,np.log10(wvpow_LH),density=True, cmax=1e-3,tau=tau_2m, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Blues')\n",
    "    \n",
    "    tau_20m = 20*60 #20 min\n",
    "    h_RH_20m, bin_centers_datetime_utc_RH_20m, bin_centers_vals_RH_20m=lista_hista2d(datetime_wvpow,np.log10(wvpow_RH),density=True, cmax=1e-3,tau=tau_20m, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Reds')\n",
    "    h_LH_20m, bin_centers_datetime_utc_LH_20m, bin_centers_vals_LH_20m=lista_hista2d(datetime_wvpow,np.log10(wvpow_LH),density=True, cmax=1e-3,tau=tau_20m, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Blues')\n",
    "\n",
    "    tau_90m = 90*60 #90 min\n",
    "    h_RH_90m, bin_centers_datetime_utc_RH_90m, bin_centers_vals_RH_90m=lista_hista2d(datetime_wvpow,np.log10(wvpow_RH),density=True, cmax=1e-3,tau=tau_90m, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Reds')\n",
    "    h_LH_90m, bin_centers_datetime_utc_LH_90m, bin_centers_vals_LH_90m=lista_hista2d(datetime_wvpow,np.log10(wvpow_LH),density=True, cmax=1e-3,tau=tau_90m, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Blues')\n",
    "\n",
    "    tau_4h = 4*60*60 #4 hr\n",
    "    h_RH_4h, bin_centers_datetime_utc_RH_4h, bin_centers_vals_RH_4h=lista_hista2d(datetime_wvpow,np.log10(wvpow_RH),density=True, cmax=1e-3,tau=tau_4h, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Reds')\n",
    "    h_LH_4h, bin_centers_datetime_utc_LH_4h, bin_centers_vals_LH_4h=lista_hista2d(datetime_wvpow,np.log10(wvpow_LH),density=True, cmax=1e-3,tau=tau_4h, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Blues')\n",
    "\n",
    "    tau_12h = 12*60*60 #12 hr\n",
    "    h_RH_12h, bin_centers_datetime_utc_RH_12h, bin_centers_vals_RH_12h=lista_hista2d(datetime_wvpow,np.log10(wvpow_RH),density=True, cmax=1e-3,tau=tau_12h, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Reds')\n",
    "    h_LH_12h, bin_centers_datetime_utc_LH_12h, bin_centers_vals_LH_12h=lista_hista2d(datetime_wvpow,np.log10(wvpow_LH),density=True, cmax=1e-3,tau=tau_12h, range = [[wvpow_times[0],wvpow_times[-1]],[-3,3]],cmap='Blues')\n",
    "\n",
    "    # This creates a 2D array where each row corresponds to a timestamp and each column represents a pitch angle bin.\n",
    "    times_RH_repeat_30s = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_RH_30s, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_30s.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    times_LH_repeat_30s = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_LH_30s, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_30s.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    \n",
    "    times_RH_repeat_2m = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_RH_2m, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_2m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    times_LH_repeat_2m = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_LH_2m, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_2m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    \n",
    "    times_RH_repeat_20m = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_RH_20m, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_20m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    times_LH_repeat_20m = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_LH_20m, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_20m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    # This creates a 2D array where each row corresponds to a timestamp and each column represents a pitch angle bin.\n",
    "    times_RH_repeat_90m = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_RH_90m, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_90m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    times_LH_repeat_90m= np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_LH_90m, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_90m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    # This creates a 2D array where each row corresponds to a timestamp and each column represents a pitch angle bin.\n",
    "    times_RH_repeat_4h = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_RH_4h, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_4h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    times_LH_repeat_4h = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_LH_4h, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_4h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    times_RH_repeat_12h = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_RH_12h, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_12h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    times_LH_repeat_12h = np.repeat(\n",
    "        np.expand_dims(bin_centers_datetime_utc_LH_12h, 1),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_12h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 1                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    \n",
    "    ydat_RH_30s = np.power(10,bin_centers_vals_RH_30s)\n",
    "    ydat_RH_repeat_30s = np.repeat(\n",
    "        np.expand_dims(ydat_RH_30s, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_30s.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_LH_30s = np.power(10,bin_centers_vals_LH_30s)\n",
    "    ydat_LH_repeat_30s = np.repeat(\n",
    "        np.expand_dims(ydat_LH_30s, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_30s.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )   \n",
    "    ydat_RH_2m = np.power(10,bin_centers_vals_RH_2m)\n",
    "    ydat_RH_repeat_2m = np.repeat(\n",
    "        np.expand_dims(ydat_RH_2m, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_2m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_LH_2m = np.power(10,bin_centers_vals_LH_2m)\n",
    "    ydat_LH_repeat_2m = np.repeat(\n",
    "        np.expand_dims(ydat_LH_2m, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_2m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "\n",
    "    ydat_RH_20m = np.power(10,bin_centers_vals_RH_20m)\n",
    "    ydat_RH_repeat_20m = np.repeat(\n",
    "        np.expand_dims(ydat_RH_20m, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_20m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_LH_20m = np.power(10,bin_centers_vals_LH_20m)\n",
    "    ydat_LH_repeat_20m = np.repeat(\n",
    "        np.expand_dims(ydat_LH_20m, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_20m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_RH_90m = np.power(10,bin_centers_vals_RH_90m)\n",
    "    ydat_RH_repeat_90m = np.repeat(\n",
    "        np.expand_dims(ydat_RH_90m, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_90m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_LH_90m = np.power(10,bin_centers_vals_LH_90m)\n",
    "    ydat_LH_repeat_90m = np.repeat(\n",
    "        np.expand_dims(ydat_LH_90m, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_90m.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_RH_4h = np.power(10,bin_centers_vals_RH_4h)\n",
    "    ydat_RH_repeat_4h = np.repeat(\n",
    "        np.expand_dims(ydat_RH_4h, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_4h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_LH_4h = np.power(10,bin_centers_vals_LH_4h)\n",
    "    ydat_LH_repeat_4h = np.repeat(\n",
    "        np.expand_dims(ydat_LH_4h, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_4h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_RH_12h = np.power(10,bin_centers_vals_RH_12h)\n",
    "    ydat_RH_repeat_12h = np.repeat(\n",
    "        np.expand_dims(ydat_RH_12h, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_RH_12h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "    ydat_LH_12h = np.power(10,bin_centers_vals_LH_12h)\n",
    "    ydat_LH_repeat_12h = np.repeat(\n",
    "        np.expand_dims(ydat_LH_12h, 0),  # Input array: expanded time array from (N,) to (N, 1) where N is the number of time steps\n",
    "        h_LH_12h.shape[1],             # Number of repetitions (e.g. 12 pitch angle bins)\n",
    "        axis = 0                          # Repeat along axis=1 (columns)\n",
    "    )\n",
    "\n",
    "    #compute centroids\n",
    "    centroids_RH_30s = np.ma.average(ydat_RH_repeat_30s,axis=1, \n",
    "                        weights=h_RH_30s.T)\n",
    "    centroids_LH_30s = np.ma.average(ydat_LH_repeat_30s,axis=1, \n",
    "                        weights=h_LH_30s.T)\n",
    "    centroids_RH_2m = np.ma.average(ydat_RH_repeat_2m,axis=1, \n",
    "                        weights=h_RH_2m.T)\n",
    "    centroids_LH_2m = np.ma.average(ydat_LH_repeat_2m,axis=1, \n",
    "                        weights=h_LH_2m.T)\n",
    "    centroids_RH_20m = np.ma.average(ydat_RH_repeat_20m,axis=1, \n",
    "                        weights=h_RH_20m.T)\n",
    "    centroids_LH_20m = np.ma.average(ydat_LH_repeat_20m,axis=1, \n",
    "                        weights=h_LH_20m.T)\n",
    "    centroids_RH_90m = np.ma.average(ydat_RH_repeat_90m,axis=1, \n",
    "                        weights=h_RH_90m.T)\n",
    "    centroids_LH_90m = np.ma.average(ydat_LH_repeat_90m,axis=1, \n",
    "                        weights=h_LH_90m.T)\n",
    "    centroids_RH_4h = np.ma.average(ydat_RH_repeat_4h,axis=1, \n",
    "                        weights=h_RH_4h.T)\n",
    "    centroids_LH_4h = np.ma.average(ydat_LH_repeat_4h,axis=1, \n",
    "                        weights=h_LH_4h.T)\n",
    "    centroids_RH_12h = np.ma.average(ydat_RH_repeat_12h,axis=1, \n",
    "                        weights=h_RH_12h.T)\n",
    "    centroids_LH_12h = np.ma.average(ydat_LH_repeat_12h,axis=1, \n",
    "                        weights=h_LH_12h.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897f470-1dd6-4450-98e7-c88e155d49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sf00_fits is not None:\n",
    "    filtered_val_index_sf00 = np.where(\n",
    "        (df_sf00['np1'] > 10) &\n",
    "        (df_sf00['np2'] > 10) &\n",
    "        (df_sf00['Tperp1'] > .03) &\n",
    "        (df_sf00['Tperp2'] > .03) &\n",
    "        (df_sf00['Trat1'] > .01) &\n",
    "        (df_sf00['Trat2'] > .01) &\n",
    "        (df_sf00['Trat1'] != 30) &\n",
    "        (df_sf00['Trat2'] != 30) &\n",
    "        (df_sf00['Trat1'] != 2.0) &\n",
    "        (df_sf00['Trat2'] != 2.0) &\n",
    "        (df_sf00['Trat1'] != 1.0) &\n",
    "        (df_sf00['Trat2'] != 1.0) &\n",
    "        (df_sf00['vdrift'] != 10000) &\n",
    "        (df_sf00['vdrift'] != -10000)\n",
    "    )[0]\n",
    "    \n",
    "    # Build a mask of rows you want to KEEP\n",
    "    mask_sf00 = df_sf00.index.isin(filtered_val_index_sf00)\n",
    "\n",
    "    # Get just the data columns (everything except timestamp)\n",
    "    columns_to_mask_sf00 = df_sf00.columns.difference(['time'])  # or whatever your time column is called\n",
    "\n",
    "    # Mask only those columns\n",
    "    df_sf00.loc[~mask_sf00, columns_to_mask_sf00] = np.nan\n",
    "\n",
    "    # Create the 'datetime' column\n",
    "    df_sf00['datetime'] = pd.to_datetime(df_sf00['time'], unit='s', utc=True)\n",
    "    \n",
    "    # Convert 'datetime' column to NumPy array of datetime.datetime objects\n",
    "    datetime_array = df_sf00['datetime'].dt.to_pydatetime()\n",
    "    \n",
    "    if len(datetime_array) < len(datetime_spi):\n",
    "        df_sf00 = upsample_to_match(df_sf00, datetime_array, datetime_spi)\n",
    "        # Convert 'datetime' column to NumPy array of datetime.datetime objects\n",
    "        datetime_array = df_sf00['datetime'].dt.to_pydatetime()\n",
    "    \n",
    "    #==============================================================\n",
    "# sf00 fits\n",
    "#==============================================================\n",
    "\n",
    "    #Access components of velocity\n",
    "    #split_vec('psp_spi_sf00_VEL_RTN_SUN')\n",
    "   # vr = get_data('psp_spi_sf00_VEL_RTN_SUN_x')\n",
    "   # vt = get_data('psp_spi_sf00_VEL_RTN_SUN_y')\n",
    "   # vn = get_data('psp_spi_sf00_VEL_RTN_SUN_z')\n",
    "   # vmag = np.sqrt(vr.y**2 + vt.y **2 + vn.y**2)\n",
    "\n",
    "    df_sf00['vsw_mom'] = vmag #⭐️ The proton moments value needs to be calculatd for vsw_mach_pfits (see below)\n",
    "\n",
    "    #calculate variables\n",
    "    df_sf00['vp1_mag'] = np.sqrt(df_sf00['vp1_x']**2 + df_sf00['vp1_y']**2 + df_sf00['vp1_z']**2) \n",
    "    df_sf00['B_mag'] = np.sqrt(df_sf00['B_inst_x']**2 + df_sf00['B_inst_y']**2 + df_sf00['B_inst_z']**2)\n",
    "    df_sf00['bhat_x'] = df_sf00['B_inst_x']/df_sf00['B_mag']\n",
    "    df_sf00['bhat_y'] = df_sf00['B_inst_y']/df_sf00['B_mag']\n",
    "    df_sf00['bhat_z'] = df_sf00['B_inst_z']/df_sf00['B_mag']\n",
    "    m = 1836 * 511000.0 / (299792.0**2) #proton mass\n",
    "    \n",
    "    df_sf00['n_tot'] = df_sf00['np1'] + df_sf00['np2']\n",
    "    df_sf00['Tperp_tot'] = (df_sf00['np1'] * df_sf00['Tperp1'] + df_sf00['np2'] * df_sf00['Tperp2'])/df_sf00['n_tot']\n",
    "    df_sf00['Tpar1'] = df_sf00['Tperp1']/df_sf00['Trat1']\n",
    "    df_sf00['Tpar2'] = df_sf00['Tperp2']/df_sf00['Trat2']\n",
    "    df_sf00['Tpar_tot'] = (df_sf00['np1'] * df_sf00['Tpar1'] + df_sf00['np2'] * df_sf00['Tpar2'] + (df_sf00['np1'] * df_sf00['np2'] / df_sf00['n_tot'])* m * (df_sf00['vdrift']**2))/df_sf00['n_tot'] \n",
    "    df_sf00['Trat_tot'] = df_sf00['Tperp_tot']/df_sf00['Tpar_tot']\n",
    "    df_sf00['Temp_tot'] = (2*df_sf00['Tperp_tot'] + df_sf00['Tpar_tot'])/3\n",
    "    \n",
    "    df_sf00['vp1_mag'] = np.sqrt(df_sf00['vp1_x']**2 + df_sf00['vp1_y']**2 + df_sf00['vp1_z']**2)\n",
    "    df_sf00['vcm_x'] = df_sf00['vp1_x'] + df_sf00['np1']/df_sf00['np2'] * df_sf00['vdrift'] * df_sf00['bhat_x'] # fitted center of mass velocity (x)\n",
    "    df_sf00['vcm_y'] = df_sf00['vp1_y'] + df_sf00['np1']/df_sf00['np2'] * df_sf00['vdrift'] * df_sf00['bhat_y'] # fitted center of mass velocity (y)\n",
    "    df_sf00['vcm_z'] = df_sf00['vp1_z'] + df_sf00['np1']/df_sf00['np2'] * df_sf00['vdrift'] * df_sf00['bhat_z'] # fitted center of mass velocity (z)\n",
    "    df_sf00['vcm_mag'] = np.sqrt(df_sf00['vcm_x']**2 + df_sf00['vcm_y']**2 + df_sf00['vcm_z']**2)\n",
    "\n",
    "    df_sf00['vp2_x'] = df_sf00['vp1_x'] + df_sf00['vdrift'] * df_sf00['bhat_x'] # fitted center of p2 velocity (x)\n",
    "    df_sf00['vp2_y'] = df_sf00['vp1_y'] + df_sf00['vdrift'] * df_sf00['bhat_y'] # fitted center of p2 velocity (y)\n",
    "    df_sf00['vp2_z'] = df_sf00['vp1_z'] + df_sf00['vdrift'] * df_sf00['bhat_z'] # fitted center of p2 velocity (x)\n",
    "    df_sf00['vp2_mag'] = np.sqrt(df_sf00['vp2_x']**2 + df_sf00['vp2_y']**2 + df_sf00['vp2_z']**2)\n",
    "\n",
    "    df_sf00['vcm_y'] = df_sf00['vp1_y'] + df_sf00['np1']/df_sf00['np2'] * df_sf00['vdrift'] * df_sf00['bhat_y'] # fitted center of mass velocity (y)\n",
    "    df_sf00['vcm_z'] = df_sf00['vp1_z'] + df_sf00['np1']/df_sf00['np2'] * df_sf00['vdrift'] * df_sf00['bhat_z'] # fitted center of mass velocity (z)\n",
    "    df_sf00['vcm_mag'] = np.sqrt(df_sf00['vcm_x']**2 + df_sf00['vcm_y']**2 + df_sf00['vcm_z']**2)\n",
    "\n",
    "    #compute heat flux\n",
    "    m = 1836 * 511000.0 / (299792.0**2)\n",
    "\n",
    "    #convert to thermal speeds\n",
    "    vt1perp2 = 2 * df_sf00['Tperp1']/m \n",
    "    vt2perp2 = 2 * df_sf00['Tperp2']/m \n",
    "    vt1par2 = 2 * df_sf00['Tpar1']/m \n",
    "    vt2par2 = 2 * df_sf00['Tpar2']/m \n",
    "\n",
    "    fac = 1.602E-10 #W/m2 conversion\n",
    "    df_sf00['qz_p'] = fac * 0.5 * m * ((df_sf00['np1'] * df_sf00['np2']) / df_sf00['n_tot']) * df_sf00['vdrift'] * (1.5 * (vt2par2 - vt1par2) + df_sf00['vdrift']**2 * (df_sf00['np1']**2 - df_sf00['np2']**2)/df_sf00['n_tot']**2 + (vt2perp2 - vt1perp2)) #⭐️ heat flux of the proton beam, in watts per meter squared, as blueviolet\n",
    "    df_sf00['|qz_p|'] = np.abs(df_sf00['qz_p'])\n",
    "\n",
    "    #compute normalisation factors\n",
    "\n",
    "    df_sf00['vt_perp_tot'] = np.sqrt(2 * (df_sf00['np1'] * df_sf00['Tperp1'] + df_sf00['np2'] * df_sf00['Tperp2']) / (m * df_sf00['n_tot']))\n",
    "    df_sf00['vt_par_tot'] = np.sqrt(2 * (df_sf00['np1'] * df_sf00['Tpar1']+ df_sf00['np2'] * df_sf00['Tpar2'] + m * df_sf00['vdrift']**2 * (df_sf00['np1'] * df_sf00['np2'])/df_sf00['n_tot']) / (m * df_sf00['n_tot']))\n",
    "\n",
    "    df_sf00['qz_p_perp']= df_sf00['qz_p'] / (m * df_sf00['n_tot'] * df_sf00['vt_perp_tot']**3) # also normalise by mass\n",
    "    df_sf00['qz_p_par'] = df_sf00['qz_p'] / (m * df_sf00['n_tot'] * df_sf00['vt_par_tot']**3)\n",
    "\n",
    "#Compute Alfven Mach number and Proton Parallel, Perp Beta\n",
    "\n",
    "#calculate Alfven speed\n",
    "    df_sf00['valfven_pfits'] = 21.8 * df_sf00['B_mag']/ np.sqrt(df_sf00['n_tot']) #using proton p1 + p2 total density\n",
    "\n",
    "#defining normalized drift speed using fitted density for vA\n",
    "    df_sf00['vdrift_va_pfits'] = df_sf00['vdrift']/df_sf00['valfven_pfits'] \n",
    "    #df_sf00['|valfven_va_pfits|'] = np.abs(df_sf00['valfven_va_pfits'])\n",
    "    #df_sf00['KE_p2'] = 0.5 * df_sf00['|valfven_pfits|']**2\n",
    "\n",
    "#defining center of mass velocity magnitude as |Vsw| compute Vsw mach\n",
    "    df_sf00['Vcm_mach_pfits'] = df_sf00['vcm_mag']/df_sf00['valfven_pfits'] #using valfven from fits\n",
    "#defining p1 velocity magnitude as |Vsw| compute Vsw mach\n",
    "    df_sf00['Vp1_mach_pfits'] = df_sf00['vp1_mag']/df_sf00['valfven_pfits'] #using valfven from fits\n",
    "#defining moment velocity magnitude as |Vsw| compute Vsw mach\n",
    "    df_sf00['vsw_mach_pfits'] = df_sf00['vsw_mom']/df_sf00['valfven_pfits'] #using valfven from fits ⭐️ Taking the solar window value from the moments and normalizing by the alfven speed, using fitted density as the density for alfven speed (Moments interpoalted to the same time cadence as the data file and added to the data frame.)\n",
    "\n",
    "#compute proton beta \n",
    "#note 1e-5 is factor conversion from nT to cgs\n",
    "    df_sf00['beta_ppar_pfits'] = (4.03E-11 * df_sf00['n_tot'] * df_sf00['Tpar_tot'])/(1e-5 * df_sf00['B_mag'])**2 #total BetaPar from pfit dens ⭐️ \n",
    "    df_sf00['beta_pperp_pfits'] = (4.03E-11 * df_sf00['n_tot'] * df_sf00['Tperp_tot'])/(1e-5 * df_sf00['B_mag'])**2 #total BetaPerp from pfit dens ⭐️ \n",
    "    df_sf00['beta_par_p1'] = (4.03E-11 * df_sf00['np1'] * df_sf00['Tpar1'])/(1e-5 * df_sf00['B_mag'])**2 #proton p1 BetaPar\n",
    "    df_sf00['beta_par_p2'] = (4.03E-11 * df_sf00['np2'] * df_sf00['Tpar2'])/(1e-5 * df_sf00['B_mag'])**2 #proton p2 BetaPar\n",
    "    df_sf00['beta_perp_p1'] = (4.03E-11 * df_sf00['np1'] * df_sf00['Tperp1'])/(1e-5 * df_sf00['B_mag'])**2 #proton p1 BetaPerp\n",
    "    df_sf00['beta_perp_p2'] = (4.03E-11 * df_sf00['np2'] * df_sf00['Tperp2'])/(1e-5 * df_sf00['B_mag'])**2 #proton p2 BetaPerp\n",
    "\n",
    "    df_sf00['beta_p_pfits_tot'] = np.sqrt(df_sf00['beta_ppar_pfits']**2 + df_sf00['beta_pperp_pfits']**2)\n",
    "\n",
    "#hammerhead diagnostic\n",
    "    df_sf00['ham_param'] = (df_sf00['Tperp2']/df_sf00['Tperp1'])/df_sf00['Trat_tot'] # ⭐️ Preliminary proxy as a value for the hamplitude, hammer headness. Ratio of the perp temp of the beam to the perp temp of the core, how much hotter the beam is over the core, all that normalized over the total temp anisot of the core and beam. Palevioletred color\n",
    "\n",
    "#--------------------FITS Variables For Plotbot--------------------\\\\\n",
    "##-----------------------------------------------------------------||\n",
    "#Converting from pandas data frames to numpy arrays\n",
    "    qz_p = df_sf00['qz_p'].to_numpy() # Heat flux of the proton beam, in watts per meter squared, as blueviolet\n",
    "    vsw_mach_pfits = df_sf00['vsw_mach_pfits'].to_numpy() # Using valfven from fits. Taking the solar window value from the moments and normalizing by the alfven speed, using fitted density as the density for alfven speed (Moments interpoalted to the same time cadence as the data file and added to the data frame.)\n",
    "    beta_ppar_pfits = df_sf00['beta_ppar_pfits'].to_numpy()  # Total Proton BetaPar from pfit dens. Hot pink color\n",
    "    beta_pperp_pfits = df_sf00['beta_pperp_pfits'].to_numpy() # Total Proton BetaPerp from pfit dens. Baby blue color\n",
    "    ham_param = df_sf00['ham_param'].to_numpy() # Preliminary proxy as a value for the hamplitude, hammer headness. Ratio of the perp temp of the beam to the perp temp of the core, how much hotter the beam is over the core, all that normalized over the total temp anisot of the core and beam. Palevioletred color\n",
    "    \n",
    "    np1 = df_sf00['np1'].to_numpy() # Core density, marker (5,1) makes a star. He likes hot pink and deep skye blue\n",
    "    np2 = df_sf00['np2'].to_numpy() # Beam density -- for s=s, s defines the size of the point, alpha defines the tranparency of the point. When looking at a full day, s 5-10 and trasparency .7, zooming in then s 200 and transparency to be 1 (adap the size dynamically, adjust the size of the symbol based on the number of points plotted).\n",
    "    n_tot = df_sf00['n_tot'].to_numpy() #total beam + core dens\n",
    "    \n",
    "    vp1_x = df_sf00['vp1_x'].to_numpy() # Core velocity vx\n",
    "    vp1_y = df_sf00['vp1_y'].to_numpy() # Core velocity vy\n",
    "    vp1_z = df_sf00['vp1_z'].to_numpy() # Core velocity vz\n",
    "    vp1_mag = df_sf00['vp1_mag'].to_numpy() # Core velocity vmag\n",
    "    vcm_x = df_sf00['vcm_x'].to_numpy() #fitted center of mass vx\n",
    "    vcm_y = df_sf00['vcm_y'].to_numpy() #fitted center of mass vy\n",
    "    vcm_z = df_sf00['vcm_z'].to_numpy() #fitted center of mass vz\n",
    "    vcm_mag = df_sf00['vcm_mag'].to_numpy() #fitted center of mass vmag\n",
    "    vdrift = df_sf00['vdrift'].to_numpy() # Drift speed \n",
    "    vdrift_va_pfits = df_sf00['vdrift_va_pfits'].to_numpy() # Drift speed normalized by the alfven speed. It's useful to have both variables available.\n",
    "    \n",
    "    Trat1 = df_sf00['Trat1'].to_numpy() # Temp anisotropy of the core\n",
    "    Trat2 = df_sf00['Trat2'].to_numpy() # Temp anisotropy of the beam\n",
    "    Trat_tot = df_sf00['Trat_tot'].to_numpy() # Temp anisotropy of the combined core and beam\n",
    "    Tperp1 = df_sf00['Tperp1'].to_numpy() # Temp perp of the core\n",
    "    Tperp2 = df_sf00['Tperp2'].to_numpy() # Temp perp of the beam\n",
    "    Tperp_tot = df_sf00['Tperp_tot'].to_numpy() # Temp perp of the combined core and beam\n",
    "    Tpar1 = df_sf00['Tpar1'].to_numpy() # Temp par of the core\n",
    "    Tpar2 = df_sf00['Tpar2'].to_numpy() # Temp par of the beam\n",
    "    Tpar_tot = df_sf00['Tpar_tot'].to_numpy() # Temp par of the combined core and beam\n",
    "    Temp_tot = df_sf00['Temp_tot'].to_numpy() # Temp of the combined core and beam\n",
    "\n",
    "    abs_qz_p = df_sf00['|qz_p|'].to_numpy()\n",
    "    \n",
    "    #uncertainties\n",
    "    np1_dpar = df_sf00['np1_dpar'].to_numpy() # Core density, marker (5,1) makes a star. He likes hot pink and deep skye blue\n",
    "    np2_dpar = df_sf00['np2_dpar'].to_numpy() # Beam density\n",
    "    vp1_x_dpar = df_sf00['vp1_x_dpar'].to_numpy() # Core velocity vx\n",
    "    vp1_y_dpar = df_sf00['vp1_y_dpar'].to_numpy() # Core velocity vy\n",
    "    vp1_z_dpar = df_sf00['vp1_z_dpar'].to_numpy() # Core velocity vz\n",
    "    vp1_mag_dpar = np.sqrt(vp1_x_dpar**2 + vp1_y_dpar**2 + vp1_z_dpar**2) \n",
    "    vdrift_dpar = df_sf00['vdrift_dpar'].to_numpy() # Drift speed\n",
    "    Trat1_dpar = df_sf00['Trat1_dpar'].to_numpy() # Temp anisotropy of the core\n",
    "    Trat2_dpar = df_sf00['Trat2_dpar'].to_numpy() # Temp anisotropy of the beam\n",
    "    Tperp1_dpar = df_sf00['Tperp1_dpar'].to_numpy() # Temp perp of the core\n",
    "    Tperp2_dpar = df_sf00['Tperp2_dpar'].to_numpy() # Temp perp of the beam\n",
    "    chi_p = df_sf00['chi'].to_numpy() # chi (overall fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd0e11-a011-416b-abf8-3338c61c2ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed9057-2e91-4994-8ce1-d6b16b929d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sf01_fits is not None:\n",
    "    filtered_val_index_sf01 = np.where(\n",
    "        (df_sf01['na'] > 0) & \n",
    "        (df_sf01['Ta_perp'] > .101) & \n",
    "        (df_sf01['Trata'] > .101) & \n",
    "        (df_sf01['Trata'] != 8) & \n",
    "        (df_sf01['Trata'] != 2.0)\n",
    "        )[0]\n",
    "        \n",
    "    # Build a mask of rows you want to KEEP\n",
    "    mask_sf01 = df_sf01.index.isin(filtered_val_index_sf01)\n",
    "\n",
    "    # Get just the data columns (everything except timestamp)\n",
    "    columns_to_mask_sf01 = df_sf01.columns.difference(['time'])  # or whatever your time column is called\n",
    "\n",
    "    # Mask only those columns\n",
    "    df_sf01.loc[~mask_sf01, columns_to_mask_sf01] = np.nan\n",
    "        \n",
    "    # Create the 'datetime' column\n",
    "    df_sf01['datetime'] = pd.to_datetime(df_sf01['time'], unit='s', utc=True)\n",
    "\n",
    "    # Convert 'datetime' column to NumPy array of datetime.datetime objects\n",
    "    datetime_array_sf01 = df_sf01['datetime'].dt.to_pydatetime()\n",
    "    \n",
    "    if len(datetime_array_sf01) < len(datetime_spi_sf0a):\n",
    "        #upsample fits file to match sf0a moment cadence\n",
    "        df_sf01 = upsample_to_match(df_sf01, datetime_array_sf01, datetime_spi_sf0a)\n",
    "        # Convert 'datetime' column to NumPy array of datetime.datetime objects\n",
    "        datetime_array_sf01 = df_sf01['datetime'].dt.to_pydatetime()\n",
    "    \n",
    "    #if sf01 has lower cadence than sf00, \n",
    "    #downsample sf00 quantities for math on sf01\n",
    "    if len(datetime_array_sf01) < len(datetime_array):\n",
    "        vp1_mag_ = downsample_to_match(datetime_array, vp1_mag, datetime_array_sf01)\n",
    "        np1_ = downsample_to_match(datetime_array, np1, datetime_array_sf01)\n",
    "        np2_ = downsample_to_match(datetime_array, np2, datetime_array_sf01)\n",
    "    else:\n",
    "        vp1_mag_ = vp1_mag\n",
    "        np1_ = np1\n",
    "        np2_ = np2\n",
    "\n",
    "    np_tot_ = np1_ + np2_\n",
    "    df_sf01['nap_tot'] = np_tot_ + df_sf01['na']\n",
    "    df_sf01['B_mag'] = np.sqrt(df_sf01['B_inst_x']**2 + df_sf01['B_inst_y']**2 + df_sf01['B_inst_z']**2)\n",
    "    df_sf01['valfven_apfits'] = 21.8 * df_sf01['B_mag']/ np.sqrt(df_sf01['nap_tot']) #using proton and alpha p1 + p2 + a  total density\n",
    "    \n",
    "    #if sf01 has lower cadence than sf00, upsample sf01 quantities for math on sf00\n",
    "    if len(datetime_array_sf01) < len(datetime_array):\n",
    "        valfven_apfits_ = upsample_to_match(df_sf01['valfven_apfits'].to_numpy(), datetime_array_sf01, datetime_array)\n",
    "    else:\n",
    "        valfven_apfits_ = df_sf01['valfven_apfits']\n",
    "    df_sf00['valfven_apfits'] = valfven_apfits_\n",
    "    #redefining normalized drift speed using fitted alpha and proton density for vA\n",
    "    df_sf00['vdrift_va_apfits'] = df_sf00['vdrift']/df_sf00['valfven_apfits'] \n",
    "\n",
    "    df_sf01['va_mag'] = np.sqrt(df_sf01['va_x']**2 + df_sf01['va_y']**2 + df_sf01['va_z']**2)\n",
    "    df_sf01['vdrift_ap1'] = df_sf01['va_mag'] - vp1_mag_\n",
    "    df_sf01['vdrift_va_ap1'] = df_sf01['vdrift_ap1']/df_sf01['valfven_apfits']\n",
    "    df_sf01['na/np_tot'] = df_sf01['na']/np_tot_\n",
    "    df_sf01['na/np1'] = df_sf01['na']/np1_\n",
    "    df_sf01['na/np2'] = df_sf01['na']/np2_\n",
    "    df_sf01['Tpara'] = df_sf01['Ta_perp']/df_sf01['Trata']\n",
    "    df_sf01['beta_par_a'] = (4.03E-11 * df_sf01['na'] * df_sf01['Tpara'])/(1e-5 * df_sf01['B_mag'])**2 #alpha BetaPar\n",
    "    \n",
    "    na = df_sf01['na'].to_numpy()\n",
    "    na_div_nptot = df_sf01['na/np_tot'].to_numpy()\n",
    "    na_div_np1 = df_sf01['na/np1'].to_numpy()\n",
    "    na_div_np2 = df_sf01['na/np2'].to_numpy()\n",
    "    va_x = df_sf01['va_x'].to_numpy()\n",
    "    va_y = df_sf01['va_y'].to_numpy()\n",
    "    va_z = df_sf01['va_z'].to_numpy()\n",
    "    va_z = df_sf01['va_z'].to_numpy()\n",
    "    va_mag = df_sf01['va_mag'].to_numpy()\n",
    "    vdrift_ap1 = df_sf01['vdrift_ap1'].to_numpy()\n",
    "    vdrift_va_ap1 = df_sf01['vdrift_va_ap1'].to_numpy()\n",
    "    vdrift_va_apfits = df_sf00['vdrift_va_apfits'].to_numpy()\n",
    "    Trata = df_sf01['Trata'].to_numpy()\n",
    "    Tperpa = df_sf01['Ta_perp'].to_numpy()\n",
    "    Tpara = df_sf01['Tpara'].to_numpy()\n",
    "    beta_par_a = df_sf01['beta_par_a'].to_numpy()\n",
    "    \n",
    "    #uncertainties\n",
    "    na_dpar = df_sf01['na_dpar'].to_numpy() # Core density, marker (5,1) makes a star. He likes hot pink and deep skye blue\n",
    "    va_x_dpar = df_sf01['va_x_dpar'].to_numpy() # Core velocity vx\n",
    "    va_y_dpar = df_sf01['va_y_dpar'].to_numpy() # Core velocity vy\n",
    "    va_z_dpar = df_sf01['va_z_dpar'].to_numpy() # Core velocity vz\n",
    "    va_mag_dpar = np.sqrt(va_x_dpar**2 + va_y_dpar**2 + va_z_dpar**2) \n",
    "    Trata_dpar = df_sf01['Trata_dpar'].to_numpy() # Temp anisotropy of the core\n",
    "    Tperpa_dpar = df_sf01['Ta_perp_dpar'].to_numpy() # Temp perp of the core\n",
    "    chi_a = df_sf01['chi'].to_numpy() # chi (overall fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030bbe58-23e4-42e6-93f4-2476c0ed15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ham is not None:\n",
    "    hardham_list = []\n",
    "    core_dens_list = []\n",
    "    neck_dens_list = []\n",
    "    ham_dens_list = []\n",
    "    core_ux_list = []\n",
    "    core_uy_list = []\n",
    "    core_uz_list = []\n",
    "    neck_ux_list = []\n",
    "    neck_uy_list = []\n",
    "    neck_uz_list = []\n",
    "    ham_ux_list = []\n",
    "    ham_uy_list = []\n",
    "    ham_uz_list = []\n",
    "    core_Txx_list = []\n",
    "    core_Txy_list = []\n",
    "    core_Txz_list = []\n",
    "    core_Tyy_list = []\n",
    "    core_Tyz_list = []\n",
    "    core_Tzz_list = []\n",
    "    neck_Txx_list = []\n",
    "    neck_Txy_list = []\n",
    "    neck_Txz_list = []\n",
    "    neck_Tyy_list = []\n",
    "    neck_Tyz_list = []\n",
    "    neck_Tzz_list = []\n",
    "    ham_Txx_list = []\n",
    "    ham_Txy_list = []\n",
    "    ham_Txz_list = []\n",
    "    ham_Tyy_list = []\n",
    "    ham_Tyz_list = []\n",
    "    ham_Tzz_list = []\n",
    "    ogflag_list = []\n",
    "\n",
    "    for key in datham.keys():\n",
    "        try:\n",
    "            core_dens_list.append(datham[key]['core_moments']['n'])\n",
    "            neck_dens_list.append(datham[key]['neck_moments']['n'])\n",
    "            ham_dens_list.append(datham[key]['hammer_moments']['n'])\n",
    "\n",
    "            core_ux_list.append(datham[key]['core_moments']['Ux'])\n",
    "            core_uy_list.append(datham[key]['core_moments']['Uy'])\n",
    "            core_uz_list.append(datham[key]['core_moments']['Uz'])\n",
    "\n",
    "            neck_ux_list.append(datham[key]['neck_moments']['Ux'])\n",
    "            neck_uy_list.append(datham[key]['neck_moments']['Uy'])\n",
    "            neck_uz_list.append(datham[key]['neck_moments']['Uz'])\n",
    "\n",
    "            ham_ux_list.append(datham[key]['hammer_moments']['Ux'])\n",
    "            ham_uy_list.append(datham[key]['hammer_moments']['Uy'])\n",
    "            ham_uz_list.append(datham[key]['hammer_moments']['Uz'])\n",
    "\n",
    "            core_Txx_list.append(datham[key]['core_moments']['Txx'])\n",
    "            core_Txy_list.append(datham[key]['core_moments']['Txy'])\n",
    "            core_Txz_list.append(datham[key]['core_moments']['Txz'])\n",
    "            core_Tyy_list.append(datham[key]['core_moments']['Tyy'])\n",
    "            core_Tyz_list.append(datham[key]['core_moments']['Tyz'])\n",
    "            core_Tzz_list.append(datham[key]['core_moments']['Tzz'])\n",
    "\n",
    "            neck_Txx_list.append(datham[key]['neck_moments']['Txx'])\n",
    "            neck_Txy_list.append(datham[key]['neck_moments']['Txy'])\n",
    "            neck_Txz_list.append(datham[key]['neck_moments']['Txz'])\n",
    "            neck_Tyy_list.append(datham[key]['neck_moments']['Tyy'])\n",
    "            neck_Tyz_list.append(datham[key]['neck_moments']['Tyz'])\n",
    "            neck_Tzz_list.append(datham[key]['neck_moments']['Tzz'])\n",
    "\n",
    "            ham_Txx_list.append(datham[key]['hammer_moments']['Txx'])\n",
    "            ham_Txy_list.append(datham[key]['hammer_moments']['Txy'])\n",
    "            ham_Txz_list.append(datham[key]['hammer_moments']['Txz'])\n",
    "            ham_Tyy_list.append(datham[key]['hammer_moments']['Tyy'])\n",
    "            ham_Tyz_list.append(datham[key]['hammer_moments']['Tyz'])\n",
    "            ham_Tzz_list.append(datham[key]['hammer_moments']['Tzz'])\n",
    "\n",
    "            # print(datham[key]['og_flag'])\n",
    "            try: ogflag_list.append(datham[key]['og_flag'])\n",
    "            except: ogflag_list.append(False)\n",
    "            hardham_list.append(key)\n",
    "\n",
    "        except: pass\n",
    "\n",
    "    ogflag_list = np.asarray(ogflag_list)\n",
    "    hardham_list = np.asarray(hardham_list)\n",
    "    hamstring = np.ones_like(hardham_list)\n",
    "    from datetime import datetime, timedelta, timezone\n",
    "    hardham_datetime_cdflib = cdflib.cdfepoch.to_datetime(hardham_list)\n",
    "\n",
    "    # Convert to pandas Timestamps (list comprehension)\n",
    "    hardham_datetime = [pd.Timestamp(t) for t in hardham_datetime_cdflib]\n",
    "    # Now timestamps.replace() will work!\n",
    "    # Converting the magnetic field time to a timezone-aware datetime object\n",
    "    hardham_datetime_utc = np.array([dt.replace(tzinfo=timezone.utc) for dt in hardham_datetime])\n",
    "    #hardham_dattime = np.datetime64(hardham_datetime)\n",
    "\n",
    "    dt_hardham_datetime_utc = np.diff(np.asarray(hardham_datetime_utc))\n",
    "    dt_hamstring = np.ones_like(dt_hardham_datetime_utc)\n",
    "    hardham_datetime_utc_plus_dt = []\n",
    "    for dt in range(len(dt_hardham_datetime_utc)):\n",
    "        hardham_datetime_utc_plus_dt.append(hardham_datetime_utc[dt] + dt_hardham_datetime_utc[dt])\n",
    "\n",
    "    hardham_datetime_utc_plus_dt = np.asarray(hardham_datetime_utc_plus_dt)    \n",
    "    hamstring_plus_dt = np.ones_like(hardham_datetime_utc_plus_dt)\n",
    "\n",
    "    core_dens_list = np.asarray(core_dens_list)\n",
    "    neck_dens_list = np.asarray(neck_dens_list)\n",
    "    ham_dens_list = np.asarray(ham_dens_list)\n",
    "\n",
    "    core_ux_list = np.asarray(core_ux_list)\n",
    "    core_uy_list = np.asarray(core_uy_list)\n",
    "    core_uz_list = np.asarray(core_uz_list)\n",
    "    core_umag = np.sqrt(core_ux_list**2 + core_uy_list**2 + core_uz_list**2)\n",
    "\n",
    "    neck_ux_list = np.asarray(neck_ux_list)\n",
    "    neck_uy_list = np.asarray(neck_uy_list)\n",
    "    neck_uz_list = np.asarray(neck_uz_list)\n",
    "    neck_umag = np.sqrt(neck_ux_list**2 + neck_uy_list**2 + neck_uz_list**2)\n",
    "    neck_core_drift = abs(neck_umag - core_umag)\n",
    "\n",
    "    ham_ux_list = np.asarray(ham_ux_list)\n",
    "    ham_uy_list = np.asarray(ham_uy_list)\n",
    "    ham_uz_list = np.asarray(ham_uz_list)\n",
    "    ham_umag = np.sqrt(ham_ux_list**2 + ham_uy_list**2 + ham_uz_list**2)\n",
    "    ham_core_drift = np.abs(ham_umag - core_umag)\n",
    "    \n",
    "    b_ds = downsample_to_min_ind(datetime_spi, B_mag_XYZ, hardham_datetime_utc)\n",
    "    d_ds = downsample_to_min_ind(datetime_spi, dens_spi.y, hardham_datetime_utc)\n",
    "    #The alfven speed\n",
    "    v_alfven_spi_ham = 21.8 * b_ds/ np.sqrt(d_ds)\n",
    "    ham_core_drift_va = ham_core_drift/v_alfven_spi_ham\n",
    "    neck_core_drift_va = neck_core_drift/v_alfven_spi_ham\n",
    "\n",
    "    conv_fac = 1e6/11600 #MK to eV\n",
    "    conv_fac = 1/conv_fac\n",
    "\n",
    "    core_Txx_list = np.asarray(core_Txx_list)/conv_fac\n",
    "    core_Txy_list = np.asarray(core_Txy_list)/conv_fac\n",
    "    core_Txz_list = np.asarray(core_Txz_list)/conv_fac\n",
    "    core_Tyy_list = np.asarray(core_Tyy_list)/conv_fac\n",
    "    core_Tyz_list = np.asarray(core_Tyz_list)/conv_fac\n",
    "    core_Tzz_list = np.asarray(core_Tzz_list)/conv_fac\n",
    "\n",
    "    core_temp = (core_Txx_list + core_Tyy_list + core_Tyy_list)/3  \n",
    "\n",
    "    neck_Txx_list = np.asarray(neck_Txx_list)/conv_fac\n",
    "    neck_Txy_list = np.asarray(neck_Txy_list)/conv_fac\n",
    "    neck_Txz_list = np.asarray(neck_Txz_list)/conv_fac\n",
    "    neck_Tyy_list = np.asarray(neck_Tyy_list)/conv_fac\n",
    "    neck_Tyz_list = np.asarray(neck_Tyz_list)/conv_fac\n",
    "    neck_Tzz_list = np.asarray(neck_Tzz_list)/conv_fac\n",
    "\n",
    "    neck_temp = (neck_Txx_list + neck_Tyy_list + neck_Tyy_list)/3  \n",
    "\n",
    "    ham_Txx_list = np.asarray(ham_Txx_list)/conv_fac\n",
    "    ham_Txy_list = np.asarray(ham_Txy_list)/conv_fac\n",
    "    ham_Txz_list = np.asarray(ham_Txz_list)/conv_fac\n",
    "    ham_Tyy_list = np.asarray(ham_Tyy_list)/conv_fac\n",
    "    ham_Tyz_list = np.asarray(ham_Tyz_list)/conv_fac\n",
    "    ham_Tzz_list = np.asarray(ham_Tzz_list)/conv_fac\n",
    "\n",
    "    ham_temp = (ham_Txx_list + ham_Tyy_list + ham_Tyy_list)/3  \n",
    "\n",
    "    spi_epoch_dat = get_data('psp_spi_sf00_Epoch')\n",
    "    spi_epoch = spi_epoch_dat.y\n",
    "\n",
    "    T_perp_core, T_parallel_core, Anisotropy_core = find_Tanisotropy(core_Txx_list,core_Tyy_list,core_Tzz_list,\n",
    "                                                                     core_Txy_list,core_Txz_list,core_Tyz_list)\n",
    "    T_perp_neck, T_parallel_neck, Anisotropy_neck = find_Tanisotropy(neck_Txx_list,neck_Tyy_list,neck_Tzz_list,\n",
    "                                                                     neck_Txy_list,neck_Txz_list,neck_Tyz_list)\n",
    "    T_perp_ham, T_parallel_ham, Anisotropy_ham = find_Tanisotropy(ham_Txx_list,ham_Tyy_list,ham_Tzz_list,\n",
    "                                                                  ham_Txy_list,ham_Txz_list,ham_Tyz_list)\n",
    "\n",
    "    Ntot = core_dens_list + neck_dens_list + ham_dens_list\n",
    "    Nham_div_Ntot = ham_dens_list/Ntot\n",
    "    Nham_div_Ncore = ham_dens_list/core_dens_list\n",
    "    Nneck_div_Ncore = neck_dens_list/core_dens_list\n",
    "    \n",
    "    Tperp_ham_div_core = T_perp_ham/T_perp_core\n",
    "    Tperprat_driftva_hc = Tperp_ham_div_core * ham_core_drift_va\n",
    "\n",
    "    tau_30s = 30 #30 sec\n",
    "    c_30s, bin_centers_datetime_ham_30s =lista_hista(hardham_datetime, tau_30s) \n",
    "    #hardham_datetime_og = hardham_datetime[ogflag_list]\n",
    "    c_og_30s, bin_centers_datetime_ham_og_30s =lista_hista(np.asarray(hardham_datetime)[ogflag_list], tau_30s)\n",
    "    \n",
    "    tau_1m = 1*60 #1 min\n",
    "    c_1m, bin_centers_datetime_ham_1m =lista_hista(hardham_datetime, tau_1m) \n",
    "    #hardham_datetime_og = hardham_datetime[ogflag_list]\n",
    "    c_og_1m, bin_centers_datetime_ham_og_1m =lista_hista(np.asarray(hardham_datetime)[ogflag_list], tau_1m) \n",
    "    \n",
    "    tau_2m = 2*60 #2 min\n",
    "    c_2m, bin_centers_datetime_ham_2m =lista_hista(hardham_datetime, tau_2m) \n",
    "    #hardham_datetime_og = hardham_datetime[ogflag_list]\n",
    "    c_og_2m, bin_centers_datetime_ham_og_2m =lista_hista(np.asarray(hardham_datetime)[ogflag_list], tau_2m) \n",
    "    \n",
    "    tau_20m = 20*60 #20 min\n",
    "    c_20m, bin_centers_datetime_ham_20m =lista_hista(hardham_datetime, tau_20m) \n",
    "    #hardham_datetime_og = hardham_datetime[ogflag_list]\n",
    "    c_og_20m, bin_centers_datetime_ham_og_20m =lista_hista(np.asarray(hardham_datetime)[ogflag_list], tau_20m) \n",
    "\n",
    "    tau_90m = 90*60 #90 min\n",
    "    c_90m, bin_centers_datetime_ham_90m =lista_hista(hardham_datetime, tau_90m) \n",
    "    #hardham_datetime_og = hardham_datetime[ogflag_list]\n",
    "    c_og_90m, bin_centers_datetime_ham_og_90m =lista_hista(np.asarray(hardham_datetime)[ogflag_list], tau_90m)\n",
    "\n",
    "    tau_4h = 4*60*60 #4 hr\n",
    "    c_4h, bin_centers_datetime_ham_4h =lista_hista(hardham_datetime, tau_4h) \n",
    "    #hardham_datetime_og = hardham_datetime[ogflag_list]\n",
    "    c_og_4h, bin_centers_datetime_ham_og_4h =lista_hista(np.asarray(hardham_datetime)[ogflag_list], tau_4h)\n",
    "\n",
    "    tau_12h = 12*60*60 #12 hr\n",
    "    c_12h, bin_centers_datetime_ham_12h =lista_hista(hardham_datetime, tau_12h) \n",
    "    #hardham_datetime_og = hardham_datetime[ogflag_list]\n",
    "    c_og_12h, bin_centers_datetime_ham_og_12h =lista_hista(np.asarray(hardham_datetime)[ogflag_list], tau_12h)\n",
    "\n",
    "    \n",
    "    \n",
    "    #upsample to sf00 moment timegrid\n",
    "    c_30s_up = upsample_to_match(bin_centers_datetime_ham_30s,c_30s, datetime_spi)\n",
    "    c_og_30s_up = upsample_to_match(bin_centers_datetime_ham_og_30s,c_og_30s, datetime_spi)\n",
    "    c_2m_up = upsample_to_match(bin_centers_datetime_ham_2m,c_2m, datetime_spi)\n",
    "    c_og_2m_up = upsample_to_match(bin_centers_datetime_ham_og_2m,c_og_2m, datetime_spi)\n",
    "    c_20m_up = upsample_to_match(bin_centers_datetime_ham_20m,c_20m, datetime_spi)\n",
    "    c_og_20m_up = upsample_to_match(bin_centers_datetime_ham_og_20m,c_og_20m, datetime_spi)\n",
    "    c_90m_up = upsample_to_match(bin_centers_datetime_ham_90m,c_90m, datetime_spi)\n",
    "    c_og_90m_up = upsample_to_match(bin_centers_datetime_ham_og_90m,c_og_90m, datetime_spi)\n",
    "    c_4h_up = upsample_to_match(bin_centers_datetime_ham_4h,c_4h, datetime_spi)\n",
    "    c_og_4h_up = upsample_to_match(bin_centers_datetime_ham_og_4h,c_og_4h, datetime_spi)\n",
    "    c_12h_up = upsample_to_match(bin_centers_datetime_ham_12h,c_12h, datetime_spi)\n",
    "    c_og_12h_up = upsample_to_match(bin_centers_datetime_ham_og_12h,c_og_12h, datetime_spi)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Anisotropy_ham_up = upsample_to_match(hardham_datetime_utc,Anisotropy_ham, datetime_spi)\n",
    "    ham_core_drift_up = upsample_to_match(hardham_datetime_utc,ham_core_drift, datetime_spi)\n",
    "    ham_core_drift_va_up = upsample_to_match(hardham_datetime_utc,ham_core_drift_va, datetime_spi)\n",
    "    Nham_div_Ncore_up = upsample_to_match(hardham_datetime_utc,Nham_div_Ncore, datetime_spi)\n",
    "    Nham_div_Ntot_up = upsample_to_match(hardham_datetime_utc,Nham_div_Ntot, datetime_spi)\n",
    "    Tperp_ham_div_core_up = upsample_to_match(hardham_datetime_utc,Tperp_ham_div_core, datetime_spi)\n",
    "    Tperprat_driftva_hc_up = upsample_to_match(hardham_datetime_utc,Tperprat_driftva_hc, datetime_spi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
